[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Science Course 1",
    "section": "",
    "text": "This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses.\nLink to course material:\nhttps://www.coursera.org/learn/python-data-analysis/ungradedLab/33VUU/your-personal-jupyter-notebook-workspace/lab?path=%2Flab"
  },
  {
    "objectID": "code/1_intro.html",
    "href": "code/1_intro.html",
    "title": "2  Introduction into python",
    "section": "",
    "text": "The syntax for writing a function in python. add_numbers is a function that takes two numbers and adds them together.\n\ndef add_numbers(x, y):\n    return x + y\n\nadd_numbers(1,2)\n\n3\n\n\nWe can easily change this to take 3 arguments instead of 2. we can also make the 3rd parameter optional.\nAll of the optional parameters, the ones that you got default values for, need to come at the end of the function declaration. It also means that you can pass an optional parameters as labeled values.\n\ndef add_numbers(x, y, z=None):\n    if z == None:\n        return x + y\n    else:\n        return x + y + z\n\nprint(add_numbers(1,2))\nprint(add_numbers(1,2,3))\n\n3\n6\n\n\nadd_numbers updated to take an optional flag parameter.\n\ndef add_numbers(x, y, z=None, flag=False):\n    if (flag):\n        print('Flag is true!')\n    if (z == None):\n        return x + y\n    else:\n        return x + y + z\n\n\nprint(add_numbers(1, 2, flag=True))\n\nFlag is true!\n3\n\n\nWe can use this for functions to add different modes of operation, i.e. we can add versus subtract:\n\ndef do_math(a, b, kind=None):\n  if (kind=='add'):\n    return a+b\n  else:\n    return a-b\n\ndo_math(1, 2, kind='add')\n\n3\n\n\nWe can also assign function add_numbers to variable a.\n\ndef add_numbers(x, y):\n    return x + y\n\n\na = add_numbers\na(1, 2)\n\n3"
  },
  {
    "objectID": "code/1_intro.html#python-types-and-sequences",
    "href": "code/1_intro.html#python-types-and-sequences",
    "title": "2  Introduction into python",
    "section": "2.2 Python Types and Sequences",
    "text": "2.2 Python Types and Sequences\nWe identify the type of a variable using the type() function:\n\ntype(add_numbers)\n\nfunction\n\n\n\n2.2.1 Tuples\nTuples are an immutable data structure (cannot be altered). We write tuples using parentheses and we can mix types within a tuple.\n\nx = (1, 'a', 2, 'b')\ntype(x)\n\ntuple\n\n\n\n\n2.2.2 Lists\nLists are a mutable data structure. A list is declared using a squared bracket.\n\nx = [1, 'a', 2, 'b']\ntype(x)\n\nlist\n\n\nWe can change the contents of a list, for example using the append function, which appends new items to the end of a list\n\nx.append(3.3)\nx\n\n[1, 'a', 2, 'b', 3.3]\n\n\n\n\n2.2.3 For loops\nBoth list and tuples are iterable types, so we can write loops to go through every value they hold.\n\nfor item in x:\n    print(item)\n\nprint('')\n\n1\na\n2\nb\n3.3\n\n\n\n\n\n2.2.4 Arrays\nLists and tuples can also be accessed as arrays by using the square bracket operator, which is called the indexing operator.\nThe first item of the list starts at position zero and to get the length of the list, we use the built in len function.\n\ni = 0\n\nwhile (i != len(x)):\n    print(x[i])\n    i = i + 1\n\nprint('')\n\n1\na\n2\nb\n3.3\n\n\n\n\n\n2.2.5 Concatenate lists\n\n[1, 2] + [3, 4]\n\n[1, 2, 3, 4]\n\n\n\n\n2.2.6 Repeat values in a list\n\n[1,2] * 3\n\n[1, 2, 1, 2, 1, 2]\n\n\n\n\n2.2.7 The in operator\nWe use the in operator to check if something is inside a list.\n\n1 in [1,2,3]\n\nTrue\n\n\n\n1 in [0,4,5]\n\nFalse\n\n\n\n\n2.2.8 Slicing\nIn Python, the indexing operator allows you to submit multiple values.\nThe first parameter is the starting location, if this is the only element then one item is return from the list.\nThe second parameter is the end of the slice. It’s an exclusive end so if you slice with the first parameter being zero the next parameter being one, then you only get back one item.\n\nx = 'This is a string'\nprint(x[0])  #first character\nprint(x[0:1])  #first character, but we have explicitly set the end character\nprint(x[0:2])  #first two characters\n\nT\nT\nTh\n\n\nOur index values can also be negative to index from the back of the string.\n\nx[-1]\n\n'g'\n\n\n\n#all characters from the 4th last to the second last\nx[-4:-2]\n\n'ri'\n\n\nStart at the first and going until the 3rd:\n\nx[:3]\n\n'Thi'\n\n\nStart with the fourth character and go to the end of the list\n\nx[4:]\n\n' is a string'\n\n\n\n\n2.2.9 Strings\nA lot of the operations we have done before, we can also do on strings\n\nfirstname = 'Christopher'\nlastname = 'Brooks'\n\n#concatenate two strings\nprint(firstname + ' ' + lastname)\n\n#repeat strings\nprint(firstname *2)\n\n#search for strings\nprint('Chris' in firstname)\n\nChristopher Brooks\nChristopherChristopher\nTrue\n\n\nBefore concatenating strings, we have to make sure to convert objects to strings. i.e. this does not work\n'Chris' + 2\nbut this does:\n\n'Chris' + str(2)\n\n'Chris2'\n\n\n\n\n2.2.10 Split\nsplit breaks up a string and returns a list of all the words in a string, or a list split on a specific character.\nBelow, we split up the string based on the presence of a space character resulting in a list of four elements. We can then use an index operator to choose parts of the list:\n\n# [0] selects the first element of the list\nfirstname = 'Christopher Arthur Hansen Brooks'.split(' ')[0]  \n\n# [-1] selects the last element of the list\nlastname = 'Christopher Arthur Hansen Brooks'.split(' ')[-1]  \n\nprint(firstname)\nprint(lastname)\n\nChristopher\nBrooks"
  },
  {
    "objectID": "code/1_intro.html#dictionaries",
    "href": "code/1_intro.html#dictionaries",
    "title": "2  Introduction into python",
    "section": "2.3 Dictionaries",
    "text": "2.3 Dictionaries\nDictionaries are similar to lists and tuples in that they hold a collection of items, but they’re labeled collections which do not have an ordering. This means that for each value you insert into the dictionary, you must also give a key to get that value out. A dictionary is denoted by curly brackets.\nWe indicate each item of the dictionary when creating it using a pair of values separated by colons.\nWe can retrieve a value for a given label using the indexing operator.\n\nx = {'Christopher Brooks': 'brooksch@umich.edu', 'Bill Gates': 'billg@microsoft.com'}\n\n# Retrieve a value by using the indexing operator\nx['Christopher Brooks']  \n\n'brooksch@umich.edu'\n\n\nWe can also add new items to the dictionary:\n\nx['Kevin Thompson'] = None\nx\n\n{'Christopher Brooks': 'brooksch@umich.edu',\n 'Bill Gates': 'billg@microsoft.com',\n 'Kevin Thompson': None}\n\n\n\n#access the values\nx.values()\n\ndict_values(['brooksch@umich.edu', 'billg@microsoft.com', None])\n\n\n\n#access the key:value pairs\nx.items()\n\ndict_items([('Christopher Brooks', 'brooksch@umich.edu'), ('Bill Gates', 'billg@microsoft.com'), ('Kevin Thompson', None)])\n\n\nWe can iterate through the items in a dictionary, i.e. we can iterate over all the keys:\n\nfor name in x:\n    print(x[name])\n\nprint('')\n\nbrooksch@umich.edu\nbillg@microsoft.com\nNone\n\n\n\nIterate over all the values:\n\nfor email in x.values():\n    print(email)\n\nprint('')    \n\nbrooksch@umich.edu\nbillg@microsoft.com\nNone\n\n\n\nWe can also iterate over all the keys and items in a dictionary:\n\nfor name, email in x.items():\n    print(name)\n    print(email)\n\nprint('')\n\nChristopher Brooks\nbrooksch@umich.edu\nBill Gates\nbillg@microsoft.com\nKevin Thompson\nNone\n\n\n\n\n2.3.1 Unpacking\nIn Python you can have a sequence. That’s a list or a tuple of values, and you can unpack those items into different variables through assignment in one statement.\n\nx = ('Christopher', 'Brooks', 'brooksch@umich.edu')\nfname, lname, email = x\n\nprint(fname)\nprint(lname)\nprint(x)\n\nChristopher\nBrooks\n('Christopher', 'Brooks', 'brooksch@umich.edu')\n\n\nMake sure the number of values you are unpacking matches the number of variables being assigned. I.e. the code below would give an error:\nx = ('Christopher', 'Brooks', 'brooksch@umich.edu', 'Ann Arbor')\nfname, lname, email = x"
  },
  {
    "objectID": "code/1_intro.html#python-more-on-strings",
    "href": "code/1_intro.html#python-more-on-strings",
    "title": "2  Introduction into python",
    "section": "2.4 Python More on Strings",
    "text": "2.4 Python More on Strings\nThe Python string formatting mini language allows you to write a string statement indicating placeholders for variables to be evaluated. You then pass these variables in either named or in order arguments, and Python handles the string manipulation for you.\nWe can write a sales statement string which includes these items using curly brackets.\nWe can then call the format method on that string and pass in the values that we want substituted as appropriate.\n\nsales_record = {\n    'price': 3.24,\n    'num_items': 4,\n    'person': 'Chris'}\n\nsales_statement = '{} bought {} item(s) at a price of {} each for a total of {}'\n\nprint(sales_statement.format(sales_record['person'],\nsales_record['num_items'],\nsales_record['price'],\nsales_record['num_items'] * sales_record['price']))\n\nChris bought 4 item(s) at a price of 3.24 each for a total of 12.96"
  },
  {
    "objectID": "code/1_intro.html#reading-and-writing-csv-files",
    "href": "code/1_intro.html#reading-and-writing-csv-files",
    "title": "2  Introduction into python",
    "section": "2.5 Reading and Writing CSV files",
    "text": "2.5 Reading and Writing CSV files\nLet’s import our datafile ../data/mpg.csv, which contains fuel economy data for 234 cars, using the csv module.\n\nmpg : miles per gallon\nclass : car classification\ncty : city mpg\ncyl : # of cylinders\ndispl : engine displacement in liters\ndrv : f = front-wheel drive, r = rear wheel drive, 4 = 4wd\nfl : fuel (e = ethanol E85, d = diesel, r = regular, p = premium, c = CNG)\nhwy : highway mpg\nmanufacturer : automobile manufacturer\nmodel : model of car\ntrans : type of transmission\nyear : model year\n\n\nimport csv\n\n\nwith open('../data/mpg.csv') as csvfile:\n    #read in data as a dictionary\n    mpg = list(csv.DictReader(csvfile))\n\nmpg[:3]\n\n[OrderedDict([('', '1'),\n              ('manufacturer', 'audi'),\n              ('model', 'a4'),\n              ('displ', '1.8'),\n              ('year', '1999'),\n              ('cyl', '4'),\n              ('trans', 'auto(l5)'),\n              ('drv', 'f'),\n              ('cty', '18'),\n              ('hwy', '29'),\n              ('fl', 'p'),\n              ('class', 'compact')]),\n OrderedDict([('', '2'),\n              ('manufacturer', 'audi'),\n              ('model', 'a4'),\n              ('displ', '1.8'),\n              ('year', '1999'),\n              ('cyl', '4'),\n              ('trans', 'manual(m5)'),\n              ('drv', 'f'),\n              ('cty', '21'),\n              ('hwy', '29'),\n              ('fl', 'p'),\n              ('class', 'compact')]),\n OrderedDict([('', '3'),\n              ('manufacturer', 'audi'),\n              ('model', 'a4'),\n              ('displ', '2'),\n              ('year', '2008'),\n              ('cyl', '4'),\n              ('trans', 'manual(m6)'),\n              ('drv', 'f'),\n              ('cty', '20'),\n              ('hwy', '31'),\n              ('fl', 'p'),\n              ('class', 'compact')])]\n\n\n\n#print the length of our list\nlen(mpg)\n\n234\n\n\nThe length of our list is 234, meaning we have a dictionary for each of the 234 cars in the CSV file.\n\n#print column names\nmpg[0].keys()\n\nodict_keys(['', 'manufacturer', 'model', 'displ', 'year', 'cyl', 'trans', 'drv', 'cty', 'hwy', 'fl', 'class'])\n\n\nFind the average cty fuel economy across all cars. All values in the dictionaries are strings, so we need to convert to float.\n\nsum(float(d['cty']) for d in mpg) / len(mpg)\n\n16.858974358974358\n\n\nFind the average city MPG grouped by the number of cylinders a car has.\nWe use set to return the unique values for the number of cylinders the cars in our dataset have.\n\ncylinders = set(d['cyl'] for d in mpg)\ncylinders\n\n{'4', '5', '6', '8'}\n\n\n\n#create empty list to store our calculations\nCtyMpgByCyl = []\n\n#iterate over all cylinder levels and then over all dics\nfor c in cylinders:\n    summpg = 0\n    cyltypecount = 0\n    #iterate over dics\n    for d in mpg:\n        #if the cylinder type matches add the cty mpg and increment the count\n        if d['cyl'] == c:\n            summpg += float(d['cty'])\n            cyltypecount += 1\n    # append the tuple ('cylinder', 'avg mpg')\n    CtyMpgByCyl.append((c, summpg/cyltypecount))\n\n#sort the list (lambda will be covered a bit later)\nCtyMpgByCyl.sort(key=lambda x: x[0])\nCtyMpgByCyl\n\n[('4', 21.012345679012345),\n ('5', 20.5),\n ('6', 16.21518987341772),\n ('8', 12.571428571428571)]\n\n\nNext, lets find the average highway MPG for the different vehicle classes.\n\n#find the different vehicle classes\nvehicleclass = set(d['class'] for d in mpg)\nvehicleclass\n\n{'2seater', 'compact', 'midsize', 'minivan', 'pickup', 'subcompact', 'suv'}\n\n\n\nHwyMpgByClass = []\n\nfor t in vehicleclass:\n    summpg = 0\n    vclasscount = 0\n    for d in mpg:\n        if d['class'] == t:\n            summpg += float(d['hwy'])\n            vclasscount += 1\n    HwyMpgByClass.append((t, summpg/vclasscount))\n\nHwyMpgByClass.sort(key=lambda x : x[1])\nHwyMpgByClass\n\n[('pickup', 16.87878787878788),\n ('suv', 18.129032258064516),\n ('minivan', 22.363636363636363),\n ('2seater', 24.8),\n ('midsize', 27.29268292682927),\n ('subcompact', 28.142857142857142),\n ('compact', 28.29787234042553)]"
  },
  {
    "objectID": "code/1_intro.html#dates-and-times",
    "href": "code/1_intro.html#dates-and-times",
    "title": "2  Introduction into python",
    "section": "2.6 Dates and Times",
    "text": "2.6 Dates and Times\nOne of the most common legacy methods for storing the date and time in online transactions systems is based on the offset from the epoch, which is January 1, 1970.\nIn Python, you can get the current time since the epoch using the time module. You can then create a time stamp using the from time stamp function on the date time object. When we print this value out, we see that the year, month, day, and so forth are also printed out.\n\nimport datetime as dt\nimport time as tm\n\ntime returns the current time in seconds since the Epoch. (January 1st, 1970)\n\ntm.time()\n\n1671458686.8570611\n\n\nCreate a timestamp:\n\ndtnow = dt.datetime.fromtimestamp(tm.time())\ndtnow\n\ndatetime.datetime(2022, 12, 19, 15, 4, 46, 864421)\n\n\nThe date time object has handy attributes to get the representative hour, day, seconds, etc.\n\n# get year, month, day, etc.from a datetime\ndtnow.year, dtnow.month, dtnow.day, dtnow.hour, dtnow.minute, dtnow.second  \n\n(2022, 12, 19, 15, 4, 46)\n\n\nDate time objects allow for simple math using time deltas. For instance, here, we can create a time delta of 100 days, then do subtraction and comparisons with the date time object.\n\n#create a timedelta of 100 days\ndelta = dt.timedelta(days = 100)\ndelta\n\ndatetime.timedelta(days=100)\n\n\n\n#return the current local date\ntoday = dt.date.today()\ntoday\n\ndatetime.date(2022, 12, 19)\n\n\n\n#extract the data 100 days ago\ntoday - delta\n\ndatetime.date(2022, 9, 10)\n\n\n\n#compare dates\ntoday > today - delta\n\nTrue"
  },
  {
    "objectID": "code/1_intro.html#objects-and-map",
    "href": "code/1_intro.html#objects-and-map",
    "title": "2  Introduction into python",
    "section": "2.7 Objects and map()",
    "text": "2.7 Objects and map()\nWe can define a class using the class keyword.\nClasses in Python are generally named using camel case, which means the first character of each word is capitalized.\nClass variables can also be declared. These are just variables which are shared across all instances. So in this example, we’re saying that the default for all people is at the school of information.\nTo define a method, you just write it as you would have a function. The one change, is that to have access to the instance, which a method is being invoked upon, you must include self, in the method signature. Similarly, if you want to refer to instance variables set on the object, you prepend them with the word self, with a full stop.\nIn this definition of a person, for instance, we have written two methods. Set name and set location. And both change instance bound variables, called name and location respectively\n\nclass Person:\n    #set a class variable\n    department = 'School of Information'\n    \n    #define a method\n    def set_name(self, new_name):\n        self.name = new_name\n    \n    def set_location(self, new_location):\n        self.location = new_location\n    \nperson = Person()\nperson.set_name('Chris Vrooks')\nperson.set_location('Mi, USA')\n\nprint('{} lives in {} and works in the department {}'. format(person.name,\nperson.location, person.department))\n\nChris Vrooks lives in Mi, USA and works in the department School of Information\n\n\nThere are a couple of implications of object-oriented programming in Python:\n\nObjects in Python do not have private or protected members. If you instantiate an object, you have full access to any of the methods or attributes of that object\nThere’s no need for an explicit constructor when creating objects in Python. You can add a constructor if you want to by declaring the __init__ method\n\nThe map function is one of the basis for functional programming in Python, it executes a specified function for each item in an iterable.\nFunctional programming is a programming paradigm in which you explicitly declare all parameters which could change through execution of a given function. Thus functional programming is referred to as being side-effect free, because there is a software contract that describes what can actually change by calling a function.\nThe map built-in function is one example of a functional programming feature of Python, that ties together a number of aspects of the language.\nThe map function signature looks like this: - The first parameters of function that you want executed - The second parameter, and every following parameter, is something which can be iterated upon\nImagine we have two list of numbers, maybe prices from two different stores on exactly the same items. And we wanted to find the minimum that we would have to pay if we bought the cheaper item between the two stores. To do this, we could iterate through each list, comparing items and choosing the cheapest. With map, we can do this comparison in a single statement.\n\nstore1 = [10.00, 11.00, 12.34, 2.34]\nstore2 = [9.00, 11.10, 12.34, 2.01]\n\ncheapest = map(min, store1, store2)\ncheapest\n\n<map at 0x7fae68fef490>\n\n\nWhen we go to print out the map, we see that we get an odd reference value instead of a list of items that we’re expecting. This is called lazy evaluation. In Python, the map function returns to you a map object. It doesn’t actually try and run the function min on two items, until you look inside for a value. This allows us to have very efficient memory management, even though something might be computationally complex.\nMaps are iterable, just like lists and tuples, so we can use a for loop to look at all of the values in the map.\n\nfor item in cheapest:\n    print(item)\n\nprint('')\n\n9.0\n11.0\n12.34\n2.01\n\n\n\nQuestion:\nHere is a list of faculty teaching this MOOC. Can you write a function and apply it using map() to get a list of all faculty titles and last names (e.g. [‘Dr. Brooks’, ‘Dr. Collins-Thompson’, …]) ?\n\npeople = ['Dr. Christopher Brooks', 'Dr. Kevyn Collins-Thompson', 'Dr. VG Vinod Vydiswaran', 'Dr. Daniel Romero']\n\ndef split_title_and_name(person):\n    title = person.split()[0]\n    lastname = person.split()[-1]\n    return '{} {}'.format(title, lastname)\n\nlist(map(split_title_and_name, people))\n\n['Dr. Brooks', 'Dr. Collins-Thompson', 'Dr. Vydiswaran', 'Dr. Romero']"
  },
  {
    "objectID": "code/1_intro.html#advanced-python-lambda-and-list-comprehensions",
    "href": "code/1_intro.html#advanced-python-lambda-and-list-comprehensions",
    "title": "2  Introduction into python",
    "section": "2.8 Advanced Python Lambda and List Comprehensions",
    "text": "2.8 Advanced Python Lambda and List Comprehensions\nLambda’s are Python’s way of creating anonymous functions. These are the same as other functions, but they have no name. The intent is that they’re simple or short lived and it’s easier just to write out the function in one line instead of going to the trouble of creating a named function.\nYou declare a lambda function with the word lambda followed by a list of arguments, followed by a colon and then a single expression. This is key: There’s only one expression to be evaluated in a lambda. The expression value is returned on execution of the lambda.\n\nmy_function = lambda a,b,c: a + b\n\nThe return of a lambda is a function reference. So in the case above, we would execute my_function and pass in three different parameters.\n\nmy_function(4,5,6)\n\n9\n\n\nNote that you can’t have default values for lambda parameters and you can’t have complex logic inside of the lambda itself because you’re limited to a single expression. Therefore, lambdas are more limited than full function definitions.\nAnother example, in which we add 10 to argument a, and return the result:\n\nx = lambda a : a + 10\nprint(x(5))\n\n15\n\n\nYou can apply the function above to an argument by surrounding the function and its argument with parentheses:\n\n(lambda x: x + 1)(5)\n\n6\n\n\nBecause a lambda function is an expression, it can be named. Therefore you could write the previous code as follows:\n\nadd_one = lambda x: x + 1\nadd_one(5)\n\n6\n\n\nExercise\nConvert the code below into a lambda:\n\npeople = ['Dr. Christopher Brooks', 'Dr. Kevyn Collins-Thompson', 'Dr. VG Vinod Vydiswaran', 'Dr. Daniel Romero']\n\ndef split_title_and_name(person):\n    return person.split()[0] + ' ' + person.split()[-1]\n\nlist(map(split_title_and_name, people))\n\n['Dr. Brooks', 'Dr. Collins-Thompson', 'Dr. Vydiswaran', 'Dr. Romero']\n\n\n\n#option 1\nfor person in people:\n    print((lambda x: x.split()[0] + ' ' + x.split()[-1])(person))\n\nprint('')\n\nDr. Brooks\nDr. Collins-Thompson\nDr. Vydiswaran\nDr. Romero\n\n\n\n\n#option 2\nlist(map(lambda person: person.split()[0] + ' ' + person.split()[-1], people))\n\n['Dr. Brooks', 'Dr. Collins-Thompson', 'Dr. Vydiswaran', 'Dr. Romero']\n\n\n\n2.8.1 List comprehensions\nWe’ve learned a lot about sequences and in Python. Tuples, lists, dictionaries and so forth.\nSequences are structures that we can iterate over, and often we create these through loops or by reading in data from a file.\nPython has built in support for creating these collections using a more abbreviated syntax called list comprehensions. The basic syntax looks as follows:\nnewlist = [expression for item in iterable if condition == True]\nLet’s start with how we usually write for loops:\n\nmy_list = []\n\nfor number in range(0,100):\n    #check for evenly dividing numbers\n    if number % 2 == 0:\n        my_list.append(number)\n\nmy_list\n\n[0,\n 2,\n 4,\n 6,\n 8,\n 10,\n 12,\n 14,\n 16,\n 18,\n 20,\n 22,\n 24,\n 26,\n 28,\n 30,\n 32,\n 34,\n 36,\n 38,\n 40,\n 42,\n 44,\n 46,\n 48,\n 50,\n 52,\n 54,\n 56,\n 58,\n 60,\n 62,\n 64,\n 66,\n 68,\n 70,\n 72,\n 74,\n 76,\n 78,\n 80,\n 82,\n 84,\n 86,\n 88,\n 90,\n 92,\n 94,\n 96,\n 98]\n\n\nWe can do the same with a list comprehension. We start the list comprehension with the value we want in the list. In this case, it’s a number. Then we put it in the for-loop, and then finally, we add any condition clauses.\n\nmy_list = [number for number in range(0,100) if number % 2 == 0]\nmy_list\n\n[0,\n 2,\n 4,\n 6,\n 8,\n 10,\n 12,\n 14,\n 16,\n 18,\n 20,\n 22,\n 24,\n 26,\n 28,\n 30,\n 32,\n 34,\n 36,\n 38,\n 40,\n 42,\n 44,\n 46,\n 48,\n 50,\n 52,\n 54,\n 56,\n 58,\n 60,\n 62,\n 64,\n 66,\n 68,\n 70,\n 72,\n 74,\n 76,\n 78,\n 80,\n 82,\n 84,\n 86,\n 88,\n 90,\n 92,\n 94,\n 96,\n 98]\n\n\nAnother example:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\n\nnewlist = [x for x in fruits if \"a\" in x]\n\nprint(newlist)\n\n['apple', 'banana', 'mango']\n\n\nNotice, that the if statement is optional:\n\nnewlist = [x for x in fruits]\nnewlist\n\n['apple', 'banana', 'cherry', 'kiwi', 'mango']\n\n\nThe expression is the current item in the iteration, but it is also the outcome, which you can manipulate before it ends up like a list item in the new list:\n\nnewlist = [x.upper() for x in fruits]\nnewlist\n\n['APPLE', 'BANANA', 'CHERRY', 'KIWI', 'MANGO']\n\n\nThe expression can also contain conditions, not like a filter, but as a way to manipulate the outcome:\n\nnewlist = [x if x != \"banana\" else \"orange\" for x in fruits]\nnewlist\n\n['apple', 'orange', 'cherry', 'kiwi', 'mango']\n\n\nExercise:\nThe function to convert:\n\ndef times_tables():\n    lst = []\n    for i in range(10):\n        for j in range (10):\n            lst.append(i*j)\n    return lst\n\nThe list comprehension:\n\ntimes_tables() == [i*j for i in range(10) for j in range(10)]\n\nTrue\n\n\nQuestion\nMany organizations have user ids which are constrained in some way. Imagine you work at an internet service provider and the user ids are all two letters followed by two numbers (e.g. aa49). Your task at such an organization might be to hold a record on the billing activity for each possible user.\nWrite an initialization line as a single list comprehension which creates a list of all possible user ids. Assume the letters are all lower case.\n\n#lowercase = 'abcdefghijklmnopqrstuvwxyz'\n#digits = '0123456789'\n\nmy_list = []\nlowercase = 'ab'\ndigits = '01'\n\nmy_list = [a+b+c+d for a in lowercase for b in lowercase for c in digits for d in digits]\n\nmy_list[0:4]\n\n['aa00', 'aa01', 'aa10', 'aa11']\n\n\n\nlen(my_list)\n\n16\n\n\nanswer = [???] correct_answer == answer ```"
  },
  {
    "objectID": "code/2_numpy.html",
    "href": "code/2_numpy.html",
    "title": "3  Numpy",
    "section": "",
    "text": "Numpy is the fundamental package for numeric computing with Python. It provides powerful ways to create, store, and/or manipulate data, which makes it able to seamlessly and speedily integrate with a wide variety of databases. This is also the foundation that Pandas is built on, which is a high-performance data-centric package that we will learn later in the course.\nIn this lecture, we will talk about creating array with certain data types, manipulating array, selecting elements from arrays, and loading dataset into array. Such functions are useful for manipulating data and understanding the functionalities of other common Python data packages."
  },
  {
    "objectID": "code/2_numpy.html#array-creation",
    "href": "code/2_numpy.html#array-creation",
    "title": "3  Numpy",
    "section": "3.1 Array creation",
    "text": "3.1 Array creation\n\n# Arrays are displayed as a list or list of lists and can be created through list as well. When creating an\n# array, we pass in a list as an argument in numpy array\na = np.array([1, 2, 3])\nprint(a)\n\n[1 2 3]\n\n\n\n# We can print the number of dimensions of a list using the ndim attribute\nprint(a.ndim)\n\n1\n\n\n\n# If we pass in a list of lists in numpy array, we create a multi-dimensional array, for instance, a matrix\nb = np.array([[1,2,3],[4,5,6]])\nb\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\n# We can print out the length of each dimension by calling the shape attribute, which returns a tuple\nb.shape\n\n(2, 3)\n\n\n\n# We can also check the type of items in the array\na.dtype\n\ndtype('int64')\n\n\n\n# Besides integers, floats are also accepted in numpy arrays\nc = np.array([2.2, 5, 1.1])\nc.dtype.name\n\n'float64'\n\n\n\n# Let's look at the data in our array\nc\n\narray([2.2, 5. , 1.1])\n\n\nNote that numpy automatically converts integers, like 5, up to floats, since there is no loss of precision. Numpy will try and give you the best data type format possible to keep your data types homogeneous, which means all the same, in the array.\nSometimes we know the shape of an array that we want to create, but not what we want to be in it. numpy offers several functions to create arrays with initial placeholders, such as zero’s or one’s.\n\n# Lets create two arrays, both the same shape but with different filler values\nd = np.zeros((2,3))\nprint(d)\n\n[[0. 0. 0.]\n [0. 0. 0.]]\n\n\n\ne = np.ones((2,3))\nprint(e)\n\n[[1. 1. 1.]\n [1. 1. 1.]]\n\n\n\n# We can also generate an array with random numbers\nnp.random.rand(2,3)\n\narray([[0.54749485, 0.67006722, 0.39011362],\n       [0.78564183, 0.71416696, 0.73579126]])\n\n\nYou’ll see zeros, ones, and rand used quite often to create example arrays, especially in stack overflow posts and other forums.\nWe can also create a sequence of numbers in an array with the arrange() function:\n\nThe fist argument is the starting bound\nthe second argument is the ending bound\nthe third argument is the difference between each consecutive numbers\n\n\n# Let's create an array of every even number from ten (inclusive) to fifty (exclusive)\nf = np.arange(10, 50, 2)\nf\n\narray([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42,\n       44, 46, 48])\n\n\nIf we want to generate a sequence of floats, we can use the linspace() function. In this function the third argument isn’t the difference between two numbers, but the total number of items you want to generate\n\n#create 15 numbers from 0 (inclusive) to 2 (inclusive)\nnp.linspace( 0, 2, 15 ) \n\narray([0.        , 0.14285714, 0.28571429, 0.42857143, 0.57142857,\n       0.71428571, 0.85714286, 1.        , 1.14285714, 1.28571429,\n       1.42857143, 1.57142857, 1.71428571, 1.85714286, 2.        ])"
  },
  {
    "objectID": "code/2_numpy.html#array-operations",
    "href": "code/2_numpy.html#array-operations",
    "title": "3  Numpy",
    "section": "3.2 Array operations",
    "text": "3.2 Array operations\nWe can do many things on arrays, such as mathematical manipulation (addition, subtraction, square, exponents) as well as use boolean arrays, which are binary values. We can also do matrix manipulation such as product, transpose, inverse, and so forth.\nArithmetic operators on array apply elementwise.\n\n# Let's create a couple of arrays\na = np.array([10,20,30,40])\nb = np.array([1, 2, 3,4])\n\n# Now let's look at a minus b\nc = a-b\nprint(c)\n\n[ 9 18 27 36]\n\n\n\n# And let's look at a times b\nd = a*b\nprint(d)\n\n[ 10  40  90 160]\n\n\nWith arithmetic manipulation, we can convert current data to the way we want it to be. Here’s a real-world problem I face - I moved down to the United States about 6 years ago from Canada. In Canada we use celcius for temperatures, and my wife still hasn’t converted to the US system which uses farenheit. With numpy I could easily convert a number of farenheit values, say the weather forecase, to ceclius:\n\n# Let's create an array of typical Ann Arbor winter farenheit values\nfarenheit = np.array([0,-10,-5,-15,0])\n\ncelcius = (farenheit - 31) * (5/9)\ncelcius\n\narray([-17.22222222, -22.77777778, -20.        , -25.55555556,\n       -17.22222222])\n\n\nAnother useful and important manipulation is the boolean array. We can apply an operator on an array, and a boolean array will be returned for any element in the original, with True being emitted if it meets the condition and False otherwise. For instance, if we want to get a boolean array to check celcius degrees that are greater than -20 degrees\n\ncelcius > -20\n\narray([ True, False, False, False,  True])\n\n\nHere’s another example, we could use the modulus operator to check numbers in an array to see if they are even. Recall that modulus does division but throws away everything but the remainder (decimal) portion)\n\ncelcius%2 == 0\n\narray([False, False,  True, False, False])\n\n\nBesides element-wise manipulation, it is important to know that numpy supports matrix manipulation. Let’s look at matrix product. if we want to do elementwise product, we use the “*” sign\n\nA = np.array([[1,1],[0,1]])\nB = np.array([[2,0],[3,4]])\nprint(A*B)\n\n[[2 0]\n [0 4]]\n\n\nIf we want to do matrix product, we use the “@” sign or use the dot function\n\nprint(A@B)\n\n[[5 4]\n [3 4]]\n\n\nA few more linear algebra concepts are worth layering in here. You might recall that the product of two matrices is only plausible when the inner dimensions of the two matrices are the same. The dimensions refer to the number of elements both horizontally and vertically in the rendered matrices you’ve seen here. We can use numpy to quickly see the shape of a matrix:\n\nA.shape\n\n(2, 2)\n\n\nWhen manipulating arrays of different types, the type of the resulting array will correspond to the more general of the two types. This is called upcasting.\n\n# Let's create an array of integers\narray1 = np.array([[1, 2, 3], [4, 5, 6]])\nprint(array1.dtype)\n\n# Now let's create an array of floats\narray2 = np.array([[7.1, 8.2, 9.1], [10.4, 11.2, 12.3]])\nprint(array2.dtype)\n\nint64\nfloat64\n\n\nIntegers (int) are whole numbers only, and Floating point numbers (float) can have a whole number portion and a decimal portion. The 64 in this example refers to the number of bits that the operating system is reserving to represent the number, which determines the size (or precision) of the numbers that can be represented.\n\n# Let's do an addition for the two arrays\narray3=array1+array2\nprint(array3)\nprint(array3.dtype)\n\n[[ 8.1 10.2 12.1]\n [14.4 16.2 18.3]]\nfloat64\n\n\nNotice how the items in the resulting array have been upcast into floating point numbers.\nNumpy arrays have many interesting aggregation functions on them, such as sum(), max(), min(), and mean().\n\nprint(array3.sum())\nprint(array3.max())\nprint(array3.min())\nprint(array3.mean())\n\n79.3\n18.3\n8.1\n13.216666666666667\n\n\nFor two dimensional arrays, we can do the same thing for each row or column, let’s create an array with 15 elements, ranging from 1 to 15, with a dimension of 3X5.\n\nb = np.arange(1,16,1).reshape(3,5)\nprint(b)\n\n[[ 1  2  3  4  5]\n [ 6  7  8  9 10]\n [11 12 13 14 15]]\n\n\nNow, we often think about two dimensional arrays being made up of rows and columns, but you can also think of these arrays as just a giant ordered list of numbers, and the shape of the array, the number of rows and columns, is just an abstraction that we have for a particular purpose. Actually, this is exactly how basic images are stored in computer environments.\nLet’s take a look at an example and see how numpy comes into play.\n\n# For this demonstration I'll use the python imaging library (PIL) and a function to display images in the\n# Jupyter notebook\nfrom PIL import Image\nfrom IPython.display import display\n\n# And let's just look at the image I'm talking about\nim = Image.open('../images/chris.tiff')\ndisplay(im)\n\n\n\n\n\n# Now, we can convert this PIL image to a numpy array\narray=np.array(im)\nprint(array.shape)\nprint(array[:1])\n\n(200, 200)\n[[118 117 118 118 112 103  92  82  66  56  45  39  38  40  43  46  53  53\n   53  52  48  43  39  36  19  15  15  16  24  28  33  35  39  34  28  23\n   15   8   8  16  41  60  91 118 135 141 141 141 133 120 103  87  73  68\n   77  91 105 109 117 121 120 114 104  98  84  76  70  72  76  82  94 107\n  113 121 126 132 133 135 137 139 144 145 150 157 166 171 170 168 169 157\n  145 138 130 124 125 134 137 141 145 148 148 143 138 136 134 133 133 133\n  138 142 147 150 146 140 126 108  85  61  44  34  39  49  65  83 104 125\n  143 155 150 155 153 146 135 120  99  80  69  63  65  78  98 116 134 147\n  155 165 170 166 159 154 146 137 125 119 117 122 127 129 131 135 138 142\n  146 150 152 150 148 145 144 141 139 138 139 137 133 129 124 126 128 129\n  129 127 125 123 120 120 124 126 125 122 117 114 115 114 113 108 105 103\n  107 110]]\n\n\n\narray.dtype\n\ndtype('uint8')\n\n\nHere we see that we have a 200x200 array and that the values are all uint8. The uint means that they are unsigned integers (so no negative numbers) and the 8 means 8 bits per byte. This means that each value can be up to 2222222*2=256 in size (well, actually 255, because we start at zero).\nFor black and white images black is stored as 0 and white is stored as 255. So if we just wanted to invert this image we could use the numpy array to do so\n\n# Let's create an array the same shape\nmask=np.full(array.shape,255)\nmask\n\narray([[255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       ...,\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255]])\n\n\n\n# Now let's subtract that from the modified array\nmodified_array=array-mask\n\n# And lets convert all of the negative values to positive values\nmodified_array=modified_array*-1\n\n# And as a last step, let's tell numpy to set the value of the datatype correctly\nmodified_array=modified_array.astype(np.uint8)\nmodified_array\n\narray([[137, 138, 137, ..., 152, 148, 145],\n       [142, 142, 142, ..., 155, 152, 149],\n       [147, 147, 148, ..., 160, 157, 153],\n       ...,\n       [ 78,  74,  73, ...,  62,  57,  63],\n       [ 77,  73,  72, ...,  62,  54,  66],\n       [ 77,  73,  71, ...,  62,  54,  68]], dtype=uint8)\n\n\nAnd lastly, lets display this new array. We do this by using the fromarray() function in the python imaging library to convert the numpy array into an object jupyter can render\n\ndisplay(Image.fromarray(modified_array))\n\n\n\n\nOk, remember how I started this by talking about how we could just think of this as a giant array of bytes, and that the shape was an abstraction? Well, we could just decide to reshape the array and still try and render it. PIL is interpreting the individual rows as lines, so we can change the number of lines and columns if we want to. What do you think that would look like?\n\nreshaped=np.reshape(modified_array,(100,400))\nprint(reshaped.shape)\ndisplay(Image.fromarray(reshaped))\n\n(100, 400)\n\n\n\n\n\nBy reshaping the array to be only 100 rows high but 400 columns we’ve essentially doubled the image by taking every other line and stacking them out in width. This makes the image look more stretched out too.\nThis isn’t an image manipulation course, but the point was to show you that these numpy arrays are really just abstractions on top of data, and that data has an underlying format (in this case, uint8). But further, we can build abstractions on top of that, such as computer code which renders a byte as either black or white, which has meaning to people."
  },
  {
    "objectID": "code/2_numpy.html#indexing-slicing-iterating",
    "href": "code/2_numpy.html#indexing-slicing-iterating",
    "title": "3  Numpy",
    "section": "3.3 Indexing, Slicing, Iterating",
    "text": "3.3 Indexing, Slicing, Iterating\nIndexing, slicing and iterating are extremely important for data manipulation and analysis because these techniques allow us to select data based on conditions, and copy or update data.\n\n3.3.1 Indexing\nFirst we are going to look at integer indexing. A one-dimensional array, works in similar ways as a list. To get an element in a one-dimensional array, we simply use the offset index.\n\na = np.array([1,3,5,7])\na[2]\n\n5\n\n\nFor multidimensional array, we need to use integer array indexing, let’s create a new multidimensional array:\n\na = np.array([[1,2], [3, 4], [5, 6]])\na\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\nIf we want to select one certain element, we can do so by entering the index, which is comprised of two integers the first being the row, and the second the column.\n\na[1,1]\n\n4\n\n\nIf we want to get multiple elements for example, 1, 4, and 6 and put them into a one-dimensional array we can enter the indices directly into an array function:\n\nnp.array([a[0, 0], a[1, 1], a[2, 1]])\n\narray([1, 4, 6])\n\n\nWe can also do that by using another form of array indexing, which essentially “zips” the first list and the second list up:\n\nprint(a[[0, 1, 2], [0, 1, 1]])\n\n[1 4 6]\n\n\n\n\n3.3.2 Boolean indexing\nBoolean indexing allows us to select arbitrary elements based on conditions. For example, in the matrix we just talked about we want to find elements that are greater than 5 so we set up a conditon a >5 :\n\nprint(a >5)\n\n[[False False]\n [False False]\n [False  True]]\n\n\n\n\n\nThis returns a boolean array showing that if the value at the corresponding index is greater than 5.\nWe can then place this array of booleans like a mask over the original array to return a one-dimensional array relating to the true values.\n\nprint(a[a>5])\n\n[6]\n\n\n\n\n3.3.3 Slicing\nSlicing is a way to create a sub-array based on the original array. For one-dimensional arrays, slicing works in similar ways to a list. To slice, we use the : sign. For instance, if we put :3 in the indexing brackets, we get elements from index 0 to index 3 (excluding index 3)\n\na = np.array([0,1,2,3,4,5])\nprint(a[:3])\n\n[0 1 2]\n\n\nBy putting 2:4 in the bracket, we get elements from index 2 to index 4 (excluding index 4)\n\nprint(a[2:4])\n\n[2 3]\n\n\nFor multi-dimensional arrays, it works similarly, lets see an example\n\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\na\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\nFirst, if we put one argument in the array, for example a[:2] then we would get all the elements from the first (0th) and second row (1th)\n\na[:2]\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\nIf we add another argument to the array, for example a[:2, 1:3], we get the first two rows but then the second and third column values only\n\na[:2, 1:3]\n\narray([[2, 3],\n       [6, 7]])\n\n\nSo, in multidimensional arrays, the first argument is for selecting rows, and the second argument is for selecting columns\nIt is important to realize that a slice of an array is a view into the same data. This is called passing by reference. So modifying the sub array will consequently modify the original array\nHere I’ll change the element at position [0, 0], which is 2, to 50, then we can see that the value in the original array is changed to 50 as well\n\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nsub_array = a[:2, 1:3]\nsub_array\n\narray([[2, 3],\n       [6, 7]])\n\n\n\nprint(\"sub array index [0,0] value before change:\", sub_array[0,0])\n\nsub array index [0,0] value before change: 2\n\n\n\nsub_array[0,0] = 50\nsub_array\n\narray([[50,  3],\n       [ 6,  7]])\n\n\n\nprint(\"sub array index [0,0] value after change:\", sub_array[0,0])\nprint(\"original array index [0,1] value after change:\", a[0,1])\n\nsub array index [0,0] value after change: 50\noriginal array index [0,1] value after change: 50"
  },
  {
    "objectID": "code/2_numpy.html#trying-numpy-with-datasets",
    "href": "code/2_numpy.html#trying-numpy-with-datasets",
    "title": "3  Numpy",
    "section": "3.4 Trying numpy with datasets",
    "text": "3.4 Trying numpy with datasets\nNow that we have learned the essentials of Numpy let’s use it on a couple of datasets.\nHere we have a very popular dataset on wine quality, and we are going to only look at red wines. The data fields include: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide,total sulfur dioxide density, pH, sulphates, alcohol, quality.\nTo load a dataset in Numpy, we can use the genfromtxt() function. We can specify data file name, delimiter (which is optional but often used), and number of rows to skip if we have a header row, hence it is 1 here\nThe genfromtxt() function has a parameter called dtype for specifying data types of each column this parameter is optional. Without specifying the types, all types will be casted the same to the more general/precise type.\n\nwines = np.genfromtxt(\"../data/week1/winequality-red.csv\", delimiter=\";\", skip_header=1)\nwines\n\narray([[ 7.4  ,  0.7  ,  0.   , ...,  0.56 ,  9.4  ,  5.   ],\n       [ 7.8  ,  0.88 ,  0.   , ...,  0.68 ,  9.8  ,  5.   ],\n       [ 7.8  ,  0.76 ,  0.04 , ...,  0.65 ,  9.8  ,  5.   ],\n       ...,\n       [ 6.3  ,  0.51 ,  0.13 , ...,  0.75 , 11.   ,  6.   ],\n       [ 5.9  ,  0.645,  0.12 , ...,  0.71 , 10.2  ,  5.   ],\n       [ 6.   ,  0.31 ,  0.47 , ...,  0.66 , 11.   ,  6.   ]])\n\n\nRecall that we can use integer indexing to get a certain column or a row. For example, if we want to select the fixed acidity column, which is the first coluumn, we can do so by entering the index into the array.\nAlso remember that for multidimensional arrays, the first argument refers to the row, and the second argument refers to the column, and if we just give one argument then we’ll get a single dimensional list back.\n\n# So all rows combined but only the first column from them would be\nprint(\"one integer 0 for slicing: \", wines[:, 0])\n\none integer 0 for slicing:  [7.4 7.8 7.8 ... 6.3 5.9 6. ]\n\n\n\n# But if we wanted the same values but wanted to preserve that they sit in their own rows we would write\nprint(\"0 to 1 for slicing: \\n\", wines[:, 0:1])\n\n0 to 1 for slicing: \n [[7.4]\n [7.8]\n [7.8]\n ...\n [6.3]\n [5.9]\n [6. ]]\n\n\nThis is another great example of how the shape of the data is an abstraction which we can layer intentionally on top of the data we are working with.\nIf we want a range of columns in order, say columns 0 through 3 (recall, this means first, second, and third, since we start at zero and don’t include the training index value), we can do that too:\n\nwines[:, 0:3]\n\narray([[7.4  , 0.7  , 0.   ],\n       [7.8  , 0.88 , 0.   ],\n       [7.8  , 0.76 , 0.04 ],\n       ...,\n       [6.3  , 0.51 , 0.13 ],\n       [5.9  , 0.645, 0.12 ],\n       [6.   , 0.31 , 0.47 ]])\n\n\nWhat if we want several non-consecutive columns? We can place the indices of the columns that we want into an array and pass the array as the second argument. Here’s an example:\n\nwines[:, [0,2,4]]\n\narray([[7.4  , 0.   , 0.076],\n       [7.8  , 0.   , 0.098],\n       [7.8  , 0.04 , 0.092],\n       ...,\n       [6.3  , 0.13 , 0.076],\n       [5.9  , 0.12 , 0.075],\n       [6.   , 0.47 , 0.067]])\n\n\nWe can also do some basic summarization of this dataset.\nFor example, if we want to find out the average quality of red wine, we can select the quality column. We could do this in a couple of ways, but the most appropriate is to use the -1 value for the index, as negative numbers mean slicing from the back of the list. We can then call the aggregation functions on this data.\n\nwines[:,-1].mean()\n\n5.6360225140712945\n\n\nLet’s take a look at another dataset, this time on graduate school admissions. It has fields such as GRE score, TOEFL score, university rating, GPA, having research experience or not, and a chance of admission.\nWith this dataset, we can do data manipulation and basic analysis to infer what conditions are associated with higher chance of admission. Let’s take a look.\nWe can specify data field names when using genfromtxt() to load CSV data. Also, we can have numpy try and infer the type of a column by setting the dtype parameter to None:\n\ngraduate_admission = np.genfromtxt('../data/week1/Admission_Predict.csv', dtype=None, delimiter=',', skip_header=1, names=('Serial No','GRE Score', 'TOEFL Score', 'University Rating', 'SOP','LOR','CGPA','Research', 'Chance of Admit'))\n\ngraduate_admission[:2]\n\narray([(1, 337, 118, 4, 4.5, 4.5, 9.65, 1, 0.92),\n       (2, 324, 107, 4, 4. , 4.5, 8.87, 1, 0.76)],\n      dtype=[('Serial_No', '<i8'), ('GRE_Score', '<i8'), ('TOEFL_Score', '<i8'), ('University_Rating', '<i8'), ('SOP', '<f8'), ('LOR', '<f8'), ('CGPA', '<f8'), ('Research', '<i8'), ('Chance_of_Admit', '<f8')])\n\n\nNotice that the resulting array is actually a one-dimensional array with 400 tuples:\n\ngraduate_admission.shape\n\n(400,)\n\n\nWe can retrieve a column from the array using the column’s name for example, let’s get the CGPA column and only the first five values.\n\ngraduate_admission['CGPA'][0:5]\n\narray([9.65, 8.87, 8.  , 8.67, 8.21])\n\n\nSince the GPA in the dataset range from 1 to 10, and in the US it’s more common to use a scale of up to 4, a common task might be to convert the GPA by dividing by 10 and then multiplying by 4\n\ngraduate_admission['CGPA'] = graduate_admission['CGPA'] /10 *4\ngraduate_admission['CGPA'][0:20] #let's get 20 values\n\narray([3.86 , 3.548, 3.2  , 3.468, 3.284, 3.736, 3.28 , 3.16 , 3.2  ,\n       3.44 , 3.36 , 3.6  , 3.64 , 3.2  , 3.28 , 3.32 , 3.48 , 3.2  ,\n       3.52 , 3.4  ])\n\n\nRecall boolean masking. We can use this to find out how many students have had research experience by creating a boolean mask and passing it to the array indexing operator\n\nlen(graduate_admission[graduate_admission['Research'] == 1])\n\n219\n\n\nSince we have the data field chance of admission, which ranges from 0 to 1, we can try to see if students with high chance of admission (>0.8) on average have higher GRE score than those with lower chance of admission (<0.4)\nSo first we use boolean masking to pull out only those students we are interested in based on their chance of admission, then we pull out only their GPA scores, then we print the mean values.\n\nprint(graduate_admission[graduate_admission['Chance_of_Admit'] > 0.8]['GRE_Score'].mean())\nprint(graduate_admission[graduate_admission['Chance_of_Admit'] < 0.4]['GRE_Score'].mean())\n\n328.7350427350427\n302.2857142857143\n\n\nTake a moment to reflect here, do you understand what is happening in these calls?\nWhen we do the boolean masking we are left with an array with tuples in it still, and numpy holds underneath this a list of the columns we specified and their name and indexes\n\ngraduate_admission[graduate_admission['Chance_of_Admit'] > 0.8][:2]\n\narray([(1, 337, 118, 4, 4.5, 4.5, 3.86 , 1, 0.92),\n       (6, 330, 115, 5, 4.5, 3. , 3.736, 1, 0.9 )],\n      dtype=[('Serial_No', '<i8'), ('GRE_Score', '<i8'), ('TOEFL_Score', '<i8'), ('University_Rating', '<i8'), ('SOP', '<f8'), ('LOR', '<f8'), ('CGPA', '<f8'), ('Research', '<i8'), ('Chance_of_Admit', '<f8')])\n\n\n\n# Let's also do this with GPA\nprint(graduate_admission[graduate_admission['Chance_of_Admit'] > 0.8]['CGPA'].mean())\nprint(graduate_admission[graduate_admission['Chance_of_Admit'] < 0.4]['CGPA'].mean())\n\n3.7106666666666666\n3.0222857142857142\n\n\nThe GPA and GRE for students who have a higher chance of being admitted, at least based on our cursory look here, seems to be higher."
  },
  {
    "objectID": "code/3_regular_expressions.html",
    "href": "code/3_regular_expressions.html",
    "title": "4  Regex",
    "section": "",
    "text": "In this lecture we’re going to talk about pattern matching in strings using regular expressions.\nRegular expressions, or regexes, are written in a condensed formatting language. In general, you can think of a regular expression as a pattern which you give to a regex processor with some source data. The processor then parses that source data using that pattern, and returns chunks of text back to the a data scientist or programmer for further manipulation.\nThere’s really three main reasons you would want to do this:\nRegexes are not trivial, but they are a foundational technique for data cleaning in data science applications, and a solid understanding of regexs will help you quickly and efficiently manipulate text data for further data science application.\nNow, you could teach a whole course on regular expressions alone, especially if you wanted to demystify how the regex parsing engine works and efficient mechanisms for parsing text.\nBy the end of this lecture, you will understand the basics of regular expressions, how to define patterns for matching, how to apply these patterns to strings, and how to use the results of those patterns in data processing.\nFinally, a note that in order to best learn regexes you need to write regexes. I encourage you to stop the video at any time and try out new patterns or syntax you learn at any time. Also, a good documentation can be found here.\nThere are several main processing functions in re that you might use. The first, match() checks for a match that is at the beginning of the string and returns a boolean. Similarly, search(), checks for a match anywhere in the string, and returns a boolean.\nIn addition to checking for conditionals, we can segment a string. The work that regex does here is called tokenizing, where the string is separated into substrings based on patterns.\nThe findall() and split() functions will parse the string for us and return chunks. Lets try and example\nYou’ll notice that split has returned an empty string, followed by a number of statements about Amy, all as elements of a list. If we wanted to count how many times we have talked about Amy, we could use findall().\nWe’ve seen that .search() looks for some pattern and returns a boolean, that .split() will use a pattern for creating a list of substrings, and that .findall() will look for a pattern and pull out all occurrences.\nNow that we know how the python regex API works, lets talk about more complex patterns. The regex specification standard defines a markup language to describe patterns in text.\nLets start with anchors. Anchors specify the start and/or the end of the string that you are trying to match. The caret character ^ means start and the dollar sign character $ means end.\nIf you put ^ before a string, it means that the text the regex processor retrieves must start with the string you specify. For ending, you have to put the $ character after the string, it means that the text Regex retrieves must end with the string you specify.\nHere’s an example:\nNotice that re.search() actually returned to us a new object, called re.Match object.\nAn re.Match object always has a boolean value of True, as something was found, so you can always evaluate it in an if statement as we did earlier. The rendering of the match object also tells you what pattern was matched, in this case the word Amy, and the location the match was in, as the span."
  },
  {
    "objectID": "code/3_regular_expressions.html#patterns-and-character-classes",
    "href": "code/3_regular_expressions.html#patterns-and-character-classes",
    "title": "4  Regex",
    "section": "4.1 Patterns and character classes",
    "text": "4.1 Patterns and character classes\nLet’s talk more about patterns and start with character classes. Let’s create a string of a single learners’ grades over a semester in one course across all of their assignments.\nIf we want to answer the question “How many B’s were in the grade list?” we would just use B.\n\ngrades=\"ACAAAABCBCBAA\"\n\nre.findall('B', grades)\n\n['B', 'B', 'B']\n\n\nIf we wanted to count the number of A’s or B’s in the list, we can’t use “AB” since this is used to match all A’s followed immediately by a B. Instead, we put the characters A and B inside square brackets.\n\nre.findall('[AB]', grades)\n\n['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A']\n\n\nThis is called the set operator.\nYou can also include a range of characters, which are ordered alphanumerically.\nFor instance, if we want to refer to all lower case letters we could use [a-z] Lets build a simple regex to parse out all instances where this student receive an A followed by a B or a C.\n\nre.findall('[A][B-C]', grades)\n\n['AC', 'AB']\n\n\nNotice how the [AB] pattern describes a set of possible characters which could be either (A OR B), while the [A][B-C] pattern denoted two sets of characters which must have been matched back to back. You can write this pattern by using the pipe operator, which means OR.\n\nre.findall(\"AB|AC\",grades)\n\n['AC', 'AB']\n\n\nWe can use the caret with the set operator to negate our results. For instance, if we want to parse out only the grades which were not A’s:\n\nre.findall('[^A]', grades)\n\n['C', 'B', 'C', 'B', 'C', 'B']\n\n\nNote this carefully - the caret was previously matched to the beginning of a string as an anchor point, but inside of the set operator the caret, and the other special characters we will be talking about, lose their meaning. This can be a bit confusing.\n\nre.findall('^[^A]', grades)\n\n[]\n\n\nThe code above gives an empty list, because the regex says that we want to match any value at the beginning of the string which is not an A. Our string though starts with an A, so there is no match found. And remember when you are using the set operator you are doing character-based matching. So you are matching individual characters in an OR method."
  },
  {
    "objectID": "code/3_regular_expressions.html#quantifiers",
    "href": "code/3_regular_expressions.html#quantifiers",
    "title": "4  Regex",
    "section": "4.2 Quantifiers",
    "text": "4.2 Quantifiers\nQuantifiers are the number of times you want a pattern to be matched in order to match.\nThe most basic quantifier is expressed as e{m,n}, where e is the expression or character we are matching, m is the minimum number of times you want it to matched, and n is the maximum number of times the item could be matched.\nLet’s use these grades as an example. How many times has this student been on a back-to-back A’s streak?\n\n## we'll use 2 as our min, but ten as our max\nre.findall('A{2,10}', grades)\n\n['AAAA', 'AA']\n\n\nSo we see that there were two streaks, one where the student had four A’s, and one where they had only two A’s.\nWe might try and do this using single values and just repeating the pattern\n\nre.findall('A{1,1}A{1,1}', grades)\n\n['AA', 'AA', 'AA']\n\n\nAs you can see, this is different than the first example. The first pattern is looking for any combination of two A’s up to ten A’s in a row. So it sees four A’s as a single streak.\nThe second pattern is looking for two A’s back to back, so it sees two A’s followed immediately by two more A’s. We say that the regex processor begins at the start of the string and consumes variables which match patterns as it does.\nIt’s important to note that the regex quantifier syntax does not allow you to deviate from the {m,n} pattern. In particular, if you have an extra space in between the braces you’ll get an empty result:\n\nre.findall(\"A{2, 2}\",grades)\n\n[]\n\n\nAnd as we have already seen, if we don’t include a quantifier then the default is {1,1}:\n\nre.findall('AA', grades)\n\n['AA', 'AA', 'AA']\n\n\nIf you just have one number in the braces, it’s considered to be both m and n:\n\nre.findall('A{2}', grades)\n\n['AA', 'AA', 'AA']\n\n\nUsing this, we could find a decreasing trend in a student’s grades\n\nre.findall('A{1,10}B{1,10}C{1,10}', grades)\n\n['AAAABC']\n\n\nNow, that’s a bit of a hack, because we included a maximum that was just arbitrarily large. There are three other quantifiers that are used as short hand:\n\nan asterix *  to match 0 or more times\na question mark ? to match one or more times\na + plus sign to match one or more times.\n\nLets look at a more complex example, and load some data scraped from wikipedia\n\nwith open('../data/week1/ferpa.txt', 'r') as file:\n    wiki = file.read()\n\nwiki\n\n'Overview[edit]\\nFERPA gives parents access to their child\\'s education records, an opportunity to seek to have the records amended, and some control over the disclosure of information from the records. With several exceptions, schools must have a student\\'s consent prior to the disclosure of education records after that student is 18 years old. The law applies only to educational agencies and institutions that receive funds under a program administered by the U.S. Department of Education.\\n\\nOther regulations under this act, effective starting January 3, 2012, allow for greater disclosures of personal and directory student identifying information and regulate student IDs and e-mail addresses.[2] For example, schools may provide external companies with a student\\'s personally identifiable information without the student\\'s consent.[2]\\n\\nExamples of situations affected by FERPA include school employees divulging information to anyone other than the student about the student\\'s grades or behavior, and school work posted on a bulletin board with a grade. Generally, schools must have written permission from the parent or eligible student in order to release any information from a student\\'s education record.\\n\\nThis privacy policy also governs how state agencies transmit testing data to federal agencies, such as the Education Data Exchange Network.\\n\\nThis U.S. federal law also gave students 18 years of age or older, or students of any age if enrolled in any post-secondary educational institution, the right of privacy regarding grades, enrollment, and even billing information unless the school has specific permission from the student to share that specific type of information.\\n\\nFERPA also permits a school to disclose personally identifiable information from education records of an \"eligible student\" (a student age 18 or older or enrolled in a postsecondary institution at any age) to his or her parents if the student is a \"dependent student\" as that term is defined in Section 152 of the Internal Revenue Code. Generally, if either parent has claimed the student as a dependent on the parent\\'s most recent income tax statement, the school may non-consensually disclose the student\\'s education records to both parents.[3]\\n\\nThe law allowed students who apply to an educational institution such as graduate school permission to view recommendations submitted by others as part of the application. However, on standard application forms, students are given the option to waive this right.\\n\\nFERPA specifically excludes employees of an educational institution if they are not students.\\n\\nThe act is also referred to as the Buckley Amendment, for one of its proponents, Senator James L. Buckley of New York.\\n\\nAccess to public records[edit]\\nThe citing of FERPA to conceal public records that are not \"educational\" in nature has been widely criticized, including by the act\\'s primary Senate sponsor.[4] For example, in the Owasso Independent School District v. Falvo case, an important part of the debate was determining the relationship between peer-grading and \"education records\" as defined in FERPA. In the Court of Appeals, it was ruled that students placing grades on the work of other students made such work into an \"education record.\" Thus, peer-grading was determined as a violation of FERPA privacy policies because students had access to other students\\' academic performance without full consent.[5] However, when the case went to the Supreme Court, it was officially ruled that peer-grading was not a violation of FERPA. This is because a grade written on a student\\'s work does not become an \"education record\" until the teacher writes the final grade into a grade book.[6]\\n\\nStudent medical records[edit]\\nLegal experts have debated the issue of whether student medical records (for example records of therapy sessions with a therapist at an on-campus counseling center) might be released to the school administration under certain triggering events, such as when a student sued his college or university.[7][8]\\n\\nUsually, student medical treatment records will remain under the protection of FERPA, not the Health Insurance Portability and Accountability Act (HIPAA). This is due to the \"FERPA Exception\" written within HIPAA.[9]'\n\n\nScanning through this document one of the things we notice is that the headers all have the words [edit] behind them, followed by a newline character. So if we wanted to get a list of all of the headers in this article we could do so using re.findall:\n\nre.findall('[a-zA-Z]{1,100}\\[edit\\]', wiki)\n\n['Overview[edit]', 'records[edit]', 'records[edit]']\n\n\nOk, that didn’t quite work. It got all of the headers, but only the last word of the header, and it really was quite clunky. Lets iteratively improve this. First, we can use \\w to match any word character, which usually means alphanumeric (letters, numbers, regardless of case) plus underscore (_).\n\nre.findall(\"[\\w]{1,100}\\[edit\\]\",wiki)\n\n['Overview[edit]', 'records[edit]', 'records[edit]']\n\n\nThis is something new. \\w is a metacharacter, and indicates a special pattern of any letter or digit. There are actually a number of different metacharacters listed in the documentation. For instance, \\s matches any whitespace character.\nNext, there are three other quantifiers we can use which shorten up the curly brace syntax. We can use an asterix * to match 0 or more times, so let’s try that.\n\nre.findall(\"[\\w]*\\[edit\\]\",wiki)\n\n['Overview[edit]', 'records[edit]', 'records[edit]']\n\n\nNow that we have shortened the regex, let’s improve it a little bit. We can add in a spaces using the space character:\n\nre.findall(\"[\\w ]*\\[edit\\]\",wiki)\n\n['Overview[edit]',\n 'Access to public records[edit]',\n 'Student medical records[edit]']\n\n\nOk, so this gets us the list of section titles in the wikipedia page! You can now create a list of titles by iterating through this and applying another regex:\n\nfor title in re.findall('[\\w ]*\\[edit\\]', wiki):\n    ## Now we will take that intermediate result and split on the square bracket [ just taking the first result\n    print(re.split('[\\[]', title)[0])\n\nprint('')\n\nOverview\nAccess to public records\nStudent medical records"
  },
  {
    "objectID": "code/3_regular_expressions.html#groups",
    "href": "code/3_regular_expressions.html#groups",
    "title": "4  Regex",
    "section": "4.3 Groups",
    "text": "4.3 Groups\nOk, this works, but it’s a bit of a pain. To this point we have been talking about a regex as a single pattern which is matched. But, you can actually match different patterns, called groups, at the same time, and then refer to the groups you want.\nTo group patterns together you use parentheses, which is actually pretty natural. Lets rewrite our findall using groups\n\nre.findall('([\\w ]*)(\\[edit\\])', wiki)\n\n[('Overview', '[edit]'),\n ('Access to public records', '[edit]'),\n ('Student medical records', '[edit]')]\n\n\nNice - we see that the python re module breaks out the result by group. We can actually refer to groups by number as well with the match objects that are returned. But, how do we get back a list of match objects?\nThus far we’ve seen that findall() returns strings, and search() and match() return individual Match objects. But what do we do if we want a list of Match objects? In this case, we use the function finditer()\n\nfor item in re.finditer('([\\w ]*)(\\[edit\\])', wiki):\n    print(item.groups())\n\nprint('')\n\n('Overview', '[edit]')\n('Access to public records', '[edit]')\n('Student medical records', '[edit]')\n\n\n\nWe see here that the groups() method returns a tuple of the group. We can get an individual group using group(number), where group(0) is the whole match, and each other number is the portion of the match we are interested in.\n\nfor item in re.finditer('([\\w ]*)(\\[edit\\])', wiki):\n    print(item.group(1))\n\nprint('')\n\nOverview\nAccess to public records\nStudent medical records\n\n\n\nOne more piece to regex groups that I rarely use but is a good idea is labeling or naming groups.\nIn the previous example I showed you how you can use the position of the group. But giving them a label and looking at the results as a dictionary is pretty useful.\nFor that we use the syntax (?P<name>), where :\n\nthe parenthesis starts the group,\nthe ?P indicates that this is an extension to basic regexes\n<name> is the dictionary key we want to use wrapped in <>.\n\n\nfor item in re.finditer(\"(?P<title>[\\w ]*)(?P<edit_link>\\[edit\\])\",wiki):\n    # We can get the dictionary returned for the item with .groupdict()\n    print(item.groupdict()['title'])\n\nprint('')\n\nOverview\nAccess to public records\nStudent medical records\n\n\n\nOf course, we can print out the whole dictionary for the item too, and see that the [edit] string is still in there. Here’s the dictionary kept for the last match:\n\nitem.groupdict()\n\n{'title': 'Student medical records', 'edit_link': '[edit]'}\n\n\nFinally, there are a number of short hands which are used with regexes for different kinds of characters, including:\n\na . for any single character which is not a newline\na \\d for any digit\nand \\s for any whitespace character, like spaces and tabs\n\nThere are more, and a full list can be found in the python documentation for regexes"
  },
  {
    "objectID": "code/3_regular_expressions.html#look-ahead-and-look-behind",
    "href": "code/3_regular_expressions.html#look-ahead-and-look-behind",
    "title": "4  Regex",
    "section": "4.4 Look-ahead and Look-behind",
    "text": "4.4 Look-ahead and Look-behind\nOne more concept to be familiar with is called “look ahead” and “look behind” matching. In this case, the pattern being given to the regex engine is for text either before or after the text we are trying to isolate.\nFor example, in our headers we want to isolate text which comes before the [edit] rendering, but we actually don’t care about the [edit] text itself. Thus far we have been throwing the [edit] away, but if we want to use them to match but don’t want to capture them we could put them in a group and use look ahead instead with ?= syntax:\nWhat the regex below says is match two groups, the first will be named and called title, will have any amount of whitespace or regular word characters, the second will be the characters [edit] but we don’t actually want this edit put in our output match objects:\n\nfor item in re.finditer('(?P<title>[\\w ]+)(?=\\[edit\\])', wiki):\n    print(item)\n\nprint('')\n\n<re.Match object; span=(0, 8), match='Overview'>\n<re.Match object; span=(2715, 2739), match='Access to public records'>\n<re.Match object; span=(3692, 3715), match='Student medical records'>"
  },
  {
    "objectID": "code/3_regular_expressions.html#example-wiki-data",
    "href": "code/3_regular_expressions.html#example-wiki-data",
    "title": "4  Regex",
    "section": "4.5 Example wiki data:",
    "text": "4.5 Example wiki data:\nLet’s look at some more wikipedia data. Here’s some data on universities in the US which are buddhist-based\n\nwith open('../data/week1/buddhist.txt', 'r') as file:\n    wiki = file.read()\n\n#wiki\n\nWe can see that each university follows a fairly similar pattern, with the name followed by an – then the words “located in” followed by the city and state.\nI’ll actually use this example to show you the verbose mode of python regexes. The verbose mode allows you to write multi-line regexes and increases readability. For this mode, we have to explicitly indicate all whitespace characters, either by prepending them with a \\ or by using the \\s special value. However, this means we can write our regex a bit more like code, and can even include comments with #\n\npattern = '''\n(?P<title>.*)       # the uni title\n(–\\ located\\ in\\ )  # an indicator for the location\n(?P<city>\\w*)       #city the uni is in\n(,\\ )               #separator for the state\n(?P<state>\\w*)      #the state the uni is located in\n'''\n\nNow when we call finditer() we just pass the re.VERBOSE flag as the last parameter, this makes it much easier to understand large regexes!\n\nfor item in re.finditer(pattern, wiki, re.VERBOSE):\n    print(item.groupdict())\n\nprint('')\n\n{'title': 'Dhammakaya Open University ', 'city': 'Azusa', 'state': 'California'}\n{'title': 'Dharmakirti College ', 'city': 'Tucson', 'state': 'Arizona'}\n{'title': 'Dharma Realm Buddhist University ', 'city': 'Ukiah', 'state': 'California'}\n{'title': 'Ewam Buddhist Institute ', 'city': 'Arlee', 'state': 'Montana'}\n{'title': 'Institute of Buddhist Studies ', 'city': 'Berkeley', 'state': 'California'}\n{'title': 'Maitripa College ', 'city': 'Portland', 'state': 'Oregon'}\n{'title': 'University of the West ', 'city': 'Rosemead', 'state': 'California'}\n{'title': 'Won Institute of Graduate Studies ', 'city': 'Glenside', 'state': 'Pennsylvania'}"
  },
  {
    "objectID": "code/3_regular_expressions.html#example-new-york-times-and-hashtags",
    "href": "code/3_regular_expressions.html#example-new-york-times-and-hashtags",
    "title": "4  Regex",
    "section": "4.6 Example: New York Times and Hashtags",
    "text": "4.6 Example: New York Times and Hashtags\nHere’s another example from the New York Times which covers health tweets on news items. This data came from the UC Irvine Machine Learning Repository which is a great source of different kinds of data\n\nwith open(\"../data/week1/nytimeshealth.txt\",\"r\") as file:\n    # We'll read everything into a variable and take a look at it\n    health=file.read()\n    \n#health\nprint('')\n\n\n\n\n'548662191340421120|Sat Dec 27 02:10:34 +0000 2014|Risks in Using Social Media to Spot Signs of Mental Distress http://nyti.ms/1rqi9I1\\n548579831169163265|Fri\nSo here we can see there are tweets with fields separated by pipes |.\nLets try and get a list of all of the hashtags that are included in this data. A hashtag begins with a pound sign (or hash mark) and continues until some whitespace is found.\nSo lets create a pattern. We want to include the hash sign first, then any number of alphanumeric characters. And we end when we see some whitespace\n\npattern = '#[\\w\\d]*(?=\\s)'\n\nNotice that the ending is a look ahead. We’re not actually interested in matching whitespace in the return value. Also notice that I use an asterix * instead of the plus + for the matching of alphabetical characters or digits, because a + would require at least one of each\nLets searching and display all of the hashtags:\n\nre.findall(pattern, health)\n\n['#askwell',\n '#pregnancy',\n '#Colorado',\n '#VegetarianThanksgiving',\n '#FallPrevention',\n '#Ebola',\n '#Ebola',\n '#ebola',\n '#Ebola',\n '#Ebola',\n '#EbolaHysteria',\n '#AskNYT',\n '#Ebola',\n '#Ebola',\n '#Liberia',\n '#Excalibur',\n '#ebola',\n '#Ebola',\n '#dallas',\n '#nobelprize2014',\n '#ebola',\n '#ebola',\n '#monrovia',\n '#ebola',\n '#nobelprize2014',\n '#ebola',\n '#nobelprize2014',\n '#Medicine',\n '#Ebola',\n '#Monrovia',\n '#Ebola',\n '#smell',\n '#Ebola',\n '#Ebola',\n '#Ebola',\n '#Monrovia',\n '#Ebola',\n '#ebola',\n '#monrovia',\n '#liberia',\n '#benzos',\n '#ClimateChange',\n '#Whole',\n '#Wheat',\n '#Focaccia',\n '#Tomatoes',\n '#Olives',\n '#Recipes',\n '#Health',\n '#Ebola',\n '#Monrovia',\n '#Liberia',\n '#Ebola',\n '#Ebola',\n '#Liberia',\n '#Ebola',\n '#blood',\n '#Ebola',\n '#organtrafficking',\n '#EbolaOutbreak',\n '#SierraLeone',\n '#Freetown',\n '#SierraLeone',\n '#ebolaoutbreak',\n '#kenema',\n '#ebola',\n '#Ebola',\n '#ebola',\n '#ebola',\n '#Ebola',\n '#ASMR',\n '#AIDS2014',\n '#AIDS',\n '#MH17',\n '#benzos']"
  },
  {
    "objectID": "code/3_regular_expressions.html#other",
    "href": "code/3_regular_expressions.html#other",
    "title": "4  Regex",
    "section": "4.7 Other",
    "text": "4.7 Other"
  },
  {
    "objectID": "code/4_pandas.html",
    "href": "code/4_pandas.html",
    "title": "5  Pandas",
    "section": "",
    "text": "The series is one of the core data structures in pandas. You think of it a cross between a list and a dictionary. The items are all stored in an order and there’s labels with which you can retrieve them. An easy way to visualize this is two columns of data. The first is the special index, a lot like keys in a dictionary. While the second is your actual data. It’s important to note that the data column has a label of its own and can be retrieved using the .name attribute. This is different than with dictionaries and is useful when it comes to merging multiple columns of data.\n\nimport pandas as pd\nimport numpy as np\nimport timeit\n\nYou can create a series by passing in a list of values. When you do this, Pandas automatically assigns an index starting with zero and sets the name of the series to None. One of the easiest ways to create a series is to use an array-like object, like a list.\n\nstudents = ['Alice', 'Jack', 'Molly']\n\nNow we just call the Series function in pandas and pass in the students:\n\npd.Series(students)\n\n0    Alice\n1     Jack\n2    Molly\ndtype: object\n\n\nThe result is a Series object which is nicely rendered to the screen.\nWe see here that the pandas has automatically identified the type of data in this Series as “object” and set the dytpe parameter as appropriate. We see that the values are indexed with integers, starting at zero.\nWe don’t have to use strings. If we passed in a list of whole numbers, for instance, we could see that panda sets the type to int64. Underneath panda stores series values in a typed array using the Numpy library. This offers significant speedup when processing data versus traditional python lists.\n\nnumbers = [1,2,3]\npd.Series(numbers)\n\n0    1\n1    2\n2    3\ndtype: int64\n\n\nAnd we see on my architecture that the result is a dtype of int64 objects"
  },
  {
    "objectID": "code/4_pandas.html#missing-data",
    "href": "code/4_pandas.html#missing-data",
    "title": "5  Pandas",
    "section": "5.2 Missing data",
    "text": "5.2 Missing data\nThere’s some other typing details that exist for performance that are important to know. The most important is how Numpy and thus pandas handle missing data.\nIn Python, we have the none type to indicate a lack of data. But what do we do if we want to have a typed list like we do in the series object?\nUnderneath, pandas does some type conversion. If we create a list of strings and we have one element, a None type, pandas inserts it as a None and uses the type object for the underlying array.\n\nstudents = ['Alice', 'Jack', None]\npd.Series(students)\n\n0    Alice\n1     Jack\n2     None\ndtype: object\n\n\nHowever, if we create a list of numbers, integers or floats, and put in the None type, pandas automatically converts this to a special floating point value designated as NaN, which stands for “Not a Number”.\n\nnumbers = [1,2,None]\npd.Series(numbers)\n\n0    1.0\n1    2.0\n2    NaN\ndtype: float64\n\n\nYou’ll notice a couple of things:\n\nFirst, NaN is a different value.\nSecond, pandas set the dytpe of this series to floating point numbers instead of object or ints. That’s maybe a bit of a surprise - why not just leave this as an integer?\n\nUnderneath, pandas represents NaN as a floating point number, and because integers can be typecast to floats, pandas went and converted our integers to floats. So when you’re wondering why the list of integers you put into a Series is not floats, it’s probably because there is some missing data.\nIt is important to stress that None and NaN might be being used by the data scientist in the same way, to denote missing data, but that underneath these are not represented by pandas in the same way.\nNaN is NOT equivilent to None and when we try the equality test, the result is False.\n\nnp.nan == None\n\nFalse\n\n\nIt turns out that you actually can’t do an equality test of NAN to itself. When you do, the answer is always False.\n\nnp.nan == np.nan\n\nFalse\n\n\nInstead, you need to use special functions to test for the presence of not a number, such as the Numpy library isnan().\n\nnp.isnan(np.nan)\n\nTrue\n\n\nSo keep in mind when you see NaN, it’s meaning is similar to None, but it’s a numeric value and treated differently for efficiency reasons."
  },
  {
    "objectID": "code/4_pandas.html#creating-series-from-dictionaries",
    "href": "code/4_pandas.html#creating-series-from-dictionaries",
    "title": "5  Pandas",
    "section": "5.3 Creating Series from dictionaries",
    "text": "5.3 Creating Series from dictionaries\nA series can be created directly from dictionary data. If you do this, the index is automatically assigned to the keys of the dictionary that you provided and not just incrementing integers.\n\nstudents_scores = {'Alice': 'Physics',\n                   'Jack': 'Chemistry',\n                   'Molly': 'English'}\ns = pd.Series(students_scores)\ns\n\nAlice      Physics\nJack     Chemistry\nMolly      English\ndtype: object\n\n\nWe see that, since it was string data, pandas set the data type of the series to “object”. We see that the index, the first column, is also a list of strings.\nOnce the series has been created, we can get the index object using the index attribute.\n\ns.index\n\nIndex(['Alice', 'Jack', 'Molly'], dtype='object')\n\n\nAs you play more with pandas you’ll notice that a lot of things are implemented as numpy arrays, and have the dtype value set. This is true of indicies, and here pandas inferred that we were using objects for the index.\nNow, this is kind of interesting. The dtype of object is not just for strings, but for arbitrary objects. Lets create a more complex type of data, say, a list of tuples.\n\nstudents = [(\"Alice\",\"Brown\"), (\"Jack\", \"White\"), (\"Molly\", \"Green\")]\npd.Series(students)\n\n0    (Alice, Brown)\n1     (Jack, White)\n2    (Molly, Green)\ndtype: object\n\n\nWe see that each of the tuples is stored in the series object, and the type is object.\nYou can also separate your index creation from the data by passing in the index as a list explicitly to the series.\n\ns = pd.Series(['Physics', 'Chemistry', 'English'], index=['Alice', 'Jack', 'Molly'])\ns\n\nAlice      Physics\nJack     Chemistry\nMolly      English\ndtype: object\n\n\nSo what happens if your list of values in the index object are not aligned with the keys in your dictionary for creating the series? Well, pandas overrides the automatic creation to favor only and all of the indices values that you provided. So it will ignore from your dictionary all keys which are not in your index, and pandas will add None or NaN type values for any index value you provide, which is not in your dictionary key list.\n\nstudents_scores = {'Alice': 'Physics',\n                   'Jack': 'Chemistry',\n                   'Molly': 'English'}\n\n# When I create the series object though I'll only ask for an index with three students, and I'll exclude Jack\ns = pd.Series(students_scores, index=['Alice', 'Molly', 'Sam'])\ns\n\nAlice    Physics\nMolly    English\nSam          NaN\ndtype: object"
  },
  {
    "objectID": "code/4_pandas.html#querying-a-series",
    "href": "code/4_pandas.html#querying-a-series",
    "title": "5  Pandas",
    "section": "5.4 Querying a Series",
    "text": "5.4 Querying a Series\nA pandas Series can be queried either by the index position or the index label. If you don’t give an index to the series when querying, the position and the label are effectively the same values.\n\nTo query by numeric location, starting at zero, use the iloc attribute.\nTo query by the index label, you can use the loc attribute.\n\n\nstudents_classes = {'Alice': 'Physics',\n                   'Jack': 'Chemistry',\n                   'Molly': 'English',\n                   'Sam': 'History'}\n\ns = pd.Series(students_classes)\ns\n\nAlice      Physics\nJack     Chemistry\nMolly      English\nSam        History\ndtype: object\n\n\nSo, for this series, if you wanted to see the fourth entry we would we would use the iloc attribute with the parameter 3.\n\ns.iloc[3]\n\n'History'\n\n\nIf you wanted to see what class Molly has, we would use the loc attribute with a parameter of Molly.\n\ns.loc['Molly']\n\n'English'\n\n\nKeep in mind that iloc and loc are not methods, they are attributes. So you don’t use parentheses to query them, but square brackets instead, which is called the indexing operator.\nPandas tries to make our code a bit more readable and provides a sort of smart syntax using the indexing operator directly on the series itself. For instance, if you pass in an integer parameter, the operator will behave as if you want it to query via the iloc attribute\n\ns[3]\n\n'History'\n\n\nIf you pass in an object, it will query as if you wanted to use the label based loc attribute.\n\ns['Molly']\n\n'English'\n\n\nSo what happens if your index is a list of integers? This is a bit complicated and Pandas can’t determine automatically whether you’re intending to query by index position or index label. So you need to be careful when using the indexing operator on the Series itself. The safer option is to be more explicit and use the iloc or loc attributes directly.\n\nclass_code = {99: 'Physics',\n              100: 'Chemistry',\n              101: 'English',\n              102: 'History'}\ns = pd.Series(class_code)\ns\n\n99       Physics\n100    Chemistry\n101      English\n102      History\ndtype: object\n\n\nIf we try and call s[0] we get a key error because there’s no item in the classes list with an index of zero, instead we have to call iloc explicitly if we want the first item.\n\n#s[0]\n\nThe code above will give us a KeyError: 0\n\ns.iloc[0]\n\n'Physics'\n\n\nNow we know how to get data out of the series, let’s talk about working with the data. A common task is to want to consider all of the values inside of a series and do some sort of operation. This could be trying to find a certain number, or summarizing data or transforming the data in some way.\nA typical programmatic approach to this would be to iterate over all the items in the series, and invoke the operation one is interested in. For instance, we could create a Series of integers representing student grades, and just try and get an average grade\n\ngrades = pd.Series([90, 80, 70, 60])\ngrades\n\n0    90\n1    80\n2    70\n3    60\ndtype: int64\n\n\n\ntotal = 0\n\nfor grade in grades:\n    total += grade\n\nprint(total/len(grades))\n\n75.0\n\n\nThis works, but it’s slow. Modern computers can do many tasks simultaneously, especially, but not only, tasks involving mathematics.\nPandas and the underlying numpy libraries support a method of computation called vectorization. Vectorization works with most of the functions in the numpy library, including the sum function.\n\ntotal = np.sum(grades)\nprint(total/len(grades))\n\n75.0\n\n\nNow both of these methods create the same value, but is one actually faster? The Jupyter Notebook has a magic function which can help.\n\nnumbers = pd.Series(np.random.randint(0,1000,10000))\n\n#look at the first 5 items\nnumbers.head()\n\n0    368\n1    720\n2    203\n3    686\n4    962\ndtype: int64\n\n\n\n#control length of series\nlen(numbers)\n\n10000\n\n\nOk, we’re confident now that we have a big series. The ipython interpreter has something called magic functions begin with a percentage sign. If we type this sign and then hit the Tab key, you can see a list of the available magic functions. You could write your own magic functions too, but that’s a little bit outside of the scope of this course.\nHere, we’re actually going to use what’s called a cellular magic function. These start with two percentage signs and wrap the code in the current Jupyter cell. The function we’re going to use is called timeit. This function will run our code a few times to determine, on average, how long it takes.\nLet’s run timeit with our original iterative code. You can give timeit the number of loops that you would like to run. By default, it is 1,000 loops. I’ll ask timeit here to use 100 runs because we’re recording this. Note that in order to use a cellular magic function, it has to be the first line in the cell\n\n%%timeit -n 100\n\ntotal = 0\nfor number in numbers:\n    total += number\n\ntotal/len(numbers)\n\n1.14 ms ± 22.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit -n 100\ntotal = np.sum(numbers)\ntotal/len(numbers)\n\n53.3 µs ± 3.61 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThis is a pretty shocking difference in the speed and demonstrates why one should be aware of parallel computing features and start thinking in functional programming terms.\nPut more simply, vectorization is the ability for a computer to execute multiple instructions at once, and with high performance chips, especially graphics cards, you can get dramatic speedups. Modern graphics cards can run thousands of instructions in parallel.\nA Related feature in pandas and nummy is called broadcasting. With broadcasting, you can apply an operation to every value in the series, changing the series. For instance, if we wanted to increase every random variable by 2, we could do so quickly using the += operator directly on the Series object.\n\nnumbers.head()\n\n0    368\n1    720\n2    203\n3    686\n4    962\ndtype: int64\n\n\n\nnumbers += 2\nnumbers.head()\n\n0    370\n1    722\n2    205\n3    688\n4    964\ndtype: int64\n\n\nThe procedural way of doing this would be to iterate through all of the items in the series and increase the values directly. Pandas does support iterating through a series much like a dictionary, allowing you to unpack values easily.\nWe can use the iteritems() function which returns a label and value\nPandas.iat(): allows to access a single value for a row/column pair by integer position.\nSelection with .at is nearly identical to .loc but it only selects a single ‘cell’ in your DataFrame/Series. We usually refer to this cell as a scalar value.\n\nloc: label based, only works on index\niloc: position based\nat: label based, gets scalar values. It’s a very fast loc; Cannot operate on array indexers. Can assign new indices and columns\niat: position based, gets scalar values. It’s a very fast iloc, Cannot work in array indexers. Cannot! assign new indices and columns.\n\n\nfor label, value in numbers.iteritems():\n    # in the early version of pandas we would use the set_value() function\n    # in the current version, we use the iat() or at() functions,\n    numbers.iat[label] = value + 2\n    #numbers.iloc[label] = value + 2\n\nnumbers.head()\n\n0    372\n1    724\n2    207\n3    690\n4    966\ndtype: int64\n\n\nLet’s compare the speed:\n\n%%timeit -n 10\n\n# we'll create a blank new series of items to deal with\ns = pd.Series(np.random.randint(0,1000,1000))\n\n# And we'll just rewrite our loop from above.\nfor label, value in s.iteritems():\n    s.loc[label]= value+2\n\n44.5 ms ± 5.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit -n 10\n\n# We need to recreate a series\ns = pd.Series(np.random.randint(0,1000,1000))\n\n# And we just broadcast with +=\ns+=2\n\n255 µs ± 89.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nNot only is it significantly faster, but it’s more concise and even easier to read too. The typical mathematical operations you would expect are vectorized, and the nump documentation outlines what it takes to create vectorized functions of your own\nOne last note on using the indexing operators to access series data. **The .loc attribute lets you not only modify data in place, but also add new data as well((. If the value you pass in as the index doesn’t exist, then a new entry is added. And keep in mind, indices can have mixed types. While it’s important to be aware of the typing going on underneath, Pandas will automatically change the underlying NumPy types as appropriate.\n\ns = pd.Series([1, 2, 3])\n\n#add a new value\ns.loc['History'] = 102\n\ns\n\n0            1\n1            2\n2            3\nHistory    102\ndtype: int64\n\n\nWe see that mixed types for data values or index labels are no problem for Pandas. Since “History” is not in the original list of indices, s.loc[‘History’] essentially creates a new element in the series, with the index named “History”, and the value of 102\nUp until now I’ve shown only examples of a series where the index values were unique. I want to end this lecture by showing an example where index values are not unique, and this makes pandas Series a little different conceptually then, for instance, a relational database.\n\n# Lets create a Series with students and the courses which they have taken\nstudents_classes = pd.Series({'Alice': 'Physics',\n                   'Jack': 'Chemistry',\n                   'Molly': 'English',\n                   'Sam': 'History'})\nstudents_classes\n\nAlice      Physics\nJack     Chemistry\nMolly      English\nSam        History\ndtype: object\n\n\n\n# Now lets create a Series just for some new student Kelly, which lists all of the courses she has taken.\n#We'll set the index to Kelly, and the data to be the names of courses.\nkelly_classes = pd.Series(['Philosophy', 'Arts', 'Math'], index=['Kelly', 'Kelly', 'Kelly'])\nkelly_classes\n\nKelly    Philosophy\nKelly          Arts\nKelly          Math\ndtype: object"
  },
  {
    "objectID": "code/4_pandas.html#appending-series",
    "href": "code/4_pandas.html#appending-series",
    "title": "5  Pandas",
    "section": "5.5 Appending Series",
    "text": "5.5 Appending Series\nFinally, we can append all of the data in this new Series to the first using the .append() function.\n\nall_students_classes = students_classes.append(kelly_classes)\nall_students_classes\n\nAlice       Physics\nJack      Chemistry\nMolly       English\nSam         History\nKelly    Philosophy\nKelly          Arts\nKelly          Math\ndtype: object\n\n\nThere are a couple of important considerations when using append.\n\nFirst, Pandas will take the series and try to infer the best data types to use. In this example, everything is a string, so there’s no problems here.\nSecond, the append method doesn’t actually change the underlying Series objects, it instead returns a new series which is made up of the two appended together. This is a common pattern in pandas - by default returning a new object instead of modifying in place - and one you should come to expect. By printing the original series we can see that that series hasn’t changed.\n\n\nstudents_classes\n\nAlice      Physics\nJack     Chemistry\nMolly      English\nSam        History\ndtype: object\n\n\nFinally, we see that when we query the appended series for Kelly, we don’t get a single value, but a series itself.\n\nall_students_classes['Kelly']\n\nKelly    Philosophy\nKelly          Arts\nKelly          Math\ndtype: object"
  },
  {
    "objectID": "code/4_pandas.html#dataframes",
    "href": "code/4_pandas.html#dataframes",
    "title": "5  Pandas",
    "section": "5.6 Dataframes",
    "text": "5.6 Dataframes\nThe DataFrame data structure is the heart of the Panda’s library. It’s a primary object that you’ll be working with in data analysis and cleaning tasks.\nThe DataFrame is conceptually a two-dimensional series object, where there’s an index and multiple columns of content, with each column having a label. In fact, the distinction between a column and a row is really only a conceptual distinction. And you can think of the DataFrame itself as simply a two-axes labeled array.\n\nrecord1 = pd.Series({'Name': 'Alice',\n                        'Class': 'Physics',\n                        'Score': 85})\nrecord2 = pd.Series({'Name': 'Jack',\n                        'Class': 'Chemistry',\n                        'Score': 82})\nrecord3 = pd.Series({'Name': 'Helen',\n                        'Class': 'Biology',\n                        'Score': 90})\n\nLike a Series, the DataFrame object is index. Here I’ll use a group of series, where each series represents a row of data. Just like the Series function, we can pass in our individual items in an array, and we can pass in our index values as a second arguments\n\ndf = pd.DataFrame([record1, record2, record3],\n    index = ['school1', 'school2', 'school1'])\n    \ndf.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Class\n      Score\n    \n  \n  \n    \n      school1\n      Alice\n      Physics\n      85\n    \n    \n      school2\n      Jack\n      Chemistry\n      82\n    \n    \n      school1\n      Helen\n      Biology\n      90\n    \n  \n\n\n\n\nYou’ll notice here that Jupyter creates a nice bit of HTML to render the results of the dataframe. So we have the index, which is the leftmost column and is the school name, and then we have the rows of data, where each row has a column header which was given in our initial record dictionaries.\nAn alternative method is that you could use a list of dictionaries, where each dictionary represents a row of data.\n\nstudents = [{'Name': 'Alice',\n              'Class': 'Physics',\n              'Score': 85},\n            {'Name': 'Jack',\n             'Class': 'Chemistry',\n             'Score': 82},\n            {'Name': 'Helen',\n             'Class': 'Biology',\n             'Score': 90}]\n\n# Then we pass this list of dictionaries into the DataFrame function\ndf = pd.DataFrame(students, index=['school1', 'school2', 'school1'])\n# And lets print the head again\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Class\n      Score\n    \n  \n  \n    \n      school1\n      Alice\n      Physics\n      85\n    \n    \n      school2\n      Jack\n      Chemistry\n      82\n    \n    \n      school1\n      Helen\n      Biology\n      90\n    \n  \n\n\n\n\nSimilar to the series, we can extract data using the .iloc and .loc attributes. Because the DataFrame is two-dimensional, passing a single value to the loc indexing operator will return the series if there’s only one row to return.\nFor instance, if we wanted to select data associated with school2, we would just query the .loc attribute with one parameter.\n\ndf.loc['school2']\n\nName          Jack\nClass    Chemistry\nScore           82\nName: school2, dtype: object\n\n\nYou’ll note that the name of the series is returned as the index value, while the column name is included in the output. We can check the data type of the return using the python type function.\n\ntype(df.loc['school2'])\n\npandas.core.series.Series\n\n\nIt’s important to remember that the indices and column names along either axes horizontal or vertical, could be non-unique. In this example, we see two records for school1 as different rows.\nIf we use a single value with the DataFrame lock attribute, multiple rows of the DataFrame will return, not as a new series, but as a new DataFrame.\n\ndf.loc['school1']\n\n\n\n\n\n  \n    \n      \n      Name\n      Class\n      Score\n    \n  \n  \n    \n      school1\n      Alice\n      Physics\n      85\n    \n    \n      school1\n      Helen\n      Biology\n      90\n    \n  \n\n\n\n\n\n# And we can see the the type of this is different too\ntype(df.loc['school1'])\n\npandas.core.frame.DataFrame\n\n\nOne of the powers of the Panda’s DataFrame is that you can quickly select data based on multiple axes.For instance, if you wanted to just list the student names for school1, you would supply two parameters to .loc, one being the row index and the other being the column name.\n\ndf.loc['school1', 'Name']\n\nschool1    Alice\nschool1    Helen\nName: Name, dtype: object\n\n\nRemember, just like the Series, the pandas developers have implemented this using the indexing operator and not as parameters to a function.\nWhat would we do if we just wanted to select a single column though? Well, there are a few mechanisms. Firstly, we could transpose the matrix. This pivots all of the rows into columns and all of the columns into rows, and is done with the T attribute\n\ndf.T\n\n\n\n\n\n  \n    \n      \n      school1\n      school2\n      school1\n    \n  \n  \n    \n      Name\n      Alice\n      Jack\n      Helen\n    \n    \n      Class\n      Physics\n      Chemistry\n      Biology\n    \n    \n      Score\n      85\n      82\n      90\n    \n  \n\n\n\n\nThen we can call .loc on the transpose to get the student names only\n\ndf.T.loc['Name']\n\nschool1    Alice\nschool2     Jack\nschool1    Helen\nName: Name, dtype: object\n\n\nHowever, since iloc and loc are used for row selection, Panda reserves the indexing operator directly on the DataFrame for column selection. In a Panda’s DataFrame, columns always have a name.\nSo this selection is always label based, and is not as confusing as it was when using the square bracket operator on the series objects. For those familiar with relational databases, this operator is analogous to column projection.\n\ndf['Name']\n\nschool1    Alice\nschool2     Jack\nschool1    Helen\nName: Name, dtype: object\n\n\nIn practice, this works really well since you’re often trying to add or drop new columns. However, this also means that you get a key error if you try and use .loc with a column name:\n\n#this gives an error:\n#df.loc['Name']\n\nNote too that the result of a single column projection is a Series object.\n\ntype(df['Name'])\n\npandas.core.series.Series\n\n\nSince the result of using the indexing operator is either a DataFrame or Series, you can chain operations together. For instance, we can select all of the rows which related to school1 using .loc, then project the name column from just those rows.\n\ndf.loc['school1']['Name']\n\nschool1    Alice\nschool1    Helen\nName: Name, dtype: object\n\n\nIf you get confused, use type to check the responses from resulting operations\n\nprint(type(df.loc['school1'])) #should be a DataFrame\nprint(type(df.loc['school1']['Name'])) #should be a Series\n\n<class 'pandas.core.frame.DataFrame'>\n<class 'pandas.core.series.Series'>\n\n\nChaining, by indexing on the return type of another index, can come with some costs and is best avoided if you can use another approach. In particular, chaining tends to cause Pandas to return a copy of the DataFrame instead of a view on the DataFrame. For selecting data, this is not a big deal, though it might be slower than necessary. If you are changing data though this is an important distinction and can be a source of error.\nHere’s another approach. As we saw, .loc does row selection, and it can take two parameters, the row index and the list of column names. The .loc attribute also supports slicing.\nIf we wanted to select all rows, we can use a colon to indicate a full slice from beginning to end. This is just like slicing characters in a list in python. Then we can add the column name as the second parameter as a string. If we wanted to include multiple columns, we could do so in a list. and Pandas will bring back only the columns we have asked for.\n\ndf.loc[:,['Name', 'Score']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Score\n    \n  \n  \n    \n      school1\n      Alice\n      85\n    \n    \n      school2\n      Jack\n      82\n    \n    \n      school1\n      Helen\n      90\n    \n  \n\n\n\n\nTake a look at that again. The colon means that we want to get all of the rows, and the list in the second argument position is the list of columns we want to get back."
  },
  {
    "objectID": "code/4_pandas.html#dropping-data",
    "href": "code/4_pandas.html#dropping-data",
    "title": "5  Pandas",
    "section": "5.7 Dropping data",
    "text": "5.7 Dropping data\nBefore we leave the discussion of accessing data in DataFrames, lets talk about dropping data.\nIt’s easy to delete data in Series and DataFrames, and we can use the drop function to do so. This function takes a single parameter, which is the index or row label, to drop.\nThis is another tricky place for new users – the drop function doesn’t change the DataFrame by default! Instead,the drop function returns to you a copy of the DataFrame with the given rows removed.\n\ndf.drop('school1')\n\n\n\n\n\n  \n    \n      \n      Name\n      Class\n      Score\n    \n  \n  \n    \n      school2\n      Jack\n      Chemistry\n      82\n    \n  \n\n\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Class\n      Score\n    \n  \n  \n    \n      school1\n      Alice\n      Physics\n      85\n    \n    \n      school2\n      Jack\n      Chemistry\n      82\n    \n    \n      school1\n      Helen\n      Biology\n      90\n    \n  \n\n\n\n\nDrop has two interesting optional parameters:\n\nThe first is called inplace, and if it’s set to true, the DataFrame will be updated in place, instead of a copy being returned.\n\nThe second parameter is the axes, which should be dropped. By default, this value is 0, indicating the row axis. But you could change it to 1 if you want to drop a column.\n\nFor example, lets make a copy of a DataFrame using .copy():\n\ncopy_df = df.copy()\n\n#drop the name column of the copy\ncopy_df.drop('Name', inplace = True, axis = 1)\ncopy_df\n\n\n\n\n\n  \n    \n      \n      Class\n      Score\n    \n  \n  \n    \n      school1\n      Physics\n      85\n    \n    \n      school2\n      Chemistry\n      82\n    \n    \n      school1\n      Biology\n      90\n    \n  \n\n\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Class\n      Score\n    \n  \n  \n    \n      school1\n      Alice\n      Physics\n      85\n    \n    \n      school2\n      Jack\n      Chemistry\n      82\n    \n    \n      school1\n      Helen\n      Biology\n      90\n    \n  \n\n\n\n\nThere is a second way to drop a column, and that’s directly through the use of the indexing operator, using the del keyword. This way of dropping data, however, takes immediate effect on the DataFrame and does not return a view.\n\ndel copy_df['Class']\ncopy_df\n\n\n\n\n\n  \n    \n      \n      Score\n    \n  \n  \n    \n      school1\n      85\n    \n    \n      school2\n      82\n    \n    \n      school1\n      90"
  },
  {
    "objectID": "code/4_pandas.html#adding-columns-to-a-df",
    "href": "code/4_pandas.html#adding-columns-to-a-df",
    "title": "5  Pandas",
    "section": "5.8 Adding columns to a df",
    "text": "5.8 Adding columns to a df\nAdding a new column to the DataFrame is as easy as assigning it to some value using the indexing operator. For instance, if we wanted to add a class ranking column with default value of None, we could do so by using the assignment operator after the square brackets. This broadcasts the default value to the new column immediately.\n\ndf['ClassRanking'] = None\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Class\n      Score\n      ClassRanking\n    \n  \n  \n    \n      school1\n      Alice\n      Physics\n      85\n      None\n    \n    \n      school2\n      Jack\n      Chemistry\n      82\n      None\n    \n    \n      school1\n      Helen\n      Biology\n      90\n      None"
  },
  {
    "objectID": "code/4_pandas.html#dataframe-indexing-and-loading",
    "href": "code/4_pandas.html#dataframe-indexing-and-loading",
    "title": "5  Pandas",
    "section": "5.9 DataFrame Indexing and Loading",
    "text": "5.9 DataFrame Indexing and Loading\nA common workflow is to read the dataset in, usually from some external file, then begin to clean and manipulate the dataset for analysis. In this lecture I’m going to demonstrate how you can load data from a comma separated file into a DataFrame.\nLets just jump right in and talk about comma separated values (csv) files.\nNow, I’m going to make a quick aside because it’s convenient here. The Jupyter notebooks use ipython as the kernel underneath, which provides convenient ways to integrate lower level shell commands, which are programs run in the underlying operating system. If you’re not familiar with the shell don’t worry too much about this, but if you are, this is super handy for integration of your data science workflows.\nI want to use one shell command here called “cat”, for “concatenate”, which just outputs the contents of a file. In ipython if we prepend the line with an exclamation mark it will execute the remainder of the line as a shell command. So lets look at the content of a CSV file.\nNotice: Instead of cat, we use head to use head to display the first 10 rows.\n\n!head ../data/week2/Admission_Predict.csv\n\nSerial No.,GRE Score,TOEFL Score,University Rating,SOP,LOR ,CGPA,Research,Chance of Admit \n1,337,118,4,4.5,4.5,9.65,1,0.92\n2,324,107,4,4,4.5,8.87,1,0.76\n3,316,104,3,3,3.5,8,1,0.72\n4,322,110,3,3.5,2.5,8.67,1,0.8\n5,314,103,2,2,3,8.21,0,0.65\n6,330,115,5,4.5,3,9.34,1,0.9\n7,321,109,3,3,4,8.2,1,0.75\n8,308,101,2,3,4,7.9,0,0.68\n9,302,102,1,2,1.5,8,0,0.5\n\n\nWe see from the output that there is a list of columns, and the column identifiers are listed as strings on the first line of the file. Then we have rows of data, all columns separated by commas. We can read in this file using pandas.\n\nimport pandas as pd\n\n#turn csv into a dataframe\ndf = pd.read_csv('../data/week2/Admission_Predict.csv')\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Serial No.\n      GRE Score\n      TOEFL Score\n      University Rating\n      SOP\n      LOR\n      CGPA\n      Research\n      Chance of Admit\n    \n  \n  \n    \n      0\n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      1\n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      2\n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      3\n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      4\n      5\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      0.65\n    \n  \n\n\n\n\nWe notice that by default index starts with 0 while the students’ serial number starts from 1. If you jump back to the CSV output you’ll deduce that pandas has create a new index. Instead, we can set the serial no as the index if we want to by using the index_col.\n\ndf = pd.read_csv('../data/week2/Admission_Predict.csv', index_col=0)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      GRE Score\n      TOEFL Score\n      University Rating\n      SOP\n      LOR\n      CGPA\n      Research\n      Chance of Admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      5\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      0.65"
  },
  {
    "objectID": "code/4_pandas.html#renaming-columns-in-a-df",
    "href": "code/4_pandas.html#renaming-columns-in-a-df",
    "title": "5  Pandas",
    "section": "5.10 Renaming columns in a df",
    "text": "5.10 Renaming columns in a df\nNotice that we have two columns “SOP” and “LOR” and probably not everyone knows what they mean So let’s change our column names to make it more clear. In Pandas, we can use the rename() function.\nIt takes a parameter called columns, and we need to pass into a dictionary which the keys are the old column name and the value is the corresponding new column name:\n\nnew_df=df.rename(columns={'GRE Score':'GRE Score', 'TOEFL Score':'TOEFL Score',\n                   'University Rating':'University Rating', \n                   'SOP': 'Statement of Purpose','LOR': 'Letter of Recommendation',\n                   'CGPA':'CGPA', 'Research':'Research',\n                   'Chance of Admit':'Chance of Admit'})\nnew_df.head()\n\n\n\n\n\n  \n    \n      \n      GRE Score\n      TOEFL Score\n      University Rating\n      Statement of Purpose\n      LOR\n      CGPA\n      Research\n      Chance of Admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      5\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      0.65\n    \n  \n\n\n\n\nFrom the output, we can see that only “SOP” is changed but not “LOR” Why is that? Let’s investigate this a bit. First we need to make sure we got all the column names correct. We can use the columns attribute of dataframe to get a list.\n\nnew_df.columns\n\nIndex(['GRE Score', 'TOEFL Score', 'University Rating', 'Statement of Purpose',\n       'LOR ', 'CGPA', 'Research', 'Chance of Admit '],\n      dtype='object')\n\n\nIf we look at the output closely, we can see that there is actually a space right after “LOR” and a space right after “Chance of Admit”. So this is why our rename dictionary does not work for LOR, because the key we used was just three characters, instead of four characters in “LOR”\nThere are a couple of ways we could address this. One way would be to change a column by including the space in the name:\n\nnew_df = new_df.rename(columns = {'LOR ': 'Letter of Recommendation'})\nnew_df.head()\n\n\n\n\n\n  \n    \n      \n      GRE Score\n      TOEFL Score\n      University Rating\n      Statement of Purpose\n      Letter of Recommendation\n      CGPA\n      Research\n      Chance of Admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      5\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      0.65\n    \n  \n\n\n\n\nSo that works well, but it’s a bit fragile. What if that was a tab instead of a space? Or two spaces?\nAnother way is to create some function that does the cleaning and then tell renamed to apply that function across all of the data. Python comes with a handy string function to strip white space called strip().\nWhen we pass this in to rename we pass the function as the mapper parameter, and then indicate whether the axis should be columns or index (row labels)\n\nnew_df=new_df.rename(mapper=str.strip, axis='columns')\n\nnew_df.columns\n\nIndex(['GRE Score', 'TOEFL Score', 'University Rating', 'Statement of Purpose',\n       'Letter of Recommendation', 'CGPA', 'Research', 'Chance of Admit'],\n      dtype='object')\n\n\nNow we’ve got it - both SOP and LOR have been renamed and Chance of Admit has been trimmed up. Remember though that the rename function isn’t modifying the dataframe. In this case, df is the same as it always was, there’s just a copy in new_df with the changed names.\n\ndf.columns\n\nIndex(['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA',\n       'Research', 'Chance of Admit '],\n      dtype='object')\n\n\nWe can also use the df.columns attribute by assigning to it a list of column names which will directly rename the columns. This will directly modify the original dataframe and is very efficient especially when you have a lot of columns and you only want to change a few. This technique is also not affected by subtle errors in the column names, a problem that we just encountered. With a list, you can use the list index to change a certain value or use list comprehension to change all of the values\nAs an example, lets change all of the column names to lower case. First we need to get our list:\n\n#get a list of our column names\ncols = list(df.columns)\ncols\n\n['GRE Score',\n 'TOEFL Score',\n 'University Rating',\n 'SOP',\n 'LOR ',\n 'CGPA',\n 'Research',\n 'Chance of Admit ']\n\n\n\n#do some cleanup via list comprehension\ncols = [x.lower().strip() for x in cols]\ncols\n\n['gre score',\n 'toefl score',\n 'university rating',\n 'sop',\n 'lor',\n 'cgpa',\n 'research',\n 'chance of admit']\n\n\n\n#overwrite the columns in our df\ndf.columns = cols\ndf.head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n      university rating\n      sop\n      lor\n      cgpa\n      research\n      chance of admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      5\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      0.65"
  },
  {
    "objectID": "code/4_pandas.html#querying-a-dataframe",
    "href": "code/4_pandas.html#querying-a-dataframe",
    "title": "5  Pandas",
    "section": "5.11 Querying a dataframe",
    "text": "5.11 Querying a dataframe\nIn this lecture we’re going to talk about querying DataFrames. The first step in the process is to understand Boolean masking. Boolean masking is the heart of fast and efficient querying in numpy and pandas, and it’s analogous to bit masking used in other areas of computational science.\n\n5.11.1 Boolean masking\nA Boolean mask is an array which can be of one dimension like a series, or two dimensions like a data frame, where each of the values in the array are either true or false. This array is essentially overlaid on top of the data structure that we’re querying. And any cell aligned with the true value will be admitted into our final result, and any cell aligned with a false value will not.\n\nimport pandas as pd\n\n#read in df\ndf = pd.read_csv('../data/week2/Admission_Predict.csv', index_col = 0)\n\n#do cleanup\ndf.columns = [x.lower().strip() for x in df.columns]\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n      university rating\n      sop\n      lor\n      cgpa\n      research\n      chance of admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      5\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      0.65\n    \n  \n\n\n\n\nBoolean masks are created by applying operators directly to the pandas Series or DataFrame objects.\nFor instance, in our graduate admission dataset, we might be interested in seeing only those students that have a chance higher than 0.7 at being admitted.\nTo build a Boolean mask for this query, we want to project the chance of admit column using the indexing operator and apply the greater than operator with a comparison value of 0.7.\nThis is essentially broadcasting a comparison operator, greater than, with the results being returned as a Boolean Series. The resultant Series is indexed where the value of each cell is either True or False depending on whether a student has a chance of admit higher than 0.7.\n\nadmit_mask = df['chance of admit'] > 0.7\nadmit_mask\n\nSerial No.\n1       True\n2       True\n3       True\n4       True\n5      False\n       ...  \n396     True\n397     True\n398     True\n399    False\n400     True\nName: chance of admit, Length: 400, dtype: bool\n\n\nThe result of broadcasting a comparison operator is a Boolean mask - true or false values depending upon the results of the comparison.\nUnderneath, pandas is applying the comparison operator you specified through vectorization (so efficiently and in parallel) to all of the values in the array you specified which, in this case, is the chance of admit column of the dataframe.\nThe result is a series, since only one column is being operator on, filled with either True or False values, which is what the comparison operator returns.\nSo, what do you do with the boolean mask once you have formed it? Well, you can just lay it on top of the data to “hide” the data you don’t want, which is represented by all of the False values. We do this by using the .where() function on the original DataFrame.\n\ndf.where(admit_mask).head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n      university rating\n      sop\n      lor\n      cgpa\n      research\n      chance of admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337.0\n      118.0\n      4.0\n      4.5\n      4.5\n      9.65\n      1.0\n      0.92\n    \n    \n      2\n      324.0\n      107.0\n      4.0\n      4.0\n      4.5\n      8.87\n      1.0\n      0.76\n    \n    \n      3\n      316.0\n      104.0\n      3.0\n      3.0\n      3.5\n      8.00\n      1.0\n      0.72\n    \n    \n      4\n      322.0\n      110.0\n      3.0\n      3.5\n      2.5\n      8.67\n      1.0\n      0.80\n    \n    \n      5\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nWe see that the resulting data frame keeps the original indexed values, and only data which met the condition was retained. All of the rows which did not meet the condition have NaN data instead, but these rows were not dropped from our dataset.\nThe next step is, if we don’t want the NaN data, we use the dropna() function:\n\ndf.where(admit_mask).dropna().head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n      university rating\n      sop\n      lor\n      cgpa\n      research\n      chance of admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337.0\n      118.0\n      4.0\n      4.5\n      4.5\n      9.65\n      1.0\n      0.92\n    \n    \n      2\n      324.0\n      107.0\n      4.0\n      4.0\n      4.5\n      8.87\n      1.0\n      0.76\n    \n    \n      3\n      316.0\n      104.0\n      3.0\n      3.0\n      3.5\n      8.00\n      1.0\n      0.72\n    \n    \n      4\n      322.0\n      110.0\n      3.0\n      3.5\n      2.5\n      8.67\n      1.0\n      0.80\n    \n    \n      6\n      330.0\n      115.0\n      5.0\n      4.5\n      3.0\n      9.34\n      1.0\n      0.90\n    \n  \n\n\n\n\nThe returned DataFrame now has all of the NaN rows dropped. Notice the index now includes one through four and six, but not five.\nDespite being really handy, where() isn’t actually used that often. Instead, the pandas devs created a shorthand syntax which combines where() and dropna(), doing both at once. And, in typical fashion, they just overloaded the indexing operator to do this.\n\ndf[df['chance of admit'] > 0.7].head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n      university rating\n      sop\n      lor\n      cgpa\n      research\n      chance of admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      6\n      330\n      115\n      5\n      4.5\n      3.0\n      9.34\n      1\n      0.90\n    \n  \n\n\n\n\nJust reviewing this indexing operator on DataFrame, it now does three things. First, it can be called with a string parameter to project a single column:\n\ndf['gre score'].head()\n\nSerial No.\n1    337\n2    324\n3    316\n4    322\n5    314\nName: gre score, dtype: int64\n\n\nOr you can send it a list of columns as strings:\n\ndf[['gre score', 'toefl score']].head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n    \n    \n      Serial No.\n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n    \n    \n      2\n      324\n      107\n    \n    \n      3\n      316\n      104\n    \n    \n      4\n      322\n      110\n    \n    \n      5\n      314\n      103\n    \n  \n\n\n\n\nOr you can send it a boolean mask:\n\ndf[df['gre score'] > 320].head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n      university rating\n      sop\n      lor\n      cgpa\n      research\n      chance of admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      6\n      330\n      115\n      5\n      4.5\n      3.0\n      9.34\n      1\n      0.90\n    \n    \n      7\n      321\n      109\n      3\n      3.0\n      4.0\n      8.20\n      1\n      0.75\n    \n  \n\n\n\n\nAnd each of these is mimicing functionality from either .loc() or .where().dropna().\nBefore we leave this, lets talk about combining multiple boolean masks, such as multiple criteria for including. In bitmasking in other places in computer science this is done with “and”, if both masks must be True for a True value to be in the final mask), or “or” if only one needs to be True. Unfortunatly, it doesn’t feel quite as natural in pandas. For instance, if you want to take two boolean series and and them together:\n\n#this gives an error\n#(df['chance of admit'] > 0.7) and (df['chance of admit'] < 0.9)\n\nThe problem is that you have series objects, and python underneath doesn’t know how to compare two series using and or or. Instead, the pandas authors have overwritten the pipe | and ampersand & operators to handle this for us:\n\ndf[(df['chance of admit'] > 0.7) & (df['chance of admit'] < 0.9)].head()\n\n\n\n\n\n  \n    \n      \n      gre score\n      toefl score\n      university rating\n      sop\n      lor\n      cgpa\n      research\n      chance of admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      7\n      321\n      109\n      3\n      3.0\n      4.0\n      8.20\n      1\n      0.75\n    \n    \n      12\n      327\n      111\n      4\n      4.0\n      4.5\n      9.00\n      1\n      0.84\n    \n  \n\n\n\n\nOne thing to watch out for is order of operations! A common error for new pandas users is to try and do boolean comparisons using the & operator but not putting parentheses around the individual terms you are interested in:\ndf['chance of admit'] > 0.7 & df['chance of admit'] < 0.9\nThe problem is that Python is trying to bitwise and, a 0.7 and a pandas dataframe, when you really want to bitwise and the broadcasted dataframes together.\nAnother way to do this is to just get rid of the comparison operator completely, and instead use the built in functions which mimic this approach:\n\ndf['chance of admit'].gt(0.7) & df['chance of admit'].lt(0.9)\n\nSerial No.\n1      False\n2       True\n3       True\n4       True\n5      False\n       ...  \n396     True\n397     True\n398    False\n399    False\n400    False\nName: chance of admit, Length: 400, dtype: bool\n\n\nThese functions are build right into the Series and DataFrame objects, so you can chain them too, which results in the same answer and the use of no visual operators. You can decide what looks best for you.\n\ndf['chance of admit'].gt(0.7).lt(0.9)\n\nSerial No.\n1      False\n2      False\n3      False\n4      False\n5       True\n       ...  \n396    False\n397    False\n398    False\n399     True\n400    False\nName: chance of admit, Length: 400, dtype: bool\n\n\nThis only works if your operator, such as less than or greater than, is built into the DataFrame, but I certainly find that last code example much more readable than one with ampersands and parenthesis.\nYou need to be able to read and write all of these, and understand the implications of the route you are choosing. It’s worth really going back and rewatching this lecture to make sure you have it. I would say 50% or more of the work you’ll be doing in data cleaning involves querying DataFrames."
  },
  {
    "objectID": "code/4_pandas.html#indexing-dataframes",
    "href": "code/4_pandas.html#indexing-dataframes",
    "title": "5  Pandas",
    "section": "5.12 Indexing dataframes",
    "text": "5.12 Indexing dataframes\nAs we’ve seen, both Series and DataFrames can have indices applied to them. The index is essentially a row level label, and in pandas the rows correspond to axis zero.\nIndices can either be either autogenerated, such as when we create a new Series without an index, in which case we get numeric values, or they can be set explicitly, like when we use the dictionary object to create the series, or when we loaded data from the CSV file and set appropriate parameters.\nAnother option for setting an index is to use the set_index() function. This function takes a list of columns and promotes those columns to an index. In this lecture we’ll explore more about how indexes work in pandas.\n\nimport pandas as pd\n\ndf = pd.read_csv('../data/week2/Admission_Predict.csv', index_col = 0)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      GRE Score\n      TOEFL Score\n      University Rating\n      SOP\n      LOR\n      CGPA\n      Research\n      Chance of Admit\n    \n    \n      Serial No.\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      0.92\n    \n    \n      2\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      0.76\n    \n    \n      3\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      0.72\n    \n    \n      4\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      0.80\n    \n    \n      5\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      0.65\n    \n  \n\n\n\n\nLet’s say that we don’t want to index the DataFrame by serial numbers, but instead by the chance of admit. But lets assume we want to keep the serial number for later. So, lets preserve the serial number into a new column. We can do this using the indexing operator on the string that has the column label. Then we can use the set_index to set index of the column to chance of admit:\n\n#cp the indexed data into its own column\ndf['Serial Number'] = df.index\n\n#set the index to another column\ndf = df.set_index('Chance of Admit ')\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      GRE Score\n      TOEFL Score\n      University Rating\n      SOP\n      LOR\n      CGPA\n      Research\n      Serial Number\n    \n    \n      Chance of Admit\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0.92\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      1\n    \n    \n      0.76\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      2\n    \n    \n      0.72\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      3\n    \n    \n      0.80\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      4\n    \n    \n      0.65\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      5\n    \n  \n\n\n\n\nYou’ll see that when we create a new index from an existing column the index has a name, which is the original name of the column.\nWe can get rid of the index completely by calling the function reset_index(). This promotes the index into a column and creates a default numbered index.\n\ndf = df.reset_index()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Chance of Admit\n      GRE Score\n      TOEFL Score\n      University Rating\n      SOP\n      LOR\n      CGPA\n      Research\n      Serial Number\n    \n  \n  \n    \n      0\n      0.92\n      337\n      118\n      4\n      4.5\n      4.5\n      9.65\n      1\n      1\n    \n    \n      1\n      0.76\n      324\n      107\n      4\n      4.0\n      4.5\n      8.87\n      1\n      2\n    \n    \n      2\n      0.72\n      316\n      104\n      3\n      3.0\n      3.5\n      8.00\n      1\n      3\n    \n    \n      3\n      0.80\n      322\n      110\n      3\n      3.5\n      2.5\n      8.67\n      1\n      4\n    \n    \n      4\n      0.65\n      314\n      103\n      2\n      2.0\n      3.0\n      8.21\n      0\n      5\n    \n  \n\n\n\n\nOne nice feature of Pandas is multi-level indexing. This is similar to composite keys in relational database systems. To create a multi-level index, we simply call set index and give it a list of columns that we’re interested in promoting to an index. Pandas will search through these in order, finding the distinct data and form composite indices.\nA good example of this is often found when dealing with geographical data which is sorted by regions or demographics.\nLet’s change data sets and look at some census data for a better example. This data is stored in the file census.csv and comes from the United States Census Bureau. In particular, this is a breakdown of the population level data at the US county level. It’s a great example of how different kinds of data sets might be formatted when you’re trying to clean them.\n\ndf = pd.read_csv('../data/week2/census.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      SUMLEV\n      REGION\n      DIVISION\n      STATE\n      COUNTY\n      STNAME\n      CTYNAME\n      CENSUS2010POP\n      ESTIMATESBASE2010\n      POPESTIMATE2010\n      ...\n      RDOMESTICMIG2011\n      RDOMESTICMIG2012\n      RDOMESTICMIG2013\n      RDOMESTICMIG2014\n      RDOMESTICMIG2015\n      RNETMIG2011\n      RNETMIG2012\n      RNETMIG2013\n      RNETMIG2014\n      RNETMIG2015\n    \n  \n  \n    \n      0\n      40\n      3\n      6\n      1\n      0\n      Alabama\n      Alabama\n      4779736\n      4780127\n      4785161\n      ...\n      0.002295\n      -0.193196\n      0.381066\n      0.582002\n      -0.467369\n      1.030015\n      0.826644\n      1.383282\n      1.724718\n      0.712594\n    \n    \n      1\n      50\n      3\n      6\n      1\n      1\n      Alabama\n      Autauga County\n      54571\n      54571\n      54660\n      ...\n      7.242091\n      -2.915927\n      -3.012349\n      2.265971\n      -2.530799\n      7.606016\n      -2.626146\n      -2.722002\n      2.592270\n      -2.187333\n    \n    \n      2\n      50\n      3\n      6\n      1\n      3\n      Alabama\n      Baldwin County\n      182265\n      182265\n      183193\n      ...\n      14.832960\n      17.647293\n      21.845705\n      19.243287\n      17.197872\n      15.844176\n      18.559627\n      22.727626\n      20.317142\n      18.293499\n    \n    \n      3\n      50\n      3\n      6\n      1\n      5\n      Alabama\n      Barbour County\n      27457\n      27457\n      27341\n      ...\n      -4.728132\n      -2.500690\n      -7.056824\n      -3.904217\n      -10.543299\n      -4.874741\n      -2.758113\n      -7.167664\n      -3.978583\n      -10.543299\n    \n    \n      4\n      50\n      3\n      6\n      1\n      7\n      Alabama\n      Bibb County\n      22915\n      22919\n      22861\n      ...\n      -5.527043\n      -5.068871\n      -6.201001\n      -0.177537\n      0.177258\n      -5.088389\n      -4.363636\n      -5.403729\n      0.754533\n      1.107861\n    \n  \n\n5 rows × 100 columns\n\n\n\nIn this data set there are two summarized levels (SUMLEV), one that contains summary data for the whole country. And one that contains summary data for each state.\nI want to see a list of all the unique values in a given column. In this DataFrame, we see that the possible values for the sum level are using the unique function on the DataFrame. This is similar to the SQL distinct operator.\nHere we can run unique() on the sum level of our current DataFrame :\n\ndf['SUMLEV'].unique()\n\narray([40, 50])\n\n\nWe see that there are only two different values, 40 and 50.\nLet’s exclude all of the rows that are summaries at the state level and just keep the county data.\n\ndf = df[df['SUMLEV'] ==  50]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      SUMLEV\n      REGION\n      DIVISION\n      STATE\n      COUNTY\n      STNAME\n      CTYNAME\n      CENSUS2010POP\n      ESTIMATESBASE2010\n      POPESTIMATE2010\n      ...\n      RDOMESTICMIG2011\n      RDOMESTICMIG2012\n      RDOMESTICMIG2013\n      RDOMESTICMIG2014\n      RDOMESTICMIG2015\n      RNETMIG2011\n      RNETMIG2012\n      RNETMIG2013\n      RNETMIG2014\n      RNETMIG2015\n    \n  \n  \n    \n      1\n      50\n      3\n      6\n      1\n      1\n      Alabama\n      Autauga County\n      54571\n      54571\n      54660\n      ...\n      7.242091\n      -2.915927\n      -3.012349\n      2.265971\n      -2.530799\n      7.606016\n      -2.626146\n      -2.722002\n      2.592270\n      -2.187333\n    \n    \n      2\n      50\n      3\n      6\n      1\n      3\n      Alabama\n      Baldwin County\n      182265\n      182265\n      183193\n      ...\n      14.832960\n      17.647293\n      21.845705\n      19.243287\n      17.197872\n      15.844176\n      18.559627\n      22.727626\n      20.317142\n      18.293499\n    \n    \n      3\n      50\n      3\n      6\n      1\n      5\n      Alabama\n      Barbour County\n      27457\n      27457\n      27341\n      ...\n      -4.728132\n      -2.500690\n      -7.056824\n      -3.904217\n      -10.543299\n      -4.874741\n      -2.758113\n      -7.167664\n      -3.978583\n      -10.543299\n    \n    \n      4\n      50\n      3\n      6\n      1\n      7\n      Alabama\n      Bibb County\n      22915\n      22919\n      22861\n      ...\n      -5.527043\n      -5.068871\n      -6.201001\n      -0.177537\n      0.177258\n      -5.088389\n      -4.363636\n      -5.403729\n      0.754533\n      1.107861\n    \n    \n      5\n      50\n      3\n      6\n      1\n      9\n      Alabama\n      Blount County\n      57322\n      57322\n      57373\n      ...\n      1.807375\n      -1.177622\n      -1.748766\n      -2.062535\n      -1.369970\n      1.859511\n      -0.848580\n      -1.402476\n      -1.577232\n      -0.884411\n    \n  \n\n5 rows × 100 columns\n\n\n\nAlso while this data set is interesting for a number of different reasons, let’s reduce the data that we’re going to look at to just the total population estimates and the total number of births. We can do this by creating a list of column names that we want to keep then project those and assign the resulting DataFrame to our df variable.\n\ncolumns_to_keep = ['STNAME','CTYNAME','BIRTHS2010','BIRTHS2011','BIRTHS2012','BIRTHS2013',\n                   'BIRTHS2014','BIRTHS2015','POPESTIMATE2010','POPESTIMATE2011',\n                   'POPESTIMATE2012','POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015']\ndf = df[columns_to_keep]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      STNAME\n      CTYNAME\n      BIRTHS2010\n      BIRTHS2011\n      BIRTHS2012\n      BIRTHS2013\n      BIRTHS2014\n      BIRTHS2015\n      POPESTIMATE2010\n      POPESTIMATE2011\n      POPESTIMATE2012\n      POPESTIMATE2013\n      POPESTIMATE2014\n      POPESTIMATE2015\n    \n  \n  \n    \n      1\n      Alabama\n      Autauga County\n      151\n      636\n      615\n      574\n      623\n      600\n      54660\n      55253\n      55175\n      55038\n      55290\n      55347\n    \n    \n      2\n      Alabama\n      Baldwin County\n      517\n      2187\n      2092\n      2160\n      2186\n      2240\n      183193\n      186659\n      190396\n      195126\n      199713\n      203709\n    \n    \n      3\n      Alabama\n      Barbour County\n      70\n      335\n      300\n      283\n      260\n      269\n      27341\n      27226\n      27159\n      26973\n      26815\n      26489\n    \n    \n      4\n      Alabama\n      Bibb County\n      44\n      266\n      245\n      259\n      247\n      253\n      22861\n      22733\n      22642\n      22512\n      22549\n      22583\n    \n    \n      5\n      Alabama\n      Blount County\n      183\n      744\n      710\n      646\n      618\n      603\n      57373\n      57711\n      57776\n      57734\n      57658\n      57673\n    \n  \n\n\n\n\nThe US Census data breaks down population estimates by state and county. We can load the data and set the index to be a combination of the state and county values and see how pandas handles it in a DataFrame.\nWe do this by creating a list of the column identifiers we want to have indexed. And then calling set index with this list and assigning the output as appropriate. We see here that we have a dual index, first the state name and second the county name.\n\ndf = df.set_index(['STNAME', 'CTYNAME'])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      \n      BIRTHS2010\n      BIRTHS2011\n      BIRTHS2012\n      BIRTHS2013\n      BIRTHS2014\n      BIRTHS2015\n      POPESTIMATE2010\n      POPESTIMATE2011\n      POPESTIMATE2012\n      POPESTIMATE2013\n      POPESTIMATE2014\n      POPESTIMATE2015\n    \n    \n      STNAME\n      CTYNAME\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Alabama\n      Autauga County\n      151\n      636\n      615\n      574\n      623\n      600\n      54660\n      55253\n      55175\n      55038\n      55290\n      55347\n    \n    \n      Baldwin County\n      517\n      2187\n      2092\n      2160\n      2186\n      2240\n      183193\n      186659\n      190396\n      195126\n      199713\n      203709\n    \n    \n      Barbour County\n      70\n      335\n      300\n      283\n      260\n      269\n      27341\n      27226\n      27159\n      26973\n      26815\n      26489\n    \n    \n      Bibb County\n      44\n      266\n      245\n      259\n      247\n      253\n      22861\n      22733\n      22642\n      22512\n      22549\n      22583\n    \n    \n      Blount County\n      183\n      744\n      710\n      646\n      618\n      603\n      57373\n      57711\n      57776\n      57734\n      57658\n      57673\n    \n  \n\n\n\n\nAn immediate question which comes up is how we can query this DataFrame. We saw previously that the loc attribute of the DataFrame can take multiple arguments. And it could query both the row and the columns.\nWhen you use a MultiIndex, you must provide the arguments in order by the level you wish to query. Inside of the index, each column is called a level and the outermost column is level zero.\nIf we want to see the population results from Washtenaw County in Michigan the state, which is where I live, the first argument would be Michigan and the second would be Washtenaw County:\n\ndf.loc['Michigan', 'Washtenaw County']\n\nBIRTHS2010            977\nBIRTHS2011           3826\nBIRTHS2012           3780\nBIRTHS2013           3662\nBIRTHS2014           3683\nBIRTHS2015           3709\nPOPESTIMATE2010    345563\nPOPESTIMATE2011    349048\nPOPESTIMATE2012    351213\nPOPESTIMATE2013    354289\nPOPESTIMATE2014    357029\nPOPESTIMATE2015    358880\nName: (Michigan, Washtenaw County), dtype: int64\n\n\nIf you are interested in comparing two counties, for example, Washtenaw and Wayne County, we can pass a list of tuples describing the indices we wish to query into loc. Since we have a MultiIndex of two values, the state and the county, we need to provide two values as each element of our filtering list. Each tuple should have two elements, the first element being the first index and the second element being the second index.\nTherefore, in this case, we will have a list of two tuples, in each tuple, the first element is Michigan, and the second element is either Washtenaw County or Wayne County\n\ndf.loc[[('Michigan', 'Washtenaw County'),\n        ('Michigan', 'Wayne County')]]\n\n\n\n\n\n  \n    \n      \n      \n      BIRTHS2010\n      BIRTHS2011\n      BIRTHS2012\n      BIRTHS2013\n      BIRTHS2014\n      BIRTHS2015\n      POPESTIMATE2010\n      POPESTIMATE2011\n      POPESTIMATE2012\n      POPESTIMATE2013\n      POPESTIMATE2014\n      POPESTIMATE2015\n    \n    \n      STNAME\n      CTYNAME\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Michigan\n      Washtenaw County\n      977\n      3826\n      3780\n      3662\n      3683\n      3709\n      345563\n      349048\n      351213\n      354289\n      357029\n      358880\n    \n    \n      Wayne County\n      5918\n      23819\n      23270\n      23377\n      23607\n      23586\n      1815199\n      1801273\n      1792514\n      1775713\n      1766008\n      1759335"
  },
  {
    "objectID": "code/4_pandas.html#missing-values",
    "href": "code/4_pandas.html#missing-values",
    "title": "5  Pandas",
    "section": "5.13 Missing values",
    "text": "5.13 Missing values\nWe’ve seen a preview of how Pandas handles missing values using the None type and NumPy NaN values. Missing values are pretty common in data cleaning activities. And, missing values can be there for any number of reasons, and I just want to touch on a few here.\nFor instance, if you are running a survey and a respondant didn’t answer a question the missing value is actually an omission. This kind of missing data is called Missing at Random if there are other variables that might be used to predict the variable which is missing. In my work when I delivery surveys I often find that missing data, say the interest in being involved in a follow up study, often has some correlation with another data field, like gender or ethnicity. If there is no relationship to other variables, then we call this data Missing Completely at Random (MCAR).\nThese are just two examples of missing data, and there are many more. For instance, data might be missing because it wasn’t collected, either by the process responsible for collecting that data, such as a researcher, or because it wouldn’t make sense if it were collected. This last example is extremely common when you start joining DataFrames together from multiple sources, such as joining a list of people at a university with a list of offices in the university (students generally don’t have offices).\nLet’s look at some ways of handling missing data in pandas.\nPandas is pretty good at detecting missing values directly from underlying data formats, like CSV files. Although most missing valuse are often formatted as NaN, NULL, None, or N/A, sometimes missing values are not labeled so clearly.\nFor example, I’ve worked with social scientists who regularly used the value of 99 in binary categories to indicate a missing value. The pandas read_csv() function has a parameter called na_values to let us specify the form of missing values. It allows scalar, string, list, or dictionaries to be used.\n\ndf = pd.read_csv('../data/week2/class_grades.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Prefix\n      Assignment\n      Tutorial\n      Midterm\n      TakeHome\n      Final\n    \n  \n  \n    \n      0\n      5\n      57.14\n      34.09\n      64.38\n      51.48\n      52.50\n    \n    \n      1\n      8\n      95.05\n      105.49\n      67.50\n      99.07\n      68.33\n    \n    \n      2\n      8\n      83.70\n      83.17\n      NaN\n      63.15\n      48.89\n    \n    \n      3\n      7\n      NaN\n      NaN\n      49.38\n      105.93\n      80.56\n    \n    \n      4\n      8\n      91.32\n      93.64\n      95.00\n      107.41\n      73.89\n    \n  \n\n\n\n\nWe can actually use the function .isnull() to create a boolean mask of the whole dataframe. This effectively broadcasts the isnull() function to every cell of data.\n\nmask = df.isnull()\nmask.head()\n\n\n\n\n\n  \n    \n      \n      Prefix\n      Assignment\n      Tutorial\n      Midterm\n      TakeHome\n      Final\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      1\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      False\n      False\n      False\n      True\n      False\n      False\n    \n    \n      3\n      False\n      True\n      True\n      False\n      False\n      False\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n    \n  \n\n\n\n\nThis can be useful for processing rows based on certain columns of data. Another useful operation is to be able to drop all of those rows which have any missing data, which can be done with the dropna() function.\n\ndf.dropna().head()\n\n\n\n\n\n  \n    \n      \n      Prefix\n      Assignment\n      Tutorial\n      Midterm\n      TakeHome\n      Final\n    \n  \n  \n    \n      0\n      5\n      57.14\n      34.09\n      64.38\n      51.48\n      52.50\n    \n    \n      1\n      8\n      95.05\n      105.49\n      67.50\n      99.07\n      68.33\n    \n    \n      4\n      8\n      91.32\n      93.64\n      95.00\n      107.41\n      73.89\n    \n    \n      5\n      7\n      95.00\n      92.58\n      93.12\n      97.78\n      68.06\n    \n    \n      6\n      8\n      95.05\n      102.99\n      56.25\n      99.07\n      50.00\n    \n  \n\n\n\n\nNote how the rows indexed with 2, 3, 7, and 11 are now gone.\nOne of the handy functions that Pandas has for working with missing values is the filling function, fillna(). This function takes a number or parameters. You could pass in a single value which is called a scalar value to change all of the missing data to one value. This isn’t really applicable in this case, but it’s a pretty common use case. So, if we wanted to fill all missing values with 0, we would use fillna.\n\ndf.fillna(0, inplace = True)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Prefix\n      Assignment\n      Tutorial\n      Midterm\n      TakeHome\n      Final\n    \n  \n  \n    \n      0\n      5\n      57.14\n      34.09\n      64.38\n      51.48\n      52.50\n    \n    \n      1\n      8\n      95.05\n      105.49\n      67.50\n      99.07\n      68.33\n    \n    \n      2\n      8\n      83.70\n      83.17\n      0.00\n      63.15\n      48.89\n    \n    \n      3\n      7\n      0.00\n      0.00\n      49.38\n      105.93\n      80.56\n    \n    \n      4\n      8\n      91.32\n      93.64\n      95.00\n      107.41\n      73.89\n    \n  \n\n\n\n\nNote that the inplace attribute causes pandas to fill the values inline and does not return a copy of the dataframe, but instead modifies the dataframe you have.\nWe can also use the na_filter option to turn off white space filtering, if white space is an actual value of interest. But in practice, this is pretty rare. In data without any NAs, passing na_filter=False, can improve the performance of reading a large file.\nIn addition to rules controlling how missing values might be loaded, it’s sometimes useful to consider missing values as actually having information. I’ll give an example from my own research. I often deal with logs from online learning systems. I’ve looked at video use in lecture capture systems. In these systems it’s common for the player for have a heartbeat functionality where playback statistics are sent to the server every so often, maybe every 30 seconds. These heartbeats can get big as they can carry the whole state of the playback system such as where the video play head is at, where the video size is, which video is being rendered to the screen, how loud the volume is.\nIf we load the data file log.csv, we can see an example of what this might look like.\n\ndf = pd.read_csv('../data/week2/log.csv')\ndf.head(n=10)\n\n\n\n\n\n  \n    \n      \n      time\n      user\n      video\n      playback position\n      paused\n      volume\n    \n  \n  \n    \n      0\n      1469974424\n      cheryl\n      intro.html\n      5\n      False\n      10.0\n    \n    \n      1\n      1469974454\n      cheryl\n      intro.html\n      6\n      NaN\n      NaN\n    \n    \n      2\n      1469974544\n      cheryl\n      intro.html\n      9\n      NaN\n      NaN\n    \n    \n      3\n      1469974574\n      cheryl\n      intro.html\n      10\n      NaN\n      NaN\n    \n    \n      4\n      1469977514\n      bob\n      intro.html\n      1\n      NaN\n      NaN\n    \n    \n      5\n      1469977544\n      bob\n      intro.html\n      1\n      NaN\n      NaN\n    \n    \n      6\n      1469977574\n      bob\n      intro.html\n      1\n      NaN\n      NaN\n    \n    \n      7\n      1469977604\n      bob\n      intro.html\n      1\n      NaN\n      NaN\n    \n    \n      8\n      1469974604\n      cheryl\n      intro.html\n      11\n      NaN\n      NaN\n    \n    \n      9\n      1469974694\n      cheryl\n      intro.html\n      14\n      NaN\n      NaN\n    \n  \n\n\n\n\nIn this data the first column is a timestamp in the Unix epoch format. The next column is the user name followed by a web page they’re visiting and the video that they’re playing. Each row of the DataFrame has a playback position. And we can see that as the playback position increases by one, the time stamp increases by about 30 seconds.\nExcept for user Bob. It turns out that Bob has paused his playback so as time increases the playbackposition doesn’t change. Note too how difficult it is for us to try and derive this knowledge from the data, because it’s not sorted by time stamp as one might expect. This is actually not uncommon on systems which have a high degree of parallelism. There are a lot of missing values in the paused and volume columns. It’s not efficient to send this information across the network if it hasn’t changed. So this articular system just inserts null values into the database if there’s no changes.\nNext up is the method parameter(). The two common fill values are ffill and bfill. ffill is for forward filling and it updates an na value for a particular cell with the value from the previous row.\nbfill is backward filling, which is the opposite of ffill. It fills the missing values with the next valid value.\nIt’s important to note that your data needs to be sorted in order for this to have the effect you might want. Data which comes from traditional database management systems usually has no order guarantee, justlike this data. So be careful.\nIn Pandas we can sort either by index or by values. Here we’ll just promote the time stamp to an index then sort on the index.\n\ndf = df.set_index('time')\ndf = df.sort_index()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      user\n      video\n      playback position\n      paused\n      volume\n    \n    \n      time\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1469974424\n      cheryl\n      intro.html\n      5\n      False\n      10.0\n    \n    \n      1469974424\n      sue\n      advanced.html\n      23\n      False\n      10.0\n    \n    \n      1469974454\n      cheryl\n      intro.html\n      6\n      NaN\n      NaN\n    \n    \n      1469974454\n      sue\n      advanced.html\n      24\n      NaN\n      NaN\n    \n    \n      1469974484\n      cheryl\n      intro.html\n      7\n      NaN\n      NaN\n    \n  \n\n\n\n\nIf we look closely at the output though we’ll notice that the index isn’t really unique.\nTwo users seem to be able to use the system at the same time. Again, a very common case. Let’s reset the index, and use some multi-level indexing on time AND user together instead,promote the user name to a second level of the index to deal with that issue.\n\ndf = df.reset_index()\ndf = df.set_index(['time', 'user'])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      \n      video\n      playback position\n      paused\n      volume\n    \n    \n      time\n      user\n      \n      \n      \n      \n    \n  \n  \n    \n      1469974424\n      cheryl\n      intro.html\n      5\n      False\n      10.0\n    \n    \n      sue\n      advanced.html\n      23\n      False\n      10.0\n    \n    \n      1469974454\n      cheryl\n      intro.html\n      6\n      NaN\n      NaN\n    \n    \n      sue\n      advanced.html\n      24\n      NaN\n      NaN\n    \n    \n      1469974484\n      cheryl\n      intro.html\n      7\n      NaN\n      NaN\n    \n  \n\n\n\n\nNow that we have the data indexed and sorted appropriately, we can fill the missing datas using ffill. It’s good to remember when dealing with missing values so you can deal with individual columns or sets of columns by projecting them. So you don’t have to fix all missing values in one command.\n\ndf = df.fillna(method = 'ffill')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      \n      video\n      playback position\n      paused\n      volume\n    \n    \n      time\n      user\n      \n      \n      \n      \n    \n  \n  \n    \n      1469974424\n      cheryl\n      intro.html\n      5\n      False\n      10.0\n    \n    \n      sue\n      advanced.html\n      23\n      False\n      10.0\n    \n    \n      1469974454\n      cheryl\n      intro.html\n      6\n      False\n      10.0\n    \n    \n      sue\n      advanced.html\n      24\n      False\n      10.0\n    \n    \n      1469974484\n      cheryl\n      intro.html\n      7\n      False\n      10.0\n    \n  \n\n\n\n\nWe can also do customized fill-in to replace values with the replace() function. It allows replacement from several approaches: value-to-value, list, dictionary, regex Let’s generate a simple example:\n\ndf = pd.DataFrame({'A': [1, 1, 2, 3, 4],\n                   'B': [3, 6, 3, 8, 9],\n                   'C': ['a', 'b', 'c', 'd', 'e']})\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      1\n      3\n      a\n    \n    \n      1\n      1\n      6\n      b\n    \n    \n      2\n      2\n      3\n      c\n    \n    \n      3\n      3\n      8\n      d\n    \n    \n      4\n      4\n      9\n      e\n    \n  \n\n\n\n\nWe can replace 1’s with 100, let’s try the value-to-value approach:\n\ndf.replace(1,100)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      100\n      3\n      a\n    \n    \n      1\n      100\n      6\n      b\n    \n    \n      2\n      2\n      3\n      c\n    \n    \n      3\n      3\n      8\n      d\n    \n    \n      4\n      4\n      9\n      e\n    \n  \n\n\n\n\nHow about changing two values? Let’s try the list approach For example, we want to change 1’s to 100 and 3’s to 300:\n\ndf.replace([1,3], [100,300])\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      100\n      300\n      a\n    \n    \n      1\n      100\n      6\n      b\n    \n    \n      2\n      2\n      300\n      c\n    \n    \n      3\n      300\n      8\n      d\n    \n    \n      4\n      4\n      9\n      e\n    \n  \n\n\n\n\nWhat’s really cool about pandas replacement is that it supports regex too! Let’s look at our data from the dataset logs again:\n\ndf = pd.read_csv('../data/week2/log.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      time\n      user\n      video\n      playback position\n      paused\n      volume\n    \n  \n  \n    \n      0\n      1469974424\n      cheryl\n      intro.html\n      5\n      False\n      10.0\n    \n    \n      1\n      1469974454\n      cheryl\n      intro.html\n      6\n      NaN\n      NaN\n    \n    \n      2\n      1469974544\n      cheryl\n      intro.html\n      9\n      NaN\n      NaN\n    \n    \n      3\n      1469974574\n      cheryl\n      intro.html\n      10\n      NaN\n      NaN\n    \n    \n      4\n      1469977514\n      bob\n      intro.html\n      1\n      NaN\n      NaN\n    \n  \n\n\n\n\nTo replace using a regex we make the first parameter to replace the regex pattern we want to match, the second parameter the value we want to emit upon match, and then we pass in a third parameter “regex=True”.\nTake a moment to pause this video and think about this problem: imagine we want to detect all html pages in the “video” column, lets say that just means they end with “.html”, and we want to overwrite that with the keyword “webpage”. How could we accomplish this?\n\ndf.replace(to_replace='.*.html$', value='webpage', regex = True).head()\n\n\n\n\n\n  \n    \n      \n      time\n      user\n      video\n      playback position\n      paused\n      volume\n    \n  \n  \n    \n      0\n      1469974424\n      cheryl\n      webpage\n      5\n      False\n      10.0\n    \n    \n      1\n      1469974454\n      cheryl\n      webpage\n      6\n      NaN\n      NaN\n    \n    \n      2\n      1469974544\n      cheryl\n      webpage\n      9\n      NaN\n      NaN\n    \n    \n      3\n      1469974574\n      cheryl\n      webpage\n      10\n      NaN\n      NaN\n    \n    \n      4\n      1469977514\n      bob\n      webpage\n      1\n      NaN\n      NaN"
  },
  {
    "objectID": "code/4_pandas.html#example-manipulating-a-dataframe",
    "href": "code/4_pandas.html#example-manipulating-a-dataframe",
    "title": "5  Pandas",
    "section": "5.14 Example: Manipulating a dataframe",
    "text": "5.14 Example: Manipulating a dataframe\nIn this lecture I’m going to walk through a basic data cleaning process with you and introduce you to a few more pandas API functions.\n\nimport pandas as pd\n\ndf = pd.read_csv('../data/week2/presidents.csv')\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      #\n      President\n      Born\n      Age atstart of presidency\n      Age atend of presidency\n      Post-presidencytimespan\n      Died\n      Age\n    \n  \n  \n    \n      0\n      1\n      George Washington\n      Feb 22, 1732[a]\n      57 years, 67 daysApr 30, 1789\n      65 years, 10 daysMar 4, 1797\n      2 years, 285 days\n      Dec 14, 1799\n      67 years, 295 days\n    \n    \n      1\n      2\n      John Adams\n      Oct 30, 1735[a]\n      61 years, 125 daysMar 4, 1797\n      65 years, 125 daysMar 4, 1801\n      25 years, 122 days\n      Jul 4, 1826\n      90 years, 247 days\n    \n    \n      2\n      3\n      Thomas Jefferson\n      Apr 13, 1743[a]\n      57 years, 325 daysMar 4, 1801\n      65 years, 325 daysMar 4, 1809\n      17 years, 122 days\n      Jul 4, 1826\n      83 years, 82 days\n    \n    \n      3\n      4\n      James Madison\n      Mar 16, 1751[a]\n      57 years, 353 daysMar 4, 1809\n      65 years, 353 daysMar 4, 1817\n      19 years, 116 days\n      Jun 28, 1836\n      85 years, 104 days\n    \n    \n      4\n      5\n      James Monroe\n      Apr 28, 1758\n      58 years, 310 daysMar 4, 1817\n      66 years, 310 daysMar 4, 1825\n      6 years, 122 days\n      Jul 4, 1831\n      73 years, 67 days\n    \n  \n\n\n\n\nOk, we have some presidents, some dates, I see a bunch of footnotes in the “Born” column which might cause issues. Let’s start with cleaning up that name into firstname and lastname.\nI’m going to tackle this with a regex. So I want to create two new columns and apply a regex to the projection of the “President” column.\nHere’s one solution, we could make a copy of the President column:\n\ndf['First'] = df['President']\n\n#use replace to extract the first name\ndf['First'] = df['First'].replace('[ ].*', '', regex = True)\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      #\n      President\n      Born\n      Age atstart of presidency\n      Age atend of presidency\n      Post-presidencytimespan\n      Died\n      Age\n      First\n    \n  \n  \n    \n      0\n      1\n      George Washington\n      Feb 22, 1732[a]\n      57 years, 67 daysApr 30, 1789\n      65 years, 10 daysMar 4, 1797\n      2 years, 285 days\n      Dec 14, 1799\n      67 years, 295 days\n      George\n    \n    \n      1\n      2\n      John Adams\n      Oct 30, 1735[a]\n      61 years, 125 daysMar 4, 1797\n      65 years, 125 daysMar 4, 1801\n      25 years, 122 days\n      Jul 4, 1826\n      90 years, 247 days\n      John\n    \n    \n      2\n      3\n      Thomas Jefferson\n      Apr 13, 1743[a]\n      57 years, 325 daysMar 4, 1801\n      65 years, 325 daysMar 4, 1809\n      17 years, 122 days\n      Jul 4, 1826\n      83 years, 82 days\n      Thomas\n    \n    \n      3\n      4\n      James Madison\n      Mar 16, 1751[a]\n      57 years, 353 daysMar 4, 1809\n      65 years, 353 daysMar 4, 1817\n      19 years, 116 days\n      Jun 28, 1836\n      85 years, 104 days\n      James\n    \n    \n      4\n      5\n      James Monroe\n      Apr 28, 1758\n      58 years, 310 daysMar 4, 1817\n      66 years, 310 daysMar 4, 1825\n      6 years, 122 days\n      Jul 4, 1831\n      73 years, 67 days\n      James\n    \n  \n\n\n\n\nThat works, but it’s kind of gross. And it’s slow, since we had to make a full copy of a column then go through and update strings. There are a few other ways we can deal with this. Let me show you the most general one first, and that’s called the apply() function. Let’s drop the column we made first:\n\ndel(df['First'])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      #\n      President\n      Born\n      Age atstart of presidency\n      Age atend of presidency\n      Post-presidencytimespan\n      Died\n      Age\n    \n  \n  \n    \n      0\n      1\n      George Washington\n      Feb 22, 1732[a]\n      57 years, 67 daysApr 30, 1789\n      65 years, 10 daysMar 4, 1797\n      2 years, 285 days\n      Dec 14, 1799\n      67 years, 295 days\n    \n    \n      1\n      2\n      John Adams\n      Oct 30, 1735[a]\n      61 years, 125 daysMar 4, 1797\n      65 years, 125 daysMar 4, 1801\n      25 years, 122 days\n      Jul 4, 1826\n      90 years, 247 days\n    \n    \n      2\n      3\n      Thomas Jefferson\n      Apr 13, 1743[a]\n      57 years, 325 daysMar 4, 1801\n      65 years, 325 daysMar 4, 1809\n      17 years, 122 days\n      Jul 4, 1826\n      83 years, 82 days\n    \n    \n      3\n      4\n      James Madison\n      Mar 16, 1751[a]\n      57 years, 353 daysMar 4, 1809\n      65 years, 353 daysMar 4, 1817\n      19 years, 116 days\n      Jun 28, 1836\n      85 years, 104 days\n    \n    \n      4\n      5\n      James Monroe\n      Apr 28, 1758\n      58 years, 310 daysMar 4, 1817\n      66 years, 310 daysMar 4, 1825\n      6 years, 122 days\n      Jul 4, 1831\n      73 years, 67 days\n    \n  \n\n\n\n\nThe apply() function on a dataframe will take some arbitrary function you have written and apply it to either a Series (a single column) or DataFrame across all rows or columns. Lets write a function which just splits a string into two pieces using a single row of data:\n\ndef splitname(row):\n    # The row is a single Series object which is a single row indexed by column values\n    # Let's extract the firstname and create a new entry in the series\n    row['First'] = row['President'].split(' ')[0]\n    row['Last'] = row['President'].split(' ')[-1]\n    # Now we just return the row and the pandas .apply() will take of merging them back into a DataFrame\n    return row\n\n# Now if we apply this to the dataframe indicating we want to apply it across columns\ndf = df.apply(splitname, axis = 'columns')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      #\n      President\n      Born\n      Age atstart of presidency\n      Age atend of presidency\n      Post-presidencytimespan\n      Died\n      Age\n      First\n      Last\n    \n  \n  \n    \n      0\n      1\n      George Washington\n      Feb 22, 1732[a]\n      57 years, 67 daysApr 30, 1789\n      65 years, 10 daysMar 4, 1797\n      2 years, 285 days\n      Dec 14, 1799\n      67 years, 295 days\n      George\n      Washington\n    \n    \n      1\n      2\n      John Adams\n      Oct 30, 1735[a]\n      61 years, 125 daysMar 4, 1797\n      65 years, 125 daysMar 4, 1801\n      25 years, 122 days\n      Jul 4, 1826\n      90 years, 247 days\n      John\n      Adams\n    \n    \n      2\n      3\n      Thomas Jefferson\n      Apr 13, 1743[a]\n      57 years, 325 daysMar 4, 1801\n      65 years, 325 daysMar 4, 1809\n      17 years, 122 days\n      Jul 4, 1826\n      83 years, 82 days\n      Thomas\n      Jefferson\n    \n    \n      3\n      4\n      James Madison\n      Mar 16, 1751[a]\n      57 years, 353 daysMar 4, 1809\n      65 years, 353 daysMar 4, 1817\n      19 years, 116 days\n      Jun 28, 1836\n      85 years, 104 days\n      James\n      Madison\n    \n    \n      4\n      5\n      James Monroe\n      Apr 28, 1758\n      58 years, 310 daysMar 4, 1817\n      66 years, 310 daysMar 4, 1825\n      6 years, 122 days\n      Jul 4, 1831\n      73 years, 67 days\n      James\n      Monroe\n    \n  \n\n\n\n\nPretty questionable as to whether that is less gross, but it achieves the result and I find that I use the apply() function regularly in my work. The pandas series has a couple of other nice convenience functions though, and the next I would like to touch on is called .extract(). Lets drop our firstname and lastname.\n\ndel(df['First'])\ndel(df['Last'])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      #\n      President\n      Born\n      Age atstart of presidency\n      Age atend of presidency\n      Post-presidencytimespan\n      Died\n      Age\n    \n  \n  \n    \n      0\n      1\n      George Washington\n      Feb 22, 1732[a]\n      57 years, 67 daysApr 30, 1789\n      65 years, 10 daysMar 4, 1797\n      2 years, 285 days\n      Dec 14, 1799\n      67 years, 295 days\n    \n    \n      1\n      2\n      John Adams\n      Oct 30, 1735[a]\n      61 years, 125 daysMar 4, 1797\n      65 years, 125 daysMar 4, 1801\n      25 years, 122 days\n      Jul 4, 1826\n      90 years, 247 days\n    \n    \n      2\n      3\n      Thomas Jefferson\n      Apr 13, 1743[a]\n      57 years, 325 daysMar 4, 1801\n      65 years, 325 daysMar 4, 1809\n      17 years, 122 days\n      Jul 4, 1826\n      83 years, 82 days\n    \n    \n      3\n      4\n      James Madison\n      Mar 16, 1751[a]\n      57 years, 353 daysMar 4, 1809\n      65 years, 353 daysMar 4, 1817\n      19 years, 116 days\n      Jun 28, 1836\n      85 years, 104 days\n    \n    \n      4\n      5\n      James Monroe\n      Apr 28, 1758\n      58 years, 310 daysMar 4, 1817\n      66 years, 310 daysMar 4, 1825\n      6 years, 122 days\n      Jul 4, 1831\n      73 years, 67 days\n    \n  \n\n\n\n\nExtract takes a regular expression as input and specifically requires you to set capture groups that correspond to the output columns you are interested in. Let’s write a regular expression that returned groups and just had the firstname and lastname in it, what would that look like?\nHere we match three groups but only return two, the first and the last name. Remember, parenthesis mark groups we want to have returned and ?: marks a group we do not want to be returned\n\npattern = '(^[\\w]*)(?:.* )([\\w]*$)'\n\nNow the extract function is built into the str attribute of the Series object, so we can call it using Series.str.extract(pattern):\n\ndf['President'].str.extract(pattern).head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      George\n      Washington\n    \n    \n      1\n      John\n      Adams\n    \n    \n      2\n      Thomas\n      Jefferson\n    \n    \n      3\n      James\n      Madison\n    \n    \n      4\n      James\n      Monroe\n    \n  \n\n\n\n\nSo that looks pretty nice, other than the column names. But if we name the groups we get named columns out:\n\npattern = '(?P<First>^[\\w]*)(?:.* )(?P<Last>[\\w]*$)'\n\nnames = df['President'].str.extract(pattern)\nnames.head()\n\n\n\n\n\n  \n    \n      \n      First\n      Last\n    \n  \n  \n    \n      0\n      George\n      Washington\n    \n    \n      1\n      John\n      Adams\n    \n    \n      2\n      Thomas\n      Jefferson\n    \n    \n      3\n      James\n      Madison\n    \n    \n      4\n      James\n      Monroe\n    \n  \n\n\n\n\nAnd we can just copy these into our main dataframe if we want to:\n\ndf[\"First\"]=names[\"First\"]\ndf[\"Last\"]=names[\"Last\"]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      #\n      President\n      Born\n      Age atstart of presidency\n      Age atend of presidency\n      Post-presidencytimespan\n      Died\n      Age\n      First\n      Last\n    \n  \n  \n    \n      0\n      1\n      George Washington\n      Feb 22, 1732[a]\n      57 years, 67 daysApr 30, 1789\n      65 years, 10 daysMar 4, 1797\n      2 years, 285 days\n      Dec 14, 1799\n      67 years, 295 days\n      George\n      Washington\n    \n    \n      1\n      2\n      John Adams\n      Oct 30, 1735[a]\n      61 years, 125 daysMar 4, 1797\n      65 years, 125 daysMar 4, 1801\n      25 years, 122 days\n      Jul 4, 1826\n      90 years, 247 days\n      John\n      Adams\n    \n    \n      2\n      3\n      Thomas Jefferson\n      Apr 13, 1743[a]\n      57 years, 325 daysMar 4, 1801\n      65 years, 325 daysMar 4, 1809\n      17 years, 122 days\n      Jul 4, 1826\n      83 years, 82 days\n      Thomas\n      Jefferson\n    \n    \n      3\n      4\n      James Madison\n      Mar 16, 1751[a]\n      57 years, 353 daysMar 4, 1809\n      65 years, 353 daysMar 4, 1817\n      19 years, 116 days\n      Jun 28, 1836\n      85 years, 104 days\n      James\n      Madison\n    \n    \n      4\n      5\n      James Monroe\n      Apr 28, 1758\n      58 years, 310 daysMar 4, 1817\n      66 years, 310 daysMar 4, 1825\n      6 years, 122 days\n      Jul 4, 1831\n      73 years, 67 days\n      James\n      Monroe\n    \n  \n\n\n\n\nIt’s worth looking at the pandas str module for other functions which have been written specifically to clean up strings in DataFrames, and you can find that in the docs in the Working with Text section: https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html\nNow lets move on to clean up that Born column. First, let’s get rid of anything that isn’t in the pattern of Month Day and Year:\n\ndf['Born'] = df['Born'].str.extract('([\\w]{3} [\\w]{1,2}, [\\w]{4})')\ndf['Born'].head()\n\n0    Feb 22, 1732\n1    Oct 30, 1735\n2    Apr 13, 1743\n3    Mar 16, 1751\n4    Apr 28, 1758\nName: Born, dtype: object\n\n\nSo, that cleans up the date format. But I’m going to foreshadow something else here - the type of this column is object, and we know that’s what pandas uses when it is dealing with string. But pandas actually has really interesting date/time features - in fact, that’s one of the reasons Wes McKinney put his efforts into the library, to deal with financial transactions. So if I were building this out, I would actually update this column to the write data type as well:\n\ndf['Born'] = pd.to_datetime(df['Born'])\ndf['Born'].head()\n\n0   1732-02-22\n1   1735-10-30\n2   1743-04-13\n3   1751-03-16\n4   1758-04-28\nName: Born, dtype: datetime64[ns]"
  },
  {
    "objectID": "code/5_pandas2.html",
    "href": "code/5_pandas2.html",
    "title": "6  Data processing with pandas",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "code/5_pandas2.html#merging-dataframes",
    "href": "code/5_pandas2.html#merging-dataframes",
    "title": "6  Data processing with pandas",
    "section": "6.1 Merging dataframes",
    "text": "6.1 Merging dataframes\nIn this lecture we’re going to address how you can bring multiple dataframe objects together, either by merging them horizontally, or by concatenating them vertically.\n\n\n\nVenn Diagram\n\n\nAbove we see a Venn Diagram. A Venn Diagram is traditionally used to show set membership. For example, the circle on the left is the population of students at a university. The circle on the right is the population of staff at a university. And the overlapping region in the middle are all of those students who are also staff. Maybe these students run tutorials for a course, or grade assignments, or engage in running research experiments.\nSo, this diagram shows two populations whom we might have data about, but there is overlap between those populations.\nWhen it comes to translating this to pandas, we can think of the case where we might have these two populations as indices in separate DataFrames, maybe with the label of Person Name. When we want to join the DataFrames together, we have some choices to make. First what if we want a list of all the people regardless of whether they’re staff or student, and all of the information we can get on them? In database terminology, this is called a full outer join. And in set theory, it’s called a union. In the Venn diagram, it represents everyone in any circle.\nHere’s an image of what that would look like in the Venn diagram.\n It’s quite possible though that we only want those people who we have maximum information for, those people who are both staff and students. Maybe being a staff member and a student involves getting a tuition waiver, and we want to calculate the cost of this. In database terminology, this is called an inner join. Or in set theory, the intersection. It is represented in the Venn diagram as the overlapping parts of each circle.\n Let’s have a look at an actual example:\n\n# First we create two DataFrames, staff and students.\nstaff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR'},\n                         {'Name': 'Sally', 'Role': 'Course liasion'},\n                         {'Name': 'James', 'Role': 'Grader'}])\n\n# And lets index these staff by name\nstaff_df = staff_df.set_index('Name')\n\n# Now we'll create a student dataframe\nstudent_df = pd.DataFrame([{'Name': 'James', 'School': 'Business'},\n                           {'Name': 'Mike', 'School': 'Law'},\n                           {'Name': 'Sally', 'School': 'Engineering'}])\n\n# And we'll index this by name too\nstudent_df = student_df.set_index('Name')\n\n# And lets just print out the dataframes\nprint(staff_df.head())\nprint(student_df.head())\n\n                 Role\nName                 \nKelly  Director of HR\nSally  Course liasion\nJames          Grader\n            School\nName              \nJames     Business\nMike           Law\nSally  Engineering\n\n\nThere’s some overlap in these DataFrames in that James and Sally are both students and staff, but Mike and Kelly are not. Importantly, both DataFrames are indexed along the value we want to merge them on, which is called Name.\nIf we want the union of these, we would call merge() passing in the DataFrame on the left and the DataFrame on the right and telling merge that we want it to use an outer join. We want to use the left and right indices as the joining columns.\n\npd.merge(staff_df, student_df, how = 'outer', left_index = True, right_index = True)\n\n\n\n\n\n  \n    \n      \n      Role\n      School\n    \n    \n      Name\n      \n      \n    \n  \n  \n    \n      James\n      Grader\n      Business\n    \n    \n      Kelly\n      Director of HR\n      NaN\n    \n    \n      Mike\n      NaN\n      Law\n    \n    \n      Sally\n      Course liasion\n      Engineering\n    \n  \n\n\n\n\nWe see in the resulting DataFrame that everyone is listed. And since Mike does not have a role, and John does not have a school, those cells are listed as missing values.\nIf we wanted to get the intersection, that is, just those who are a student AND a staff, we could set the how attribute to inner. Again, we set both left and right indices to be true as the joining columns:\n\npd.merge(staff_df, student_df, how = 'inner', left_index = True, right_index = True)\n\n\n\n\n\n  \n    \n      \n      Role\n      School\n    \n    \n      Name\n      \n      \n    \n  \n  \n    \n      Sally\n      Course liasion\n      Engineering\n    \n    \n      James\n      Grader\n      Business\n    \n  \n\n\n\n\nAnd we see the resulting DataFrame has only James and Sally in it. Now there are two other common use cases when merging DataFrames, and both are examples of what we would call set addition.\nThe first is when we would want to get a list of all staff regardless of whether they were students or not. But if they were students, we would want to get their student details as well. To do this we would use a left join. It is important to note the order of dataframes in this function: the first dataframe is the left dataframe and the second is the right:\n\npd.merge(staff_df, student_df, how = 'left', left_index = True, right_index = True)\n\n\n\n\n\n  \n    \n      \n      Role\n      School\n    \n    \n      Name\n      \n      \n    \n  \n  \n    \n      Kelly\n      Director of HR\n      NaN\n    \n    \n      Sally\n      Course liasion\n      Engineering\n    \n    \n      James\n      Grader\n      Business\n    \n  \n\n\n\n\nYou could probably guess what comes next. We want a list of all of the students and their roles if they were also staff. To do this we would do a right join.\n\npd.merge(staff_df, student_df, how = 'right', left_index = True, right_index = True)\n\n\n\n\n\n  \n    \n      \n      Role\n      School\n    \n    \n      Name\n      \n      \n    \n  \n  \n    \n      James\n      Grader\n      Business\n    \n    \n      Mike\n      NaN\n      Law\n    \n    \n      Sally\n      Course liasion\n      Engineering\n    \n  \n\n\n\n\nWe can also do it another way. The merge method has a couple of other interesting parameters. First, you don’t need to use indices to join on, you can use columns as well.\nHere’s an example. Here we have a parameter called “on”, and we can assign a column that both dataframe has as the joining column\nFirst, lets remove our index from both of our dataframes:\n\nstaff_df = staff_df.reset_index()\nstudent_df = student_df.reset_index()\n\n#merge\npd.merge(staff_df, student_df, how = 'right', on = 'Name')\n\n\n\n\n\n  \n    \n      \n      Name\n      Role\n      School\n    \n  \n  \n    \n      0\n      James\n      Grader\n      Business\n    \n    \n      1\n      Mike\n      NaN\n      Law\n    \n    \n      2\n      Sally\n      Course liasion\n      Engineering\n    \n  \n\n\n\n\nSo what happens when we have conflicts between the DataFrames?\nLet’s take a look by creating new staff and student DataFrames that have a location information added to them.\n\nstaff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR', \n                          'Location': 'State Street'},\n                         {'Name': 'Sally', 'Role': 'Course liasion', \n                          'Location': 'Washington Avenue'},\n                         {'Name': 'James', 'Role': 'Grader', \n                          'Location': 'Washington Avenue'}])\n\nstudent_df = pd.DataFrame([{'Name': 'James', 'School': 'Business', \n                            'Location': '1024 Billiard Avenue'},\n                           {'Name': 'Mike', 'School': 'Law', \n                            'Location': 'Fraternity House #22'},\n                           {'Name': 'Sally', 'School': 'Engineering', \n                            'Location': '512 Wilson Crescent'}])\n                            \nprint(staff_df)\nprint(student_df)\n\n    Name            Role           Location\n0  Kelly  Director of HR       State Street\n1  Sally  Course liasion  Washington Avenue\n2  James          Grader  Washington Avenue\n    Name       School              Location\n0  James     Business  1024 Billiard Avenue\n1   Mike          Law  Fraternity House #22\n2  Sally  Engineering   512 Wilson Crescent\n\n\nIn the staff DataFrame, this is an office location where we can find the staff person. And we can see the Director of HR is on State Street, while the two students are on Washington Avenue. But for the student DataFrame, the location information is actually their home address.\nThe merge function preserves this information, but appends an _x or _y to help differentiate between which index went with which column of data. The _x is always the left DataFrame information, and the _y is always the right DataFrame information.\nHere, if we want all the staff information regardless of whether they were students or not. But if they were students, we would want to get their student details as well.Then we can do a left join and on the column of Name:\n\npd.merge(staff_df, student_df, how = 'left', on = 'Name')\n\n\n\n\n\n  \n    \n      \n      Name\n      Role\n      Location_x\n      School\n      Location_y\n    \n  \n  \n    \n      0\n      Kelly\n      Director of HR\n      State Street\n      NaN\n      NaN\n    \n    \n      1\n      Sally\n      Course liasion\n      Washington Avenue\n      Engineering\n      512 Wilson Crescent\n    \n    \n      2\n      James\n      Grader\n      Washington Avenue\n      Business\n      1024 Billiard Avenue\n    \n  \n\n\n\n\nFrom the output, we can see there are columns Location_x and Location_y. Location_x refers to the Location column in the left dataframe, which is staff dataframe and Location_y refers to the Location column in the right dataframe, which is student dataframe.\nBefore we leave merging of DataFrames, let’s talk about multi-indexing and multiple columns. It’s quite possible that the first name for students and staff might overlap, but the last name might not. In this case, we use a list of the multiple columns that should be used to join keys from both dataframes on the on parameter. Recall that the column name(s) assigned to the on parameter needs to exist in both dataframes.\n\nstaff_df = pd.DataFrame([{'First Name': 'Kelly', 'Last Name': 'Desjardins', \n                          'Role': 'Director of HR'},\n                         {'First Name': 'Sally', 'Last Name': 'Brooks', \n                          'Role': 'Course liasion'},\n                         {'First Name': 'James', 'Last Name': 'Wilde', \n                          'Role': 'Grader'}])\n\nstudent_df = pd.DataFrame([{'First Name': 'James', 'Last Name': 'Hammond', \n                            'School': 'Business'},\n                           {'First Name': 'Mike', 'Last Name': 'Smith', \n                            'School': 'Law'},\n                           {'First Name': 'Sally', 'Last Name': 'Brooks', \n                            'School': 'Engineering'}])\n\nprint(staff_df)\nprint(student_df)\n\n  First Name   Last Name            Role\n0      Kelly  Desjardins  Director of HR\n1      Sally      Brooks  Course liasion\n2      James       Wilde          Grader\n  First Name Last Name       School\n0      James   Hammond     Business\n1       Mike     Smith          Law\n2      Sally    Brooks  Engineering\n\n\nAs you see here, James Wilde and James Hammond don’t match on both keys since they have different last names. So we would expect that an inner join doesn’t include these individuals in the output, and only Sally Brooks will be retained.\n\npd.merge(staff_df, student_df, how = 'inner', on = ['First Name', 'Last Name'])\n\n\n\n\n\n  \n    \n      \n      First Name\n      Last Name\n      Role\n      School\n    \n  \n  \n    \n      0\n      Sally\n      Brooks\n      Course liasion\n      Engineering\n    \n  \n\n\n\n\nIf we think of merging as joining “horizontally”, meaning we join on similar values in a column found in two dataframes then concatenating is joining “vertically”, meaning we put dataframes on top or at the bottom of each other.\nLet’s understand this from an example. You have a dataset that tracks some information over the years. And each year’s record is a separate CSV and every CSV of every year’s record has the exactly same columns. What happens if you want to put all the data, from all years’ record, together? You can concatenate them.\nLet’s take a look at the US Department of Education College Scorecard data It has each US university’s data on student completion, student debt, after-graduation income, etc. The data is stored in separate CSV’s with each CSV containing a year’s record Let’s say we want the records from 2011 to 2013 we first create three dataframe, each containing one year’s record. And, because the csv files we’re working with are messy, I want to supress some of the jupyter warning messages and just tell read_csv to ignore bad lines, so I’m going to start the cell with a cell magic called %%capture:\n\n#df_2011 = pd.read_csv(\"../data/week3/MERGED2011_12_PP.csv\", error_bad_lines=False)\ndf_2012 = pd.read_csv(\"../data/week3/MERGED2012_13_PP.csv\", error_bad_lines=False)\ndf_2013 = pd.read_csv(\"../data/week3/MERGED2013_14_PP.csv\", error_bad_lines=False)\n\nWe see that there is a whopping number of columns - more than 1900! We can calculate the length of each dataframe as well:\n\nprint(len(df_2012))\nprint(len(df_2013))\n\n7793\n7804\n\n\nThat’s a bit surprising that the number of schools in the scorecard for 2011 is almost double that of the next two years. But let’s not worry about that. Instead, let’s just put all three dataframes in a list and call that list frames and pass the list into the concat() function Let’s see what it looks like:\n\nframes = [df_2012, df_2013]\n#pd.concat(frames).head()\n\nAs you can see, we have more observations in one dataframe and columns remain the same. If we scroll down to the bottom of the output, we see that there are a total of 30,832 rows after concatenating three dataframes.\nLet’s add the number of rows of the three dataframes and see if the two numbers match:\n\nprint(pd.concat(frames).shape)\nprint(len(df_2012)+len(df_2013))\n\n(15597, 1977)\n15597\n\n\nThe two numbers match! Which means our concatenation is successful. But wait, now that all the data is concatenated together, we don’t know what observations are from what year anymore! Actually the concat function has a parameter that solves such problem with the keys parameter, we can set an extra level of indices, we pass in a list of keys that we want to correspond to the dataframes into the keys parameter:\n\n#print(pd.concat(frames, keys=['2012','2013']).head())\n#print(pd.concat(frames, keys=['2012','2013']).tail())"
  },
  {
    "objectID": "code/6_idioms.html",
    "href": "code/6_idioms.html",
    "title": "7  Panda idioms",
    "section": "",
    "text": "A sort of sub-language within Python, Pandas has its own set of idioms. We’ve alluded to some of these already, such as using vectorization whenever possible, and not using iterative loops if you don’t need to. Several developers and users within the Panda’s community have used the term pandorable for these idioms. I think it’s a great term. So, I wanted to share with you a couple of key features of how you can make your code pandorable."
  },
  {
    "objectID": "code/6_idioms.html#method-chaining",
    "href": "code/6_idioms.html#method-chaining",
    "title": "7  Panda idioms",
    "section": "7.1 Method chaining",
    "text": "7.1 Method chaining\nThe first of these is called method chaining.\nThe general idea behind method chaining is that every method on an object returns a reference to that object. The beauty of this is that you can condense many different operations on a DataFrame, for instance, into one line or at least one statement of code.\nHere’s an example of two pieces of code in pandas using our census data.\nThe first is the pandorable way to write the code with method chaining. In this code, there’s no in place flag being used and you can see that when we first run a where query, then a dropna, then a set_index, and then a rename. You might wonder why the whole statement is enclosed in parentheses and that’s just to make the statement more readable.\n\n(df.where(df['SUMLEV']==50) \\\n    .dropna() \\\n    .set_index(['STNAME', 'CTYNAME']) \\\n    .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'})).head()\n\n\n\n\n\n  \n    \n      \n      \n      SUMLEV\n      REGION\n      DIVISION\n      STATE\n      COUNTY\n      CENSUS2010POP\n      Estimates Base 2010\n      POPESTIMATE2010\n      POPESTIMATE2011\n      POPESTIMATE2012\n      ...\n      RDOMESTICMIG2011\n      RDOMESTICMIG2012\n      RDOMESTICMIG2013\n      RDOMESTICMIG2014\n      RDOMESTICMIG2015\n      RNETMIG2011\n      RNETMIG2012\n      RNETMIG2013\n      RNETMIG2014\n      RNETMIG2015\n    \n    \n      STNAME\n      CTYNAME\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Alabama\n      Autauga County\n      50.0\n      3.0\n      6.0\n      1.0\n      1.0\n      54571.0\n      54571.0\n      54660.0\n      55253.0\n      55175.0\n      ...\n      7.242091\n      -2.915927\n      -3.012349\n      2.265971\n      -2.530799\n      7.606016\n      -2.626146\n      -2.722002\n      2.592270\n      -2.187333\n    \n    \n      Baldwin County\n      50.0\n      3.0\n      6.0\n      1.0\n      3.0\n      182265.0\n      182265.0\n      183193.0\n      186659.0\n      190396.0\n      ...\n      14.832960\n      17.647293\n      21.845705\n      19.243287\n      17.197872\n      15.844176\n      18.559627\n      22.727626\n      20.317142\n      18.293499\n    \n    \n      Barbour County\n      50.0\n      3.0\n      6.0\n      1.0\n      5.0\n      27457.0\n      27457.0\n      27341.0\n      27226.0\n      27159.0\n      ...\n      -4.728132\n      -2.500690\n      -7.056824\n      -3.904217\n      -10.543299\n      -4.874741\n      -2.758113\n      -7.167664\n      -3.978583\n      -10.543299\n    \n    \n      Bibb County\n      50.0\n      3.0\n      6.0\n      1.0\n      7.0\n      22915.0\n      22919.0\n      22861.0\n      22733.0\n      22642.0\n      ...\n      -5.527043\n      -5.068871\n      -6.201001\n      -0.177537\n      0.177258\n      -5.088389\n      -4.363636\n      -5.403729\n      0.754533\n      1.107861\n    \n    \n      Blount County\n      50.0\n      3.0\n      6.0\n      1.0\n      9.0\n      57322.0\n      57322.0\n      57373.0\n      57711.0\n      57776.0\n      ...\n      1.807375\n      -1.177622\n      -1.748766\n      -2.062535\n      -1.369970\n      1.859511\n      -0.848580\n      -1.402476\n      -1.577232\n      -0.884411\n    \n  \n\n5 rows × 98 columns\n\n\n\nLets walk through this:\n\nFirst, we use the where() function on the dataframe and pass in a boolean mask which is only true for those rows where the SUMLEV is equal to 50. This indicates in our source data that the data is summarized at the county level.\nWith the result of the where() function evaluated, we drop missing values. Remember that .where() doesn’t drop missing values by default.\nThen we set an index on the result of that. In this case I’ve set it to the state name followed by the county name.\nFinally, we rename a column to make it more readable.\nNote that instead of writing this all on one line, as I could have done, I began the statement with a parenthesis, which tells python I’m going to span the statement over multiple lines for readability.\n\nThe second example is a more traditional way of writing code.\nThere’s nothing wrong with this code in the functional sense, you might even be able to understand it better as a new person to the language. It’s just not as pandorable as the first example.\n\ndf = df[df['SUMLEV']==50]\ndf.set_index(['STNAME','CTYNAME']).rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}).head()\n\n\n\n\n\n  \n    \n      \n      \n      SUMLEV\n      REGION\n      DIVISION\n      STATE\n      COUNTY\n      CENSUS2010POP\n      Estimates Base 2010\n      POPESTIMATE2010\n      POPESTIMATE2011\n      POPESTIMATE2012\n      ...\n      RDOMESTICMIG2011\n      RDOMESTICMIG2012\n      RDOMESTICMIG2013\n      RDOMESTICMIG2014\n      RDOMESTICMIG2015\n      RNETMIG2011\n      RNETMIG2012\n      RNETMIG2013\n      RNETMIG2014\n      RNETMIG2015\n    \n    \n      STNAME\n      CTYNAME\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Alabama\n      Autauga County\n      50\n      3\n      6\n      1\n      1\n      54571\n      54571\n      54660\n      55253\n      55175\n      ...\n      7.242091\n      -2.915927\n      -3.012349\n      2.265971\n      -2.530799\n      7.606016\n      -2.626146\n      -2.722002\n      2.592270\n      -2.187333\n    \n    \n      Baldwin County\n      50\n      3\n      6\n      1\n      3\n      182265\n      182265\n      183193\n      186659\n      190396\n      ...\n      14.832960\n      17.647293\n      21.845705\n      19.243287\n      17.197872\n      15.844176\n      18.559627\n      22.727626\n      20.317142\n      18.293499\n    \n    \n      Barbour County\n      50\n      3\n      6\n      1\n      5\n      27457\n      27457\n      27341\n      27226\n      27159\n      ...\n      -4.728132\n      -2.500690\n      -7.056824\n      -3.904217\n      -10.543299\n      -4.874741\n      -2.758113\n      -7.167664\n      -3.978583\n      -10.543299\n    \n    \n      Bibb County\n      50\n      3\n      6\n      1\n      7\n      22915\n      22919\n      22861\n      22733\n      22642\n      ...\n      -5.527043\n      -5.068871\n      -6.201001\n      -0.177537\n      0.177258\n      -5.088389\n      -4.363636\n      -5.403729\n      0.754533\n      1.107861\n    \n    \n      Blount County\n      50\n      3\n      6\n      1\n      9\n      57322\n      57322\n      57373\n      57711\n      57776\n      ...\n      1.807375\n      -1.177622\n      -1.748766\n      -2.062535\n      -1.369970\n      1.859511\n      -0.848580\n      -1.402476\n      -1.577232\n      -0.884411\n    \n  \n\n5 rows × 98 columns\n\n\n\nNow, the key with any good idiom is to understand when it isn’t helping you. In this case, you can actually time both methods and see which one runs faster.\nWe can put the approach into a function and pass the function into the timeit function to count the time the parameter number allows us to choose how many times we want to run the function. Here we will just set it to 1:\n\ndef first_approach():\n    global df\n    return (df.where(df['SUMLEV']==50)\n             .dropna()\n             .set_index(['STNAME','CTYNAME'])\n             .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))\n\n# Read in our dataset anew\ndf = pd.read_csv('../data/week3/census.csv')\n    \ntimeit.timeit(first_approach, number=1)\n\n0.03298127200000023\n\n\nNow let’s test the second approach. As we notice, we use our global variable df in the function. However, changing a global variable inside a function will modify the variable even in a global scope and we do not want that to happen in this case. Therefore, for selecting summary levels of 50 only, I create a new dataframe for those records.\nLet’s run this for once and see how fast it is:\n\ndef second_approach():\n    global df\n    new_df = df[df['SUMLEV']==50]\n    new_df.set_index(['STNAME','CTYNAME'], inplace=True)\n    return new_df.rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'})\n\n# Read in our dataset anew\ndf = pd.read_csv('../data/week3/census.csv')\n\ntimeit.timeit(second_approach, number=1)\n\n0.00593096099999979\n\n\nAs you can see, the second approach is much faster! So, this is a particular example of a classic time readability trade off."
  },
  {
    "objectID": "code/6_idioms.html#the-map-idiom",
    "href": "code/6_idioms.html#the-map-idiom",
    "title": "7  Panda idioms",
    "section": "7.2 The map idiom",
    "text": "7.2 The map idiom\nHere’s another pandas idiom. Python has a wonderful function called map, which is sort of a basis for functional programming in the language. When you want to use map in Python, you pass it some function you want called, and some iterable, like a list, that you want the function to be applied to. The results are that the function is called against each item in the list,and there’s a resulting list of all of the evaluations of that function.\nPython has a similar function called applymap. In applymap, you provide some function which should operate on each cell of a DataFrame, and the return set is itself a DataFrame. Now I think applymap is fine, but I actually rarely use it.\nInstead, I find myself often wanting to map across all of the rows in a DataFrame. And pandas has a function that I use heavily there, called apply. Let’s look at an example.\nLet’s take our census DataFrame. In this DataFrame, we have five columns for population estimates. Each column corresponding with one year of estimates. It’s quite reasonable to want to create some new columns for minimum or maximum values, and the apply function is an easy way to do this.\nFirst, we need to write a function which takes in a particular row of data, finds a minimum and maximum values, and returns a new row of data nd returns a new row of data. We’ll call this function min_max, this is pretty straight forward. We can create some small slice of a row by projecting the population columns. Then use the NumPy min and max functions, and create a new series with a label values represent the new values we want to apply.\n\ndef min_max(row):\n    data = row[[\n        'POPESTIMATE2010',\n        'POPESTIMATE2011',\n        'POPESTIMATE2012',\n        'POPESTIMATE2013',\n        'POPESTIMATE2014',\n        'POPESTIMATE2015']]\n    return pd.Series({'min': np.min(data), 'max': np.max(data)})\n\nThen we just need to call apply on the DataFrame.\nApply takes the function and the axis on which to operate as parameters. Now, we have to be a bit careful, we’ve talked about axis zero being the rows of the DataFrame in the past. But this parameter is really the parameter of the index to use. So, to apply across all rows, which is applying on all columns, you pass axis equal to one.\n\ndf.apply(min_max, axis = 1).head()\n\n\n\n\n\n  \n    \n      \n      min\n      max\n    \n  \n  \n    \n      0\n      4785161\n      4858979\n    \n    \n      1\n      54660\n      55347\n    \n    \n      2\n      183193\n      203709\n    \n    \n      3\n      26489\n      27341\n    \n    \n      4\n      22512\n      22861\n    \n  \n\n\n\n\nOf course there’s no need to limit yourself to returning a new series object.\nIf you’re doing this as part of data cleaning your likely to find yourself wanting to add new data to the existing DataFrame. In that case you just take the row values and add in new columns indicating the max and minimum scores.\nHere we have a revised version of the function min_max. Instead of returning a separate series to display the min and max. We add two new columns in the original dataframe to store min and max:\n\ndef min_max(row):\n    data = row[['POPESTIMATE2010',\n                'POPESTIMATE2011',\n                'POPESTIMATE2012',\n                'POPESTIMATE2013',\n                'POPESTIMATE2014',\n                'POPESTIMATE2015']]\n    row['max'] = np.max(data)\n    row['min'] = np.min(data)\n    return row\n\ndf.apply(min_max, axis=1).head()\n\n\n\n\n\n  \n    \n      \n      SUMLEV\n      REGION\n      DIVISION\n      STATE\n      COUNTY\n      STNAME\n      CTYNAME\n      CENSUS2010POP\n      ESTIMATESBASE2010\n      POPESTIMATE2010\n      ...\n      RDOMESTICMIG2013\n      RDOMESTICMIG2014\n      RDOMESTICMIG2015\n      RNETMIG2011\n      RNETMIG2012\n      RNETMIG2013\n      RNETMIG2014\n      RNETMIG2015\n      max\n      min\n    \n  \n  \n    \n      0\n      40\n      3\n      6\n      1\n      0\n      Alabama\n      Alabama\n      4779736\n      4780127\n      4785161\n      ...\n      0.381066\n      0.582002\n      -0.467369\n      1.030015\n      0.826644\n      1.383282\n      1.724718\n      0.712594\n      4858979\n      4785161\n    \n    \n      1\n      50\n      3\n      6\n      1\n      1\n      Alabama\n      Autauga County\n      54571\n      54571\n      54660\n      ...\n      -3.012349\n      2.265971\n      -2.530799\n      7.606016\n      -2.626146\n      -2.722002\n      2.592270\n      -2.187333\n      55347\n      54660\n    \n    \n      2\n      50\n      3\n      6\n      1\n      3\n      Alabama\n      Baldwin County\n      182265\n      182265\n      183193\n      ...\n      21.845705\n      19.243287\n      17.197872\n      15.844176\n      18.559627\n      22.727626\n      20.317142\n      18.293499\n      203709\n      183193\n    \n    \n      3\n      50\n      3\n      6\n      1\n      5\n      Alabama\n      Barbour County\n      27457\n      27457\n      27341\n      ...\n      -7.056824\n      -3.904217\n      -10.543299\n      -4.874741\n      -2.758113\n      -7.167664\n      -3.978583\n      -10.543299\n      27341\n      26489\n    \n    \n      4\n      50\n      3\n      6\n      1\n      7\n      Alabama\n      Bibb County\n      22915\n      22919\n      22861\n      ...\n      -6.201001\n      -0.177537\n      0.177258\n      -5.088389\n      -4.363636\n      -5.403729\n      0.754533\n      1.107861\n      22861\n      22512\n    \n  \n\n5 rows × 102 columns\n\n\n\nApply is an extremely important tool in your toolkit. The reason I introduced apply here is because you rarely see it used with large function definitions, like we did. Instead, you typically see it used with lambdas. To get the most of the discussions you’ll see online, you’re going to need to know how to at least read lambdas.\nHere’s You can imagine how you might chain several apply calls with lambdas together to create a readable yet succinct data manipulation script. One line example of how you might calculate the max of the columns using the apply function.\nNotice, when using the axis parameter 1 or ‘columns’ means we apply the function to each row. 0 or ‘index’ means we apply the function to each column\n\nrows = ['POPESTIMATE2010',\n        'POPESTIMATE2011',\n        'POPESTIMATE2012',\n        'POPESTIMATE2013',\n        'POPESTIMATE2014',\n        'POPESTIMATE2015']\n\ndf.apply(lambda x: np.max(x[rows]), axis = 'columns').head()\n\n0    4858979\n1      55347\n2     203709\n3      27341\n4      22861\ndtype: int64\n\n\nThe beauty of the apply function is that it allows flexibility in doing whatever manipulation that you desire, and the function you pass into apply can be any customized function that you write.\nLet’s say we want to divide the states into four categories: Northeast, Midwest, South, and West. We can write a customized function that returns the region based on the state the state regions information is obtained from Wikipedia:\n\ndef get_state_region(x):\n    northeast = ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', \n                 'Rhode Island','Vermont','New York','New Jersey','Pennsylvania']\n    midwest = ['Illinois','Indiana','Michigan','Ohio','Wisconsin','Iowa',\n               'Kansas','Minnesota','Missouri','Nebraska','North Dakota',\n               'South Dakota']\n    south = ['Delaware','Florida','Georgia','Maryland','North Carolina',\n             'South Carolina','Virginia','District of Columbia','West Virginia',\n             'Alabama','Kentucky','Mississippi','Tennessee','Arkansas',\n             'Louisiana','Oklahoma','Texas']\n    west = ['Arizona','Colorado','Idaho','Montana','Nevada','New Mexico','Utah',\n            'Wyoming','Alaska','California','Hawaii','Oregon','Washington']\n    \n    if x in northeast:\n        return \"Northeast\"\n    elif x in midwest:\n        return \"Midwest\"\n    elif x in south:\n        return \"South\"\n    else:\n        return \"West\"\n\nNow we have the customized function, let’s say we want to create a new column called Region, which shows the state’s region, we can use the customized function and the apply function to do so. The customized function is supposed to work on the state name column STNAME. So we will set the apply function on the state name column and pass the customized function into the apply function:\n\ndf['state_region'] = df['STNAME'].apply(lambda x: get_state_region(x))\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      SUMLEV\n      REGION\n      DIVISION\n      STATE\n      COUNTY\n      STNAME\n      CTYNAME\n      CENSUS2010POP\n      ESTIMATESBASE2010\n      POPESTIMATE2010\n      ...\n      RDOMESTICMIG2012\n      RDOMESTICMIG2013\n      RDOMESTICMIG2014\n      RDOMESTICMIG2015\n      RNETMIG2011\n      RNETMIG2012\n      RNETMIG2013\n      RNETMIG2014\n      RNETMIG2015\n      state_region\n    \n  \n  \n    \n      0\n      40\n      3\n      6\n      1\n      0\n      Alabama\n      Alabama\n      4779736\n      4780127\n      4785161\n      ...\n      -0.193196\n      0.381066\n      0.582002\n      -0.467369\n      1.030015\n      0.826644\n      1.383282\n      1.724718\n      0.712594\n      South\n    \n    \n      1\n      50\n      3\n      6\n      1\n      1\n      Alabama\n      Autauga County\n      54571\n      54571\n      54660\n      ...\n      -2.915927\n      -3.012349\n      2.265971\n      -2.530799\n      7.606016\n      -2.626146\n      -2.722002\n      2.592270\n      -2.187333\n      South\n    \n    \n      2\n      50\n      3\n      6\n      1\n      3\n      Alabama\n      Baldwin County\n      182265\n      182265\n      183193\n      ...\n      17.647293\n      21.845705\n      19.243287\n      17.197872\n      15.844176\n      18.559627\n      22.727626\n      20.317142\n      18.293499\n      South\n    \n    \n      3\n      50\n      3\n      6\n      1\n      5\n      Alabama\n      Barbour County\n      27457\n      27457\n      27341\n      ...\n      -2.500690\n      -7.056824\n      -3.904217\n      -10.543299\n      -4.874741\n      -2.758113\n      -7.167664\n      -3.978583\n      -10.543299\n      South\n    \n    \n      4\n      50\n      3\n      6\n      1\n      7\n      Alabama\n      Bibb County\n      22915\n      22919\n      22861\n      ...\n      -5.068871\n      -6.201001\n      -0.177537\n      0.177258\n      -5.088389\n      -4.363636\n      -5.403729\n      0.754533\n      1.107861\n      South\n    \n  \n\n5 rows × 101 columns"
  },
  {
    "objectID": "code/7_groupby.html",
    "href": "code/7_groupby.html",
    "title": "8  Grouping data",
    "section": "",
    "text": "Sometimes we want to select data based on groups and understand aggregated data on a group level. We have seen that even though Pandas allows us to iterate over every row in a dataframe, it is generally very slow to do so. Fortunately Pandas has a groupby() function to speed up such task.\nThe idea behind the groupby() function is that it takes some dataframe, splits it into chunks based on some key values, applies computation on those chunks, then combines the results back together into another dataframe. In pandas this is referred to as the split-apply-combine pattern.\nLet’s prepare some data to work with:\nIn the first example for groupby() I want to use the census date. Let’s get a list of the unique states, then we can iterate over all the states and for each state we reduce the data frame and calculate the average.\nLet’s run such task for 3 times and time it. For this we’ll use the cell magic function %%timeit:\nIf you scroll down to the bottom of that output you can see it takes a fair bit of time to finish. I.e. in the jupyter notebook it takes 1.07s per loop. Now let’s try another approach using groupby().\nYou’ll notice there are two values we set here. groupby() returns a tuple, where: - the first value is the value of the key we were trying to group by, in this case a specific state name - the second one is projected dataframe that was found for that group\nThis one would take 4.71 m to run in jupyter."
  },
  {
    "objectID": "code/7_groupby.html#grouping-multiple-columns",
    "href": "code/7_groupby.html#grouping-multiple-columns",
    "title": "8  Grouping data",
    "section": "8.1 Grouping multiple columns",
    "text": "8.1 Grouping multiple columns\nNow, 99% of the time, you’ll use group by on one or more columns. But you can also provide a function to group by and use that to segment your data.\nThis is a bit of a fabricated example but lets say that you have a big batch job with lots of processing and you want to work on only a third or so of the states at a given time. We could create some function which returns a number between zero and two based on the first character of the state name. Then we can tell group by to use this function to split up our data frame.\nIt’s important to note that in order to do this you need to set the index of the data frame to be the column that you want to group by first.\nWe’ll create some new function called set_batch_number and if the first letter of the parameter is a capital M we’ll return a 0. If it’s a capital Q we’ll return a 1 and otherwise we’ll return a 2. Then we’ll pass this function to the data frame:\n\ndf = df.set_index('STNAME')\n\ndef set_batch_number(item):\n    if item[0] < 'M':\n        return 0\n    if item[0] < 'Q':\n        return 1\n    return 2\n\nThe dataframe is supposed to be grouped by according to the batch number And we will loop through each batch group:\n\nfor group, frame in df.groupby(set_batch_number):\n    print('There are ' + str(len(frame)) + ' records in group ' + str(group) + ' for processing.')\n\nprint('')\n\nThere are 1177 records in group 0 for processing.\nThere are 1134 records in group 1 for processing.\nThere are 831 records in group 2 for processing.\n\n\n\nNotice that this time I didn’t pass in a column name to groupby(). Instead, I set the index of the dataframe to be STNAME, and if no column identifier is passed groupby() will automatically use the index.\nLet’s take one more look at an example of how we might group data. In this example, I want to use a dataset of housing from airbnb. In this dataset there are two columns of interest, one is the cancellation_policy and the other is the review_scores_value:\n\ndf = pd.read_csv('../data/week3/listings.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      id\n      listing_url\n      scrape_id\n      last_scraped\n      name\n      summary\n      space\n      description\n      experiences_offered\n      neighborhood_overview\n      ...\n      review_scores_value\n      requires_license\n      license\n      jurisdiction_names\n      instant_bookable\n      cancellation_policy\n      require_guest_profile_picture\n      require_guest_phone_verification\n      calculated_host_listings_count\n      reviews_per_month\n    \n  \n  \n    \n      0\n      12147973\n      https://www.airbnb.com/rooms/12147973\n      20160906204935\n      2016-09-07\n      Sunny Bungalow in the City\n      Cozy, sunny, family home.  Master bedroom high...\n      The house has an open and cozy feel at the sam...\n      Cozy, sunny, family home.  Master bedroom high...\n      none\n      Roslindale is quiet, convenient and friendly. ...\n      ...\n      NaN\n      f\n      NaN\n      NaN\n      f\n      moderate\n      f\n      f\n      1\n      NaN\n    \n    \n      1\n      3075044\n      https://www.airbnb.com/rooms/3075044\n      20160906204935\n      2016-09-07\n      Charming room in pet friendly apt\n      Charming and quiet room in a second floor 1910...\n      Small but cozy and quite room with a full size...\n      Charming and quiet room in a second floor 1910...\n      none\n      The room is in Roslindale, a diverse and prima...\n      ...\n      9.0\n      f\n      NaN\n      NaN\n      t\n      moderate\n      f\n      f\n      1\n      1.30\n    \n    \n      2\n      6976\n      https://www.airbnb.com/rooms/6976\n      20160906204935\n      2016-09-07\n      Mexican Folk Art Haven in Boston\n      Come stay with a friendly, middle-aged guy in ...\n      Come stay with a friendly, middle-aged guy in ...\n      Come stay with a friendly, middle-aged guy in ...\n      none\n      The LOCATION: Roslindale is a safe and diverse...\n      ...\n      10.0\n      f\n      NaN\n      NaN\n      f\n      moderate\n      t\n      f\n      1\n      0.47\n    \n    \n      3\n      1436513\n      https://www.airbnb.com/rooms/1436513\n      20160906204935\n      2016-09-07\n      Spacious Sunny Bedroom Suite in Historic Home\n      Come experience the comforts of home away from...\n      Most places you find in Boston are small howev...\n      Come experience the comforts of home away from...\n      none\n      Roslindale is a lovely little neighborhood loc...\n      ...\n      10.0\n      f\n      NaN\n      NaN\n      f\n      moderate\n      f\n      f\n      1\n      1.00\n    \n    \n      4\n      7651065\n      https://www.airbnb.com/rooms/7651065\n      20160906204935\n      2016-09-07\n      Come Home to Boston\n      My comfy, clean and relaxing home is one block...\n      Clean, attractive, private room, one block fro...\n      My comfy, clean and relaxing home is one block...\n      none\n      I love the proximity to downtown, the neighbor...\n      ...\n      10.0\n      f\n      NaN\n      NaN\n      f\n      flexible\n      f\n      f\n      1\n      2.25\n    \n  \n\n5 rows × 95 columns\n\n\n\nSo, how would I group by both of these columns? A first approach might be to promote them to a multiindex and just call groupby().\nWhen we have a multiindex we need to pass in the levels we are interested in grouping by:\n\ndf = df.set_index(['cancellation_policy', 'review_scores_value'])\n\nfor group, frame in df.groupby(level=(0,1)):\n    print(group)\n\nprint('')\n\n('flexible', 2.0)\n('flexible', 4.0)\n('flexible', 5.0)\n('flexible', 6.0)\n('flexible', 7.0)\n('flexible', 8.0)\n('flexible', 9.0)\n('flexible', 10.0)\n('moderate', 2.0)\n('moderate', 4.0)\n('moderate', 6.0)\n('moderate', 7.0)\n('moderate', 8.0)\n('moderate', 9.0)\n('moderate', 10.0)\n('strict', 2.0)\n('strict', 3.0)\n('strict', 4.0)\n('strict', 5.0)\n('strict', 6.0)\n('strict', 7.0)\n('strict', 8.0)\n('strict', 9.0)\n('strict', 10.0)\n('super_strict_30', 6.0)\n('super_strict_30', 7.0)\n('super_strict_30', 8.0)\n('super_strict_30', 9.0)\n('super_strict_30', 10.0)\n\n\n\nWhat if we wanted to group by the cancelation policy and review scores, but separate out all the 10’s from those under ten? In this case, we could use a function to manage the groupings.\nIn this function, we want to check the “review_scores_value” portion of the index. item is in the tuple format: (cancellation_policy,review_scores_value):\n\ndef grouping_fun(item):\n    if item[1] == 10.0:\n        return (item[0], \"10.0\")\n    else:\n        return (item[0], \"not 10.0\")\n\nfor group, frame in df.groupby(by=grouping_fun):\n    print(group)\n\nprint('')\n\n('flexible', '10.0')\n('flexible', 'not 10.0')\n('moderate', '10.0')\n('moderate', 'not 10.0')\n('strict', '10.0')\n('strict', 'not 10.0')\n('super_strict_30', '10.0')\n('super_strict_30', 'not 10.0')"
  },
  {
    "objectID": "code/7_groupby.html#aggregation-and-.agg",
    "href": "code/7_groupby.html#aggregation-and-.agg",
    "title": "8  Grouping data",
    "section": "8.2 Aggregation and .agg()",
    "text": "8.2 Aggregation and .agg()\nTo this point we have applied very simple processing to our data after splitting, really just outputting some print statements to demonstrate how the splitting works. The pandas developers have three broad categories of data processing to happen during the apply step:\n\nAggregation of group data\nTransformation of group data\nFiltration of group data\n\nThe most straight forward apply step is the aggregation of data, and uses the method agg() on the groupby object. The agg() method allows you to apply a function or a list of function names to be executed along one of the axis of the DataFrame, default 0, which is the index (row) axis.\nThus far we have only iterated through the groupby object, unpacking it into a label (the group name) and a dataframe. But with agg we can pass in a dictionary of the columns we are interested in aggregating along with the function we are looking to apply to aggregate.\nLet’s reset the index for our airbnb data:\n\ndf = df.reset_index()\n\n#group by cancellation policy and find the avg review scores\ndf.groupby('cancellation_policy').agg({'review_scores_value' : np.average})\n\n\n\n\n\n  \n    \n      \n      review_scores_value\n    \n    \n      cancellation_policy\n      \n    \n  \n  \n    \n      flexible\n      NaN\n    \n    \n      moderate\n      NaN\n    \n    \n      strict\n      NaN\n    \n    \n      super_strict_30\n      NaN\n    \n  \n\n\n\n\nThat didn’t seem to work at all. Just a bunch of not a numbers. The issue is actually in the function that we sent to aggregate. np.average does not ignore nans! However, there is a function we can use for this:\n\ndf.groupby('cancellation_policy').agg({'review_scores_value': np.nanmean})\n\n\n\n\n\n  \n    \n      \n      review_scores_value\n    \n    \n      cancellation_policy\n      \n    \n  \n  \n    \n      flexible\n      9.237421\n    \n    \n      moderate\n      9.307398\n    \n    \n      strict\n      9.081441\n    \n    \n      super_strict_30\n      8.537313\n    \n  \n\n\n\n\nWe can just extend this dictionary to aggregate by multiple functions or multiple columns.\n\ndf.groupby('cancellation_policy').agg({'review_scores_value': (np.nanmean, np.nanstd),\n'reviews_per_month' : np.nanmean})\n\n\n\n\n\n  \n    \n      \n      review_scores_value\n      reviews_per_month\n    \n    \n      \n      nanmean\n      nanstd\n      nanmean\n    \n    \n      cancellation_policy\n      \n      \n      \n    \n  \n  \n    \n      flexible\n      9.237421\n      1.096271\n      1.829210\n    \n    \n      moderate\n      9.307398\n      0.859859\n      2.391922\n    \n    \n      strict\n      9.081441\n      1.040531\n      1.873467\n    \n    \n      super_strict_30\n      8.537313\n      0.840785\n      0.340143\n    \n  \n\n\n\n\n\nFirst we’re doing a group by on the dataframe object by the column “cancellation_policy”. This creates a new GroupBy object.\nThen we are invoking the agg() function on that object. The agg function is going to apply one or more functions we specify to the group dataframes and return a single row per dataframe/group.\nWhen we called this function we sent it two dictionary entries, each with the key indicating which column we wanted functions applied to.\nFor the first column we actually supplied a tuple of two functions. Note that these are not function invocations, like np.nanmean(), or function names, like “nanmean” they are references to functions which will return single values. The group by object will recognize the tuple and call each function in order on the same column.\nThe results will be in a hierarchical index, but since they are columns they don’t show as an index per se. Then we indicated another column and a single function we wanted to run."
  },
  {
    "objectID": "code/7_groupby.html#transformation",
    "href": "code/7_groupby.html#transformation",
    "title": "8  Grouping data",
    "section": "8.3 Transformation",
    "text": "8.3 Transformation\nTransformation is different from aggregation.\nWhere agg() returns a single value per column, so one row per group, transform() returns an object that is the same size as the group.\nEssentially, it broadcasts the function you supply over the grouped dataframe, returning a new dataframe. This makes combining data later easy.\nFor instance, suppose we want to include the average rating values in a given group by cancellation policy, but preserve the dataframe shape so that we could generate a difference between an individual observation and the sum.\n\n#define what columns we are interested in\ncols = ['cancellation_policy', 'review_scores_value']\n\n#transform the data and store it in a new df\ntransform_df = df[cols].groupby('cancellation_policy').transform(np.nanmean)\ntransform_df.head()\n\n\n\n\n\n  \n    \n      \n      review_scores_value\n    \n  \n  \n    \n      0\n      9.307398\n    \n    \n      1\n      9.307398\n    \n    \n      2\n      9.307398\n    \n    \n      3\n      9.307398\n    \n    \n      4\n      9.237421\n    \n  \n\n\n\n\nSo we can see that the index here is actually the same as the original dataframe. So lets just join this in. Before we do that, lets rename the column in the transformed version:\n\ntransform_df.rename({'review_scores_value':'mean_review_scores'}, axis = 'columns', inplace = True)\n\ndf = df.merge(transform_df, left_index = True, right_index = True)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      cancellation_policy\n      review_scores_value\n      id\n      listing_url\n      scrape_id\n      last_scraped\n      name\n      summary\n      space\n      description\n      ...\n      review_scores_location\n      requires_license\n      license\n      jurisdiction_names\n      instant_bookable\n      require_guest_profile_picture\n      require_guest_phone_verification\n      calculated_host_listings_count\n      reviews_per_month\n      mean_review_scores\n    \n  \n  \n    \n      0\n      moderate\n      NaN\n      12147973\n      https://www.airbnb.com/rooms/12147973\n      20160906204935\n      2016-09-07\n      Sunny Bungalow in the City\n      Cozy, sunny, family home.  Master bedroom high...\n      The house has an open and cozy feel at the sam...\n      Cozy, sunny, family home.  Master bedroom high...\n      ...\n      NaN\n      f\n      NaN\n      NaN\n      f\n      f\n      f\n      1\n      NaN\n      9.307398\n    \n    \n      1\n      moderate\n      9.0\n      3075044\n      https://www.airbnb.com/rooms/3075044\n      20160906204935\n      2016-09-07\n      Charming room in pet friendly apt\n      Charming and quiet room in a second floor 1910...\n      Small but cozy and quite room with a full size...\n      Charming and quiet room in a second floor 1910...\n      ...\n      9.0\n      f\n      NaN\n      NaN\n      t\n      f\n      f\n      1\n      1.30\n      9.307398\n    \n    \n      2\n      moderate\n      10.0\n      6976\n      https://www.airbnb.com/rooms/6976\n      20160906204935\n      2016-09-07\n      Mexican Folk Art Haven in Boston\n      Come stay with a friendly, middle-aged guy in ...\n      Come stay with a friendly, middle-aged guy in ...\n      Come stay with a friendly, middle-aged guy in ...\n      ...\n      9.0\n      f\n      NaN\n      NaN\n      f\n      t\n      f\n      1\n      0.47\n      9.307398\n    \n    \n      3\n      moderate\n      10.0\n      1436513\n      https://www.airbnb.com/rooms/1436513\n      20160906204935\n      2016-09-07\n      Spacious Sunny Bedroom Suite in Historic Home\n      Come experience the comforts of home away from...\n      Most places you find in Boston are small howev...\n      Come experience the comforts of home away from...\n      ...\n      10.0\n      f\n      NaN\n      NaN\n      f\n      f\n      f\n      1\n      1.00\n      9.307398\n    \n    \n      4\n      flexible\n      10.0\n      7651065\n      https://www.airbnb.com/rooms/7651065\n      20160906204935\n      2016-09-07\n      Come Home to Boston\n      My comfy, clean and relaxing home is one block...\n      Clean, attractive, private room, one block fro...\n      My comfy, clean and relaxing home is one block...\n      ...\n      9.0\n      f\n      NaN\n      NaN\n      f\n      f\n      f\n      1\n      2.25\n      9.237421\n    \n  \n\n5 rows × 96 columns\n\n\n\nGreat, we can see that our new column is in place, the mean_review_scores. So now we could create, for instance, the difference between a given row and it’s group (the cancellation policy) means:\n\ndf['mean_dff'] = np.absolute(df['review_scores_value'] - df['mean_review_scores'])\ndf['mean_dff'].head()\n\n0         NaN\n1    0.307398\n2    0.692602\n3    0.692602\n4    0.762579\nName: mean_dff, dtype: float64\n\n\nNumpy absolute value calculates the absolute value of the values in a Numpy array. The absolute value (or modulus) | x | of a real number x is the non-negative value of x without regard to its sign."
  },
  {
    "objectID": "code/7_groupby.html#filtering",
    "href": "code/7_groupby.html#filtering",
    "title": "8  Grouping data",
    "section": "8.4 Filtering",
    "text": "8.4 Filtering\nThe GroupBy object has build in support for filtering groups as well. It’s often that you’ll want to group by some feature, then make some transformation to the groups, then drop certain groups as part of your cleaning routines.\nThe filter() function takes in a function which it applies to each group dataframe and returns either a True or a False, depending upon whether that group should be included in the results.\nFor instance, if we only want those groups which have a mean rating above 9 included in our results:\n\ndf.groupby('cancellation_policy').filter(lambda x: np.nanmean(x['review_scores_value']) > 9.2).head()\n\n\n\n\n\n  \n    \n      \n      cancellation_policy\n      review_scores_value\n      id\n      listing_url\n      scrape_id\n      last_scraped\n      name\n      summary\n      space\n      description\n      ...\n      requires_license\n      license\n      jurisdiction_names\n      instant_bookable\n      require_guest_profile_picture\n      require_guest_phone_verification\n      calculated_host_listings_count\n      reviews_per_month\n      mean_review_scores\n      mean_dff\n    \n  \n  \n    \n      0\n      moderate\n      NaN\n      12147973\n      https://www.airbnb.com/rooms/12147973\n      20160906204935\n      2016-09-07\n      Sunny Bungalow in the City\n      Cozy, sunny, family home.  Master bedroom high...\n      The house has an open and cozy feel at the sam...\n      Cozy, sunny, family home.  Master bedroom high...\n      ...\n      f\n      NaN\n      NaN\n      f\n      f\n      f\n      1\n      NaN\n      9.307398\n      NaN\n    \n    \n      1\n      moderate\n      9.0\n      3075044\n      https://www.airbnb.com/rooms/3075044\n      20160906204935\n      2016-09-07\n      Charming room in pet friendly apt\n      Charming and quiet room in a second floor 1910...\n      Small but cozy and quite room with a full size...\n      Charming and quiet room in a second floor 1910...\n      ...\n      f\n      NaN\n      NaN\n      t\n      f\n      f\n      1\n      1.30\n      9.307398\n      0.307398\n    \n    \n      2\n      moderate\n      10.0\n      6976\n      https://www.airbnb.com/rooms/6976\n      20160906204935\n      2016-09-07\n      Mexican Folk Art Haven in Boston\n      Come stay with a friendly, middle-aged guy in ...\n      Come stay with a friendly, middle-aged guy in ...\n      Come stay with a friendly, middle-aged guy in ...\n      ...\n      f\n      NaN\n      NaN\n      f\n      t\n      f\n      1\n      0.47\n      9.307398\n      0.692602\n    \n    \n      3\n      moderate\n      10.0\n      1436513\n      https://www.airbnb.com/rooms/1436513\n      20160906204935\n      2016-09-07\n      Spacious Sunny Bedroom Suite in Historic Home\n      Come experience the comforts of home away from...\n      Most places you find in Boston are small howev...\n      Come experience the comforts of home away from...\n      ...\n      f\n      NaN\n      NaN\n      f\n      f\n      f\n      1\n      1.00\n      9.307398\n      0.692602\n    \n    \n      4\n      flexible\n      10.0\n      7651065\n      https://www.airbnb.com/rooms/7651065\n      20160906204935\n      2016-09-07\n      Come Home to Boston\n      My comfy, clean and relaxing home is one block...\n      Clean, attractive, private room, one block fro...\n      My comfy, clean and relaxing home is one block...\n      ...\n      f\n      NaN\n      NaN\n      f\n      f\n      f\n      1\n      2.25\n      9.237421\n      0.762579\n    \n  \n\n5 rows × 97 columns\n\n\n\nNotice that the results are still indexed, but that any of the results which were in a group with a mean review score of less than or equal to 9.2 were not copied over."
  },
  {
    "objectID": "code/7_groupby.html#applying",
    "href": "code/7_groupby.html#applying",
    "title": "8  Grouping data",
    "section": "8.5 Applying",
    "text": "8.5 Applying\nBy far the most common operation I invoke on groupby objects is the apply() function. This allows you to apply an arbitrary function to each group, and stitch the results back for each apply() into a single dataframe where the index is preserved.\nLets look at an example using our airbnb data, I’m going to get a clean copy of the dataframe:\n\ndf=pd.read_csv(\"../data/week3/listings.csv\")\n\n# And lets just include some of the columns we were interested in previously\ndf = df[['cancellation_policy','review_scores_value']]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      cancellation_policy\n      review_scores_value\n    \n  \n  \n    \n      0\n      moderate\n      NaN\n    \n    \n      1\n      moderate\n      9.0\n    \n    \n      2\n      moderate\n      10.0\n    \n    \n      3\n      moderate\n      10.0\n    \n    \n      4\n      flexible\n      10.0\n    \n  \n\n\n\n\nIn previous work we wanted to find the average review score of a listing and its deviation from the group mean. This was a two step process, first we used transform() on the groupby object and then we had to broadcast to create a new column. With apply() we could wrap this logic in one place:\n\ndef calc_mean_review_scores(group):\n    # group is a dataframe just of whatever we have grouped by, e.g. cancellation policy,     \n    #so we can treat this as the complete dataframe\n    avg = np.nanmean(group['review_scores_value'])\n    \n    #now braodcast our formula and create a new column\n    group['review_scores_mean_diff'] = np.abs(avg - group['review_scores_value'])\n    return group\n\n#now we apply this to all the groups\ndf.groupby('cancellation_policy').apply(calc_mean_review_scores).head()\n\n\n\n\n\n  \n    \n      \n      cancellation_policy\n      review_scores_value\n      review_scores_mean_diff\n    \n  \n  \n    \n      0\n      moderate\n      NaN\n      NaN\n    \n    \n      1\n      moderate\n      9.0\n      0.307398\n    \n    \n      2\n      moderate\n      10.0\n      0.692602\n    \n    \n      3\n      moderate\n      10.0\n      0.692602\n    \n    \n      4\n      flexible\n      10.0\n      0.762579\n    \n  \n\n\n\n\nUsing apply can be slower than using some of the specialized functions, especially agg(). But, if your dataframes are not huge, it’s a solid general purpose approach."
  },
  {
    "objectID": "code/8_scales.html",
    "href": "code/8_scales.html",
    "title": "9  Scales",
    "section": "",
    "text": "Let’s say that we have got a DataFrame of students and their academic levels such as being in grade one, grade two, and grade three. Is the difference between a student in grade one and a student in grade two the same as the difference between a student in grade eight and one grade nine?\nAs a data scientist, there’s at least four different scales that’s worth knowing about:\n\nRatio scales:\n\nUnites are equally spaces\nmathematical operations of +-* are valid\nI.e. height or weight\n\nInterval scale:\n\nMeasurement units are equally are spaced\nThere is no true 0, so mathematical operations are not valid\nI.e. temperature in C\n\nOrdinal scale:\n\nThe order of the units is important but not evenly spaced\nLetter grades such as A+, A, B are a good example\n\nNominal scale:\n\nCategories of data, but the categories have no order with respect to each other\nAlso called categorical data in python\nI.e. teams of a sport\n\n\nPandas has a number of interesting functions to deal with converting between measurement scales.\nLets first create a dataframe of letter grades in descending order. We can also set an index value and here we’ll just make it some human judgement of how good a student was, like “excellent” or “good”:\n\nimport pandas as pd\nimport numpy as np\n\n\ndf=pd.DataFrame(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D'],\n                index=['excellent', 'excellent', 'excellent', 'good', 'good', 'good', \n                       'ok', 'ok', 'ok', 'poor', 'poor'],\n               columns=[\"Grades\"])\ndf\n\n\n\n\n\n  \n    \n      \n      Grades\n    \n  \n  \n    \n      excellent\n      A+\n    \n    \n      excellent\n      A\n    \n    \n      excellent\n      A-\n    \n    \n      good\n      B+\n    \n    \n      good\n      B\n    \n    \n      good\n      B-\n    \n    \n      ok\n      C+\n    \n    \n      ok\n      C\n    \n    \n      ok\n      C-\n    \n    \n      poor\n      D+\n    \n    \n      poor\n      D\n    \n  \n\n\n\n\nNow, if we check the datatype of this column, we see that it’s just an object, since we set string values:\n\ndf.dtypes\n\nGrades    object\ndtype: object\n\n\nWe can, however, tell pandas that we want to change the type to category, using the astype() function:\n\ndf['Grades'].astype('category').head()\n\nexcellent    A+\nexcellent     A\nexcellent    A-\ngood         B+\ngood          B\nName: Grades, dtype: category\nCategories (11, object): ['A', 'A+', 'A-', 'B', ..., 'C+', 'C-', 'D', 'D+']\n\n\nWe see now that there are eleven categories, and pandas is aware of what those categories are. More interesting though is that our data isn’t just categorical, but that it’s ordered. That is, an A- comes after a B+, and B comes before a B+.\nWe can tell pandas that the data is ordered by first creating a new categorical data type with the list of the categories (in order) and the ordered=True flag:\n\nmy_categories=pd.CategoricalDtype(categories=['D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'], ordered=True)\n\n# then we can just pass this to the astype() function\ngrades=df[\"Grades\"].astype(my_categories)\ngrades.head()\n\nexcellent    A+\nexcellent     A\nexcellent    A-\ngood         B+\ngood          B\nName: Grades, dtype: category\nCategories (11, object): ['D' < 'D+' < 'C-' < 'C' ... 'B+' < 'A-' < 'A' < 'A+']\n\n\nNow we see that pandas is not only aware that there are 11 categories, but it is also aware of the order of those categories.\nSo, what can you do with this? Well because there is an ordering this can help with comparisons and boolean masking. For instance, if we have a list of our grades and we compare them to a ‘C’ we see that the lexicographical comparison returns results we were not intending.\n\ndf[df['Grades'] > 'C']\n\n\n\n\n\n  \n    \n      \n      Grades\n    \n  \n  \n    \n      ok\n      C+\n    \n    \n      ok\n      C-\n    \n    \n      poor\n      D+\n    \n    \n      poor\n      D\n    \n  \n\n\n\n\nSo a C+ is great than a C, but a C- and D are not. However, if we broadcast over the dataframe which has the type set to an ordered categorical:\n\ngrades[grades > 'C']\n\nexcellent    A+\nexcellent     A\nexcellent    A-\ngood         B+\ngood          B\ngood         B-\nok           C+\nName: Grades, dtype: category\nCategories (11, object): ['D' < 'D+' < 'C-' < 'C' ... 'B+' < 'A-' < 'A' < 'A+']\n\n\nWe see that the operator works as we would expect. We can then use a certain set of mathematical operators, like minimum, maximum, etc., on the ordinal data.\nSometimes it is useful to represent categorical values as each being a column with a true or a false as to whether the category applies. This is especially common in feature extraction, which is a topic in the data mining course. Variables with a boolean value are typically called dummy variables, and pandas has a built in function called get_dummies which will convert the values of a single column into multiple columns of zeros and ones indicating the presence of the dummy variable.\nThere’s one more common scale-based operation, and that’s on converting a scale from something that is on the interval or ratio scale, like a numeric grade, into one which is categorical.\nNow, this might seem a bit counter intuitive to you, since you are losing information about the value. But it’s commonly done in a couple of places. For instance, if you are visualizing the frequencies of categories, this can be an extremely useful approach, and histograms are regularly used with converted interval or ratio data. In addition, if you’re using a machine learning classification approach on data, you need to be using categorical data, so reducing dimensionality may be useful just to apply a given technique.\nPandas has a function called cut which takes as an argument some array-like structure like a column of a dataframe or a series. It also takes a number of bins to be used, and all bins are kept at equal spacing.\nLets go back to our census data for an example. We saw that we could group by state, then aggregate to get a list of the average county size by state. If we further apply cut to this with, say, ten bins, we can see the states listed as categoricals using the average county size.\n\ndf = pd.read_csv('../data/week3/census.csv')\n\n#only work with country data\ndf = df[df['SUMLEV'] == 50]\n\n#work with only a few rows\ndf = df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'].agg(np.average)\n\ndf.head()\n\nSTNAME\nAlabama        71339.343284\nAlaska         24490.724138\nArizona       426134.466667\nArkansas       38878.906667\nCalifornia    642309.586207\nName: CENSUS2010POP, dtype: float64\n\n\nNow if we just want to make “bins” of each of these, we can use cut():\n\npd.cut(df,10)\n\nSTNAME\nAlabama                   (11706.087, 75333.413]\nAlaska                    (11706.087, 75333.413]\nArizona                 (390320.176, 453317.529]\nArkansas                  (11706.087, 75333.413]\nCalifornia              (579312.234, 642309.586]\nColorado                 (75333.413, 138330.766]\nConnecticut             (390320.176, 453317.529]\nDelaware                (264325.471, 327322.823]\nDistrict of Columbia    (579312.234, 642309.586]\nFlorida                 (264325.471, 327322.823]\nGeorgia                   (11706.087, 75333.413]\nHawaii                  (264325.471, 327322.823]\nIdaho                     (11706.087, 75333.413]\nIllinois                 (75333.413, 138330.766]\nIndiana                   (11706.087, 75333.413]\nIowa                      (11706.087, 75333.413]\nKansas                    (11706.087, 75333.413]\nKentucky                  (11706.087, 75333.413]\nLouisiana                 (11706.087, 75333.413]\nMaine                    (75333.413, 138330.766]\nMaryland                (201328.118, 264325.471]\nMassachusetts           (453317.529, 516314.881]\nMichigan                 (75333.413, 138330.766]\nMinnesota                 (11706.087, 75333.413]\nMississippi               (11706.087, 75333.413]\nMissouri                  (11706.087, 75333.413]\nMontana                   (11706.087, 75333.413]\nNebraska                  (11706.087, 75333.413]\nNevada                  (138330.766, 201328.118]\nNew Hampshire            (75333.413, 138330.766]\nNew Jersey              (390320.176, 453317.529]\nNew Mexico                (11706.087, 75333.413]\nNew York                (264325.471, 327322.823]\nNorth Carolina           (75333.413, 138330.766]\nNorth Dakota              (11706.087, 75333.413]\nOhio                     (75333.413, 138330.766]\nOklahoma                  (11706.087, 75333.413]\nOregon                   (75333.413, 138330.766]\nPennsylvania            (138330.766, 201328.118]\nRhode Island            (201328.118, 264325.471]\nSouth Carolina           (75333.413, 138330.766]\nSouth Dakota              (11706.087, 75333.413]\nTennessee                 (11706.087, 75333.413]\nTexas                    (75333.413, 138330.766]\nUtah                     (75333.413, 138330.766]\nVermont                   (11706.087, 75333.413]\nVirginia                  (11706.087, 75333.413]\nWashington              (138330.766, 201328.118]\nWest Virginia             (11706.087, 75333.413]\nWisconsin                (75333.413, 138330.766]\nWyoming                   (11706.087, 75333.413]\nName: CENSUS2010POP, dtype: category\nCategories (10, interval[float64, right]): [(11706.087, 75333.413] < (75333.413, 138330.766] < (138330.766, 201328.118] < (201328.118, 264325.471] ... (390320.176, 453317.529] < (453317.529, 516314.881] < (516314.881, 579312.234] < (579312.234, 642309.586]]\n\n\nHere we see that states like alabama and alaska fall into the same category, while california and the disctrict of columbia fall in a very different category.\nNow, cutting is just one way to build categories from your data, and there are many other methods. For instance, cut gives you interval data, where the spacing between each category is equal sized. But sometimes you want to form categories based on frequency – you want the number of items in each bin to the be the same, instead of the spacing between bins. It really depends on what the shape of your data is, and what you’re planning to do with it."
  },
  {
    "objectID": "code/9_pivot.html",
    "href": "code/9_pivot.html",
    "title": "10  Pivot tables",
    "section": "",
    "text": "A pivot table is itself a DataFrame, where the rows represent one variable that you’re interested in, the columns another, and the cell’s some aggregate value. A pivot table also tends to includes marginal values as well, which are the sums for each column and row. This allows you to be able to see the relationship between two variables at just a glance.\n\nimport pandas as pd\nimport numpy as np\n\nHere we have the Times Higher Education World University Ranking dataset, which is one of the most influential university measures. Let’s import the dataset and see what it looks like:\n\ndf = pd.read_csv('../data/week3/cwurData.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      world_rank\n      institution\n      country\n      national_rank\n      quality_of_education\n      alumni_employment\n      quality_of_faculty\n      publications\n      influence\n      citations\n      broad_impact\n      patents\n      score\n      year\n    \n  \n  \n    \n      0\n      1\n      Harvard University\n      USA\n      1\n      7\n      9\n      1\n      1\n      1\n      1\n      NaN\n      5\n      100.00\n      2012\n    \n    \n      1\n      2\n      Massachusetts Institute of Technology\n      USA\n      2\n      9\n      17\n      3\n      12\n      4\n      4\n      NaN\n      1\n      91.67\n      2012\n    \n    \n      2\n      3\n      Stanford University\n      USA\n      3\n      17\n      11\n      5\n      4\n      2\n      2\n      NaN\n      15\n      89.50\n      2012\n    \n    \n      3\n      4\n      University of Cambridge\n      United Kingdom\n      1\n      10\n      24\n      4\n      16\n      16\n      11\n      NaN\n      50\n      86.17\n      2012\n    \n    \n      4\n      5\n      California Institute of Technology\n      USA\n      4\n      2\n      29\n      7\n      37\n      22\n      22\n      NaN\n      18\n      85.21\n      2012\n    \n  \n\n\n\n\nHere we can see each institution’s rank, country, quality of education, other metrics, and overall score.\nLet’s say we want to create a new column called Rank_Level, where institutions with world ranking 1-100 are categorized as first tier and those with world ranking 101 - 200 are second tier, ranking 201 - 300 are third tier, after 301 is other top universities.\n\n#define ranking fct\ndef create_category(ranking):\n    if (ranking >= 1) & (ranking <=100):\n        return \"First tier\"\n    elif (ranking >= 101) & (ranking <=200):\n        return \"Second tier\"\n    elif (ranking >= 201) & (ranking <=300):\n        return('Third tier')\n    return \"Other Uni\"\n\n#apply function\ndf['Rank_Level'] = df['world_rank'].apply(lambda x: create_category(x))\n\n#view data\ndf.head()\n\n\n\n\n\n  \n    \n      \n      world_rank\n      institution\n      country\n      national_rank\n      quality_of_education\n      alumni_employment\n      quality_of_faculty\n      publications\n      influence\n      citations\n      broad_impact\n      patents\n      score\n      year\n      Rank_Level\n    \n  \n  \n    \n      0\n      1\n      Harvard University\n      USA\n      1\n      7\n      9\n      1\n      1\n      1\n      1\n      NaN\n      5\n      100.00\n      2012\n      First tier\n    \n    \n      1\n      2\n      Massachusetts Institute of Technology\n      USA\n      2\n      9\n      17\n      3\n      12\n      4\n      4\n      NaN\n      1\n      91.67\n      2012\n      First tier\n    \n    \n      2\n      3\n      Stanford University\n      USA\n      3\n      17\n      11\n      5\n      4\n      2\n      2\n      NaN\n      15\n      89.50\n      2012\n      First tier\n    \n    \n      3\n      4\n      University of Cambridge\n      United Kingdom\n      1\n      10\n      24\n      4\n      16\n      16\n      11\n      NaN\n      50\n      86.17\n      2012\n      First tier\n    \n    \n      4\n      5\n      California Institute of Technology\n      USA\n      4\n      2\n      29\n      7\n      37\n      22\n      22\n      NaN\n      18\n      85.21\n      2012\n      First tier\n    \n  \n\n\n\n\nA pivot table allows us to pivot out one of these columns a new column headers and compare it against another column as row indices. Let’s say we want to compare rank level versus country of the universities and we want to compare in terms of overall score\nTo do this, we tell Pandas we want the values to be Score, and index to be the country and the columns to be the rank levels. Then we specify that the aggregation function, and here we’ll use the NumPy mean to get the average rating for universities in that country:\n\ndf.pivot_table(values='score', index='country', columns='Rank_Level', aggfunc = [np.mean]).head()\n\n\n\n\n\n  \n    \n      \n      mean\n    \n    \n      Rank_Level\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Argentina\n      NaN\n      44.672857\n      NaN\n      NaN\n    \n    \n      Australia\n      47.9425\n      44.645750\n      49.2425\n      47.285000\n    \n    \n      Austria\n      NaN\n      44.864286\n      NaN\n      47.066667\n    \n    \n      Belgium\n      51.8750\n      45.081000\n      49.0840\n      46.746667\n    \n    \n      Brazil\n      NaN\n      44.499706\n      49.5650\n      NaN\n    \n  \n\n\n\n\nWe can see a hierarchical dataframe where the index, or rows, are by country and the columns have two levels, the top level indicating that the mean value is being used and the second level being our ranks. In this example we only have one variable, the mean, that we are looking at, so we don’t really need a hierarchical index.\nWe notice that there are some NaN values, for example, the first row, Argentia. The NaN values indicate that Argentia has only observations in the “Other Top Universities” category.\nNow, pivot tables aren’t limited to one function that you might want to apply. You can pass a named parameter, aggfunc, which is a list of the different functions to apply, and pandas will provide you with the result using hierarchical column names. Let’s try that same query, but pass in the max() function too:\n\ndf.pivot_table(values='score', index='country', columns='Rank_Level', aggfunc = [np.mean, np.max]).head()\n\n\n\n\n\n  \n    \n      \n      mean\n      amax\n    \n    \n      Rank_Level\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Argentina\n      NaN\n      44.672857\n      NaN\n      NaN\n      NaN\n      45.66\n      NaN\n      NaN\n    \n    \n      Australia\n      47.9425\n      44.645750\n      49.2425\n      47.285000\n      51.61\n      45.97\n      50.40\n      47.47\n    \n    \n      Austria\n      NaN\n      44.864286\n      NaN\n      47.066667\n      NaN\n      46.29\n      NaN\n      47.78\n    \n    \n      Belgium\n      51.8750\n      45.081000\n      49.0840\n      46.746667\n      52.03\n      46.21\n      49.73\n      47.14\n    \n    \n      Brazil\n      NaN\n      44.499706\n      49.5650\n      NaN\n      NaN\n      46.08\n      49.82\n      NaN\n    \n  \n\n\n\n\nSo now we see we have both the mean and the max.\nAs mentioned earlier, we can also summarize the values within a given top level column. For instance, if we want to see an overall average for the country for the mean and we want to see the max of the max, we can indicate that we want pandas to provide marginal values:\n\ndf.pivot_table(values='score', index='country', columns='Rank_Level', aggfunc=[np.mean, np.max], margins=True).head()\n\n\n\n\n\n  \n    \n      \n      mean\n      amax\n    \n    \n      Rank_Level\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n      All\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n      All\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Argentina\n      NaN\n      44.672857\n      NaN\n      NaN\n      44.672857\n      NaN\n      45.66\n      NaN\n      NaN\n      45.66\n    \n    \n      Australia\n      47.9425\n      44.645750\n      49.2425\n      47.285000\n      45.825517\n      51.61\n      45.97\n      50.40\n      47.47\n      51.61\n    \n    \n      Austria\n      NaN\n      44.864286\n      NaN\n      47.066667\n      45.139583\n      NaN\n      46.29\n      NaN\n      47.78\n      47.78\n    \n    \n      Belgium\n      51.8750\n      45.081000\n      49.0840\n      46.746667\n      47.011000\n      52.03\n      46.21\n      49.73\n      47.14\n      52.03\n    \n    \n      Brazil\n      NaN\n      44.499706\n      49.5650\n      NaN\n      44.781111\n      NaN\n      46.08\n      49.82\n      NaN\n      49.82\n    \n  \n\n\n\n\nA pivot table is just a multi-level dataframe, and we can access series or cells in the dataframe in a similar way as we do so for a regular dataframe.\nLet’s create a new dataframe from our previous example:\n\nnew_df=df.pivot_table(values='score', index='country', columns='Rank_Level', aggfunc=[np.mean, np.max], margins=True)\n\n# Now let's look at the index\nprint(new_df.index)\n\n# And let's look at the columns\nprint(new_df.columns)\n\nIndex(['Argentina', 'Australia', 'Austria', 'Belgium', 'Brazil', 'Bulgaria',\n       'Canada', 'Chile', 'China', 'Colombia', 'Croatia', 'Cyprus',\n       'Czech Republic', 'Denmark', 'Egypt', 'Estonia', 'Finland', 'France',\n       'Germany', 'Greece', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Iran',\n       'Ireland', 'Israel', 'Italy', 'Japan', 'Lebanon', 'Lithuania',\n       'Malaysia', 'Mexico', 'Netherlands', 'New Zealand', 'Norway', 'Poland',\n       'Portugal', 'Puerto Rico', 'Romania', 'Russia', 'Saudi Arabia',\n       'Serbia', 'Singapore', 'Slovak Republic', 'Slovenia', 'South Africa',\n       'South Korea', 'Spain', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand',\n       'Turkey', 'USA', 'Uganda', 'United Arab Emirates', 'United Kingdom',\n       'Uruguay', 'All'],\n      dtype='object', name='country')\nMultiIndex([('mean',  'First tier'),\n            ('mean',   'Other Uni'),\n            ('mean', 'Second tier'),\n            ('mean',  'Third tier'),\n            ('mean',         'All'),\n            ('amax',  'First tier'),\n            ('amax',   'Other Uni'),\n            ('amax', 'Second tier'),\n            ('amax',  'Third tier'),\n            ('amax',         'All')],\n           names=[None, 'Rank_Level'])\n\n\nWe can see the columns are hierarchical. The top level column indices have two categories: mean and max, and the lower level column indices have four categories, which are the four rank levels.\nHow would we query this if we want to get the average scores of First Tier Top Unversity levels in each country? We would just need to make two dataframe projections, the first for the mean, then the second for the top tier:\n\nnew_df['mean']['First tier'].head()\n\ncountry\nArgentina        NaN\nAustralia    47.9425\nAustria          NaN\nBelgium      51.8750\nBrazil           NaN\nName: First tier, dtype: float64\n\n\nWe can see that the output is a series object which we can confirm by printing the type. Remember that when you project a single column of values out of a DataFrame you get a series.\n\ntype(new_df['mean']['First tier'])\n\npandas.core.series.Series\n\n\nWhat if we want to find the country that has the maximum average score on First Tier Top University level? We can use the idxmax() function.\n\nnew_df['mean']['First tier'].idxmax()\n\n'United Kingdom'\n\n\nNow, the idxmax() function isn’t special for pivot tables, it’s a built in function to the Series object.\nWe don’t have time to go over all pandas functions and attributes, and I want to encourage you to explore the API to learn more deeply what is available to you.\nIf you want to achieve a different shape of your pivot table, you can do so with the stack and unstack functions.\n\nStacking is pivoting the lowermost column index to become the innermost row index. - Unstacking is the inverse of stacking, pivoting the innermost row index to become the lowermost column index. An example will help make this clear\n\nLet’s look at our pivot table first to refresh what it looks like:\n\nnew_df.head()\n\n\n\n\n\n  \n    \n      \n      mean\n      amax\n    \n    \n      Rank_Level\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n      All\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n      All\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Argentina\n      NaN\n      44.672857\n      NaN\n      NaN\n      44.672857\n      NaN\n      45.66\n      NaN\n      NaN\n      45.66\n    \n    \n      Australia\n      47.9425\n      44.645750\n      49.2425\n      47.285000\n      45.825517\n      51.61\n      45.97\n      50.40\n      47.47\n      51.61\n    \n    \n      Austria\n      NaN\n      44.864286\n      NaN\n      47.066667\n      45.139583\n      NaN\n      46.29\n      NaN\n      47.78\n      47.78\n    \n    \n      Belgium\n      51.8750\n      45.081000\n      49.0840\n      46.746667\n      47.011000\n      52.03\n      46.21\n      49.73\n      47.14\n      52.03\n    \n    \n      Brazil\n      NaN\n      44.499706\n      49.5650\n      NaN\n      44.781111\n      NaN\n      46.08\n      49.82\n      NaN\n      49.82\n    \n  \n\n\n\n\nNow let’s try stacking, this should move the lowermost column, so the tiers of the university rankings, to the inner most row:\n\nnew_df = new_df.stack()\nnew_df.head()\n\n\n\n\n\n  \n    \n      \n      \n      mean\n      amax\n    \n    \n      country\n      Rank_Level\n      \n      \n    \n  \n  \n    \n      Argentina\n      Other Uni\n      44.672857\n      45.66\n    \n    \n      All\n      44.672857\n      45.66\n    \n    \n      Australia\n      First tier\n      47.942500\n      51.61\n    \n    \n      Other Uni\n      44.645750\n      45.97\n    \n    \n      Second tier\n      49.242500\n      50.40\n    \n  \n\n\n\n\nIn the original pivot table, rank levels are the lowermost column, after stacking, rank levels become the innermost index, appearing to the right after country.\nNow let’s try unstacking:\n\nnew_df.unstack().head()\n\n\n\n\n\n  \n    \n      \n      mean\n      amax\n    \n    \n      Rank_Level\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n      All\n      First tier\n      Other Uni\n      Second tier\n      Third tier\n      All\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      All\n      58.350675\n      44.738871\n      49.06545\n      46.843450\n      47.798395\n      100.00\n      46.34\n      51.29\n      47.93\n      100.00\n    \n    \n      Argentina\n      NaN\n      44.672857\n      NaN\n      NaN\n      44.672857\n      NaN\n      45.66\n      NaN\n      NaN\n      45.66\n    \n    \n      Australia\n      47.942500\n      44.645750\n      49.24250\n      47.285000\n      45.825517\n      51.61\n      45.97\n      50.40\n      47.47\n      51.61\n    \n    \n      Austria\n      NaN\n      44.864286\n      NaN\n      47.066667\n      45.139583\n      NaN\n      46.29\n      NaN\n      47.78\n      47.78\n    \n    \n      Belgium\n      51.875000\n      45.081000\n      49.08400\n      46.746667\n      47.011000\n      52.03\n      46.21\n      49.73\n      47.14\n      52.03\n    \n  \n\n\n\n\nThat seems to restore our dataframe to its original shape. What do you think would happen if we unstacked twice in a row?\n\nnew_df.unstack().unstack().head()\n\n      Rank_Level  country  \nmean  First tier  All          58.350675\n                  Argentina          NaN\n                  Australia    47.942500\n                  Austria            NaN\n                  Belgium      51.875000\ndtype: float64\n\n\nWe actually end up unstacking all the way to just a single column, so a series object is returned. This column is just a “value”, the meaning of which is denoted by the hierarchical index of operation, rank, and country."
  },
  {
    "objectID": "code/10_date_and_time.html",
    "href": "code/10_date_and_time.html",
    "title": "11  Dates and times",
    "section": "",
    "text": "Now we’ll be looking at the time series and date functionally in pandas. Manipulating dates and time is quite flexible in Pandas and thus allows us to conduct more analysis such as time series analysis, which we will talk about soon. Actually, pandas was originally created by Wed McKinney to handle date and time data when he worked as a consultant for hedge funds."
  },
  {
    "objectID": "code/10_date_and_time.html#timestamp",
    "href": "code/10_date_and_time.html#timestamp",
    "title": "11  Dates and times",
    "section": "11.1 Timestamp",
    "text": "11.1 Timestamp\nPandas has four main time related classes. Timestamp, DatetimeIndex, Period, and PeriodIndex. First, let’s look at Timestamp. It represents a single timestamp and associates values with points in time.\nFor example, let’s create a timestamp using a string 9/1/2019 10:05AM, and here we have our timestamp. Timestamp is interchangeable with Python’s datetime in most cases.\n\npd.Timestamp('10/1/2019 10:05AM')\n\nTimestamp('2019-10-01 10:05:00')\n\n\nWe can also create a timestamp by passing multiple parameters such as year, month, date, hour, minute, separately:\n\npd.Timestamp(2019,12,20,0,0)\n\nTimestamp('2019-12-20 00:00:00')\n\n\nTimestamp also has some useful attributes, such as isoweekday(), which shows the weekday of the timestamp note that 1 represents Monday and 7 represents Sunday:\n\npd.Timestamp(2019,12,30,0,0).isoweekday()\n\n1\n\n\nYou can find extract the specific year, month, day, hour, minute, second from a timestamp:\n\npd.Timestamp(2019,12,20,5,2,23).second\n\n23"
  },
  {
    "objectID": "code/10_date_and_time.html#period",
    "href": "code/10_date_and_time.html#period",
    "title": "11  Dates and times",
    "section": "11.2 Period",
    "text": "11.2 Period\nSuppose we weren’t interested in a specific point in time and instead wanted a span of time. This is where the Period class comes into play. Period represents a single time span, such as a specific day or month.\nHere we are creating a period that is January 2016:\n\npd.Period('1/2016')\n\nPeriod('2016-01', 'M')\n\n\nYou’ll notice when we print that out that the granularity of the period is M for month, since that was the finest grained piece we provided. Here’s an example of a period that is March 5th, 2016.\n\npd.Period('3/5/2016')\n\nPeriod('2016-03-05', 'D')\n\n\nPeriod objects represent the full timespan that you specify. Arithmetic on period is very easy and intuitive, for instance, if we want to find out 5 months after January 2016, we simply plus 5:\n\npd.Period('1/2016') + 5\n\nPeriod('2016-06', 'M')\n\n\nFrom the result, you can see we get June 2016. If we want to find out two days before March 5th 2016, we simply subtract 2:\n\npd.Period('3/5/2016') - 2\n\nPeriod('2016-03-03', 'D')\n\n\nThe key here is that the period object encapsulates the granularity for arithmetic."
  },
  {
    "objectID": "code/10_date_and_time.html#datetimeindex-and-periodindex",
    "href": "code/10_date_and_time.html#datetimeindex-and-periodindex",
    "title": "11  Dates and times",
    "section": "11.3 DatetimeIndex and PeriodIndex",
    "text": "11.3 DatetimeIndex and PeriodIndex\nThe index of a timestamp is DatetimeIndex. Let’s look at a quick example. First, let’s create our example series t1, we’ll use the Timestamp of September 1st, 2nd and 3rd of 2016. When we look at the series, each Timestamp is the index and has a value associated with it, in this case, a, b and c.\n\nt1 = pd.Series(list('abc'), [pd.Timestamp('2016-09-01'), pd.Timestamp('2016-09-02'), \n                             pd.Timestamp('2016-09-03')])\nt1\n\n2016-09-01    a\n2016-09-02    b\n2016-09-03    c\ndtype: object\n\n\nLooking at the type of our series index, we see that it’s DatetimeIndex.\n\ntype(t1.index)\n\npandas.core.indexes.datetimes.DatetimeIndex\n\n\nSimilarly, we can create a period-based index as well.\n\nt2 = pd.Series(list('def'), [pd.Period('2016-09'), pd.Period('2016-10'), \n                             pd.Period('2016-11')])\nt2\n\n2016-09    d\n2016-10    e\n2016-11    f\nFreq: M, dtype: object\n\n\nLooking at the type of the ts2.index, we can see that it’s PeriodIndex.\n\ntype(t2.index)\n\npandas.core.indexes.period.PeriodIndex"
  },
  {
    "objectID": "code/10_date_and_time.html#converting-to-datetime",
    "href": "code/10_date_and_time.html#converting-to-datetime",
    "title": "11  Dates and times",
    "section": "11.4 Converting to datetime",
    "text": "11.4 Converting to datetime\nNow, let’s look into how to convert to Datetime. Suppose we have a list of dates as strings and we want to create a new dataframe.\nI’m going to try a bunch of different date formats:\n\nd1 = ['2 June 2013', 'Aug 29, 2014', '2015-06-26', '7/12/16']\n\n# And just some random data\nts3 = pd.DataFrame(np.random.randint(10, 100, (4,2)), index=d1, \n                   columns=list('ab'))\nts3\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      2 June 2013\n      35\n      39\n    \n    \n      Aug 29, 2014\n      54\n      93\n    \n    \n      2015-06-26\n      47\n      27\n    \n    \n      7/12/16\n      86\n      97\n    \n  \n\n\n\n\nUsing pandas to_datetime, pandas will try to convert these to Datetime and put them in a standard format.\n\nts3.index = pd.to_datetime(ts3.index, dayfirst = True)\nts3\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      2013-06-02\n      35\n      39\n    \n    \n      2014-08-29\n      54\n      93\n    \n    \n      2015-06-26\n      47\n      27\n    \n    \n      2016-12-07\n      86\n      97\n    \n  \n\n\n\n\nto_datetime also() has options to change the date parse order. For example, we can pass in the argument dayfirst = True to parse the date in European date.\n\npd.to_datetime('4.7.12', dayfirst=True)\n\nTimestamp('2012-07-04 00:00:00')"
  },
  {
    "objectID": "code/10_date_and_time.html#timedelta",
    "href": "code/10_date_and_time.html#timedelta",
    "title": "11  Dates and times",
    "section": "11.5 Timedelta",
    "text": "11.5 Timedelta\nTimedeltas are differences in times. This is not the same as a a period, but conceptually similar. For instance, if we want to take the difference between September 3rd and September 1st, we get a Timedelta of two days.\n\npd.Timestamp('9/3/2016') - pd.Timestamp('9/1/2016')\n\nTimedelta('2 days 00:00:00')\n\n\nWe can also do something like find what the date and time is for 12 days and three hours past September 2nd, at 8:10 AM.\n\npd.Timestamp('9/2/2016 8:10AM') + pd.Timedelta('12D 3H')\n\nTimestamp('2016-09-14 11:10:00')"
  },
  {
    "objectID": "code/10_date_and_time.html#offset",
    "href": "code/10_date_and_time.html#offset",
    "title": "11  Dates and times",
    "section": "11.6 Offset",
    "text": "11.6 Offset\nOffset is similar to timedelta, but it follows specific calendar duration rules. Offset allows flexibility in terms of types of time intervals. Besides hour, day, week, month, etc it also has business day, end of month, semi month begin etc.\nLet’s create a timestamp, and see what day is that:\n\npd.Timestamp('9/4/2016').weekday()\n\n6\n\n\nNow we can now add the timestamp with a week ahead:\n\npd.Timestamp('09/04/2016') + pd.offsets.Week()\n\nTimestamp('2016-09-11 00:00:00')\n\n\nNow let’s try to do the month end, then we would have the last day of September:\n\npd.Timestamp('9/4/2016') + pd.offsets.MonthEnd()\n\nTimestamp('2016-09-30 00:00:00')"
  },
  {
    "objectID": "code/10_date_and_time.html#working-with-dates-in-a-dataframe",
    "href": "code/10_date_and_time.html#working-with-dates-in-a-dataframe",
    "title": "11  Dates and times",
    "section": "11.7 Working with dates in a dataframe",
    "text": "11.7 Working with dates in a dataframe\nNext, let’s look at a few tricks for working with dates in a DataFrame. Suppose we want to look at nine measurements, taken bi-weekly, every Sunday, starting in October 2016.\nUsing date_range, we can create this DatetimeIndex. In data_range, we have to either specify the start or end date. If it is not explicitly specified, by default, the date is considered the start date. Then we have to specify number of periods, and a frequency. Here, we set it to “2W-SUN”, which means biweekly on Sunday:\n\ndates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN')\ndates\n\nDatetimeIndex(['2016-10-02', '2016-10-16', '2016-10-30', '2016-11-13',\n               '2016-11-27', '2016-12-11', '2016-12-25', '2017-01-08',\n               '2017-01-22'],\n              dtype='datetime64[ns]', freq='2W-SUN')\n\n\nThere are many other frequencies that you can specify. For example, you can do business day:\n\npd.date_range('10-01-2016', periods=9, freq='B')\n\nDatetimeIndex(['2016-10-03', '2016-10-04', '2016-10-05', '2016-10-06',\n               '2016-10-07', '2016-10-10', '2016-10-11', '2016-10-12',\n               '2016-10-13'],\n              dtype='datetime64[ns]', freq='B')\n\n\nOr you can do quarterly, with the quarter start in June:\n\npd.date_range('04-01-2016', periods=12, freq='QS-JUN')\n\nDatetimeIndex(['2016-06-01', '2016-09-01', '2016-12-01', '2017-03-01',\n               '2017-06-01', '2017-09-01', '2017-12-01', '2018-03-01',\n               '2018-06-01', '2018-09-01', '2018-12-01', '2019-03-01'],\n              dtype='datetime64[ns]', freq='QS-JUN')\n\n\nNow, let’s go back to our weekly on Sunday example and create a DataFrame using these dates, and some random data, and see what we can do with it.\n\ndates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN')\ndf = pd.DataFrame({'Count 1': 100 + np.random.randint(-5, 10, 9).cumsum(),\n                  'Count 2': 120 + np.random.randint(-5, 10, 9)}, index=dates)\ndf\n\n\n\n\n\n  \n    \n      \n      Count 1\n      Count 2\n    \n  \n  \n    \n      2016-10-02\n      103\n      125\n    \n    \n      2016-10-16\n      109\n      127\n    \n    \n      2016-10-30\n      110\n      116\n    \n    \n      2016-11-13\n      119\n      128\n    \n    \n      2016-11-27\n      117\n      120\n    \n    \n      2016-12-11\n      115\n      116\n    \n    \n      2016-12-25\n      113\n      116\n    \n    \n      2017-01-08\n      121\n      116\n    \n    \n      2017-01-22\n      120\n      118\n    \n  \n\n\n\n\nFirst, we can check what day of the week a specific date is. For example, here we can see that all the dates in our index are on a Sunday. Which matches the frequency that we set:\n\ndf.index.weekday\n\nInt64Index([6, 6, 6, 6, 6, 6, 6, 6, 6], dtype='int64')\n\n\nWe can also use diff() to find the difference between each date’s value.\n\ndf.diff()\n\n\n\n\n\n  \n    \n      \n      Count 1\n      Count 2\n    \n  \n  \n    \n      2016-10-02\n      NaN\n      NaN\n    \n    \n      2016-10-16\n      6.0\n      2.0\n    \n    \n      2016-10-30\n      1.0\n      -11.0\n    \n    \n      2016-11-13\n      9.0\n      12.0\n    \n    \n      2016-11-27\n      -2.0\n      -8.0\n    \n    \n      2016-12-11\n      -2.0\n      -4.0\n    \n    \n      2016-12-25\n      -2.0\n      0.0\n    \n    \n      2017-01-08\n      8.0\n      0.0\n    \n    \n      2017-01-22\n      -1.0\n      2.0\n    \n  \n\n\n\n\nSuppose we want to know what the mean count is for each month in our DataFrame. We can do this using resample. Converting from a higher frequency from a lower frequency is called downsampling (we’ll talk about this in a moment):\n\ndf.resample('M').mean()\n\n\n\n\n\n  \n    \n      \n      Count 1\n      Count 2\n    \n  \n  \n    \n      2016-10-31\n      107.333333\n      122.666667\n    \n    \n      2016-11-30\n      118.000000\n      124.000000\n    \n    \n      2016-12-31\n      114.000000\n      116.000000\n    \n    \n      2017-01-31\n      120.500000\n      117.000000\n    \n  \n\n\n\n\nNow let’s talk about datetime indexing and slicing, which is a wonderful feature of the pandas DataFrame. For instance, we can use partial string indexing to find values from a particular year:\n\ndf['2017']\n\n\n\n\n\n  \n    \n      \n      Count 1\n      Count 2\n    \n  \n  \n    \n      2017-01-08\n      121\n      116\n    \n    \n      2017-01-22\n      120\n      118\n    \n  \n\n\n\n\nOr we can do it from a particular month:\n\ndf.loc['2016-12']\n\n\n\n\n\n  \n    \n      \n      Count 1\n      Count 2\n    \n  \n  \n    \n      2016-12-11\n      115\n      116\n    \n    \n      2016-12-25\n      113\n      116\n    \n  \n\n\n\n\nOr we can even slice on a range of dates For example, here we only want the values from December 2016 onwards.\n\ndf['2016-12':]\n\n\n\n\n\n  \n    \n      \n      Count 1\n      Count 2\n    \n  \n  \n    \n      2016-12-11\n      115\n      116\n    \n    \n      2016-12-25\n      113\n      116\n    \n    \n      2017-01-08\n      121\n      116\n    \n    \n      2017-01-22\n      120\n      118"
  }
]